

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="sanyinjiang">
  <meta name="keywords" content="">
  
    <meta name="description" content="一、参考文档与信息说明 KTransformers 是由清华大学发起的一个项目，它利用 DeepSeek 模型的 MoE 架构特性，将专家模型的权重加载到内存上，并分配 CPU 完成相关计算，同时将 ML&#x2F;KV Cache 加载到 GPU 上，从而实现 CPU+GPU 混合推理。这种方法能够在最大化降低显存占用的同时，保持一定的推理速度。KTransformers 项目旨在解决大模型本地部署难题">
<meta property="og:type" content="article">
<meta property="og:title" content="KTransformers部署DeepSeek-R1-Q4_K_M">
<meta property="og:url" content="https://jiangsanyin.github.io/2025/03/12/KTransformers%E9%83%A8%E7%BD%B2DeepSeek-R1-Q4-K-M/index.html">
<meta property="og:site_name" content="sanyinjiang">
<meta property="og:description" content="一、参考文档与信息说明 KTransformers 是由清华大学发起的一个项目，它利用 DeepSeek 模型的 MoE 架构特性，将专家模型的权重加载到内存上，并分配 CPU 完成相关计算，同时将 ML&#x2F;KV Cache 加载到 GPU 上，从而实现 CPU+GPU 混合推理。这种方法能够在最大化降低显存占用的同时，保持一定的推理速度。KTransformers 项目旨在解决大模型本地部署难题">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2025/03/15/mMQ7nO9Uj8fohJI.png">
<meta property="og:image" content="https://s2.loli.net/2025/03/15/CaKZwxAcIYXeU5O.png">
<meta property="og:image" content="https://s2.loli.net/2025/03/15/XVSxMlW8rKqpc2R.png">
<meta property="og:image" content="https://s2.loli.net/2025/03/16/eKdXrGVz1RD48th.png">
<meta property="og:image" content="https://s2.loli.net/2025/03/18/sKoiTC7548E3rGN.png">
<meta property="og:image" content="https://s2.loli.net/2025/03/18/w7XSJMEuI9yi8Om.png">
<meta property="og:image" content="c:\Users\jiangsanyin\AppData\Roaming\Typora\typora-user-images\image-20250319160528821.png">
<meta property="og:image" content="https://s2.loli.net/2025/03/19/K4hIlFSWxVzQd3e.png">
<meta property="og:image" content="https://s2.loli.net/2025/03/19/RV5CGyF3UTxajkH.png">
<meta property="og:image" content="https://s2.loli.net/2025/03/19/AUjrnoeQc1gFstK.png">
<meta property="og:image" content="https://s2.loli.net/2025/03/19/cNX6p1FGWhUC2gl.png">
<meta property="og:image" content="https://s2.loli.net/2025/03/18/wcEQDdAToZ3MFUL.png">
<meta property="og:image" content="https://s2.loli.net/2025/03/19/R51KPNGjdcb8E6H.png">
<meta property="og:image" content="https://s2.loli.net/2025/03/19/mr3EcPARvQbqIxk.png">
<meta property="og:image" content="https://s2.loli.net/2025/03/19/3PVtvs7umwoqbzI.png">
<meta property="og:image" content="https://s2.loli.net/2025/03/19/RV5CGyF3UTxajkH.png">
<meta property="og:image" content="https://s2.loli.net/2025/03/19/NpZwmkMroBP2WLt.png">
<meta property="article:published_time" content="2025-03-12T03:33:43.000Z">
<meta property="article:modified_time" content="2025-04-06T11:05:10.000Z">
<meta property="article:author" content="sanyinjiang">
<meta property="article:tag" content="KTransformers">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://s2.loli.net/2025/03/15/mMQ7nO9Uj8fohJI.png">
  
  
  
  <title>KTransformers部署DeepSeek-R1-Q4_K_M - sanyinjiang</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"jiangsanyin.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":["home"]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":"lhgjNNoNy5Syl0F4Bw8i5P5K-gzGzoHsz","app_key":"0d6M8Wx7ZmYewOQqA20Nbqen","server_url":"https://lhgjnnon.lc-cn-n1-shared.com","path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  

  

  

  
    
  



  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>sanyinjiang</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle">KTransformers部署DeepSeek-R1-Q4_K_M</span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-03-12 11:33" pubdate>
          2025年3月12日 中午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          3.4k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          29 分钟
        
      </span>
    

    
    
      
        <span id="leancloud-page-views-container" class="post-meta" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="leancloud-page-views"></span> 次
        </span>
        
      
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">KTransformers部署DeepSeek-R1-Q4_K_M</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="一参考文档与信息说明">一、参考文档与信息说明</h1>
<p>KTransformers 是由清华大学发起的一个项目，它利用 DeepSeek 模型的 MoE 架构特性，将专家模型的权重加载到内存上，并分配 CPU 完成相关计算，同时将 ML/KV Cache 加载到 GPU 上，从而实现 CPU+GPU 混合推理。这种方法能够在最大化降低显存占用的同时，保持一定的推理速度。KTransformers 项目旨在解决大模型本地部署难题，实现资源有限情况下大模型的高效本地部署，让更多人能够在自己的设备上运行曾经遥不可及的大型模型。</p>
<h2 id="参考文档">1.1 参考文档</h2>
<ul>
<li><p>参考文章：</p>
<ul>
<li><p>KT的github仓库：https://github.com/kvcache-ai/ktransformers/tree/v0.2.3post2</p></li>
<li><p>安装文档：Kt官方安装文档（https://kvcache-ai.github.io/ktransformers/en/install.html）</p></li>
<li><p>https://mp.weixin.qq.com/s/1keAGOQlkTf_dKrzWmCRZQ</p></li>
<li><p>https://mp.weixin.qq.com/s/C4aTsxzYGV7bFrKyx6juug</p></li>
<li><p>https://kq4b3vgg5b.feishu.cn/wiki/QJ5ywpjnvieTKZk5kPHcG3sLnkd</p></li>
</ul></li>
<li><p>模型下载页面：</p>
<ul>
<li>https://modelscope.cn/models/unsloth/DeepSeek-R1-GGUF/files</li>
</ul></li>
<li><p>下载模型文件：</p>
<ul>
<li>https://modelscope.cn/models/unsloth/DeepSeek-R1-GGUF/resolve/master/DeepSeek-R1-Q4_K_M/DeepSeek-R1-Q4_K_M-00001-of-00009.gguf</li>
</ul></li>
</ul>
<p>只下载DeepSeek-R1-Q4_K_M这个量化版本：https://www.modelscope.cn/models/unsloth/DeepSeek-R1-GGUF/feedback/issueDetail/23220</p>
<h2 id="信息说明">1.2 信息说明</h2>
<p>第1次此次部署是在Centos8-x86_64物理服务器上部署，一直有报错，相关报错信息描述在第2章。是其他同学的服务器，后建议其在物理服务器上创建ubuntu2204容器进行操作。</p>
<p>第2次，准备在自己的ubuntu20.04 LTS-x86_64物理服务器上进行操作，成功。</p>
<h1 id="二centos8上报错处理">二、Centos8上报错处理</h1>
<h2 id="使用torch2.6.0时kt仓库根目录下执行pip-install-.报read-time-out">2.1 使用torch2.6.0时，KT仓库根目录下执行"pip install ."报“Read time out”</h2>
<figure>
<img src="https://s2.loli.net/2025/03/15/mMQ7nO9Uj8fohJI.png" srcset="/img/loading.gif" lazyload alt="image-20250315204632081" /><figcaption aria-hidden="true">image-20250315204632081</figcaption>
</figure>
<p>#换成使用torch2.4.1</p>
<p><code>pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu124</code></p>
<p>#再指定pip安装源为清华源（因为KT就是清华主导的开源项目）：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">sh install.sh<br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">#pip install . -i https://pypi.tuna.tsinghua</span></span><br></code></pre></td></tr></table></figure>
<p>但报如下错误（报错时，安装的cuda版本是<strong><code>cuda_12.6.r12.6/compiler.34431801_0</code></strong>，torch版本是<strong><code>2.4.1+cu124</code></strong>）：</p>
<p>如下提示探查到的CUDA12.6小版本与用来编译当前使用PyTorch2.4.1所使用的CUDA版本（应该是12.4）不匹配，但大部分情况下这不是一个严重问题，所以只是一个警告信息。</p>
<p>第2行提示，不存在为CUDA12.6定义的g++版本边界。</p>
<figure>
<img src="https://s2.loli.net/2025/03/15/CaKZwxAcIYXeU5O.png" srcset="/img/loading.gif" lazyload alt="image-20250315211142720" /><figcaption aria-hidden="true">image-20250315211142720</figcaption>
</figure>
<figure>
<img src="https://s2.loli.net/2025/03/15/XVSxMlW8rKqpc2R.png" srcset="/img/loading.gif" lazyload alt="image-20250315211246828" /><figcaption aria-hidden="true">image-20250315211246828</figcaption>
</figure>
<p>然后，安装<code>cuda12.4+torch2.4.1</code>，再执行<code>sh install.sh</code>，还有g++版本、cmake相关报错</p>
<figure>
<img src="https://s2.loli.net/2025/03/16/eKdXrGVz1RD48th.png" srcset="/img/loading.gif" lazyload alt="image-20250316091400204" /><figcaption aria-hidden="true">image-20250316091400204</figcaption>
</figure>
<p>对于g++版本相关报错，可以考虑升级gcc与g++版本，然后再执行<code>sh install.sh</code>，再查看是否仍有报销</p>
<h1 id="三ubuntu20.04-上部署">三、Ubuntu20.04 上部署</h1>
<h2 id="升级cmake">3.0 升级cmake</h2>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">升级cmake版本（从3.16.3到3.23.0）</span><br>root@ksp-registry:/opt/installPkgs# wget https://cmake.org/files/v3.23/cmake-3.23.0.tar.gz<br>root@ksp-registry:/opt/installPkgs# tar -zxvf cmake-3.23.0.tar.gz<br>root@ksp-registry:/opt/installPkgs# cp -rp cmake-3.23.0 /usr/share/cmake-3.23.0<br>root@ksp-registry:/opt/installPkgs# ln -sf /usr/share/cmake-3.23.0/bin/cmake /usr/bin/cmake  <br>root@ksp-registry:/opt/installPkgs# cmake --version<br>cmake version 3.23.0<br><br>CMake suite maintained and supported by Kitware (kitware.com/cmake).<br></code></pre></td></tr></table></figure>
<h2 id="下载deepseek-r1-q4_k_m">3.1 下载DeepSeek-R1-Q4_K_M</h2>
<h3 id="下载gguf文件">下载GGUF文件</h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">##下载方法1（推荐）</span></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">使用conda创建python虚拟环境</span><br>root@ksp-registry:/opt/code_repos/AI_models# conda create -n self-llm python=3.12<br>root@ksp-registry:/opt/code_repos/AI_models# conda activate self-llm<br><br>(self-llm) root@ksp-registry:/opt/code_repos/AI_models# mkdir DeepSeek-R1-Q4_K_M<br>(self-llm) root@ksp-registry:/opt/code_repos/AI_models# cd DeepSeek-R1-Q4_K_M<br>(self-llm) root@ksp-registry:/opt/code_repos/AI_models/DeepSeek-R1-Q4_K_M# vi download-DeepSeek-R1-Q4_K_M.sh<br><span class="hljs-meta prompt_">#</span><span class="language-bash">!/bin/bash</span><br><br>for i in $(seq 1 9); do<br>  aria2c -s 16 -x 16 https://modelscope.cn/models/unsloth/DeepSeek-R1-GGUF/resolve/master/DeepSeek-R1-Q4_K_M/DeepSeek-R1-Q4_K_M-0000$&#123;i&#125;-of-00009.gguf<br>done<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">所有文件下载完成后，大概有600多G</span><br>(self-llm) root@ksp-registry:/opt/code_repos/AI_models/DeepSeek-R1-Q4_K_M# apt-get update &amp;&amp; apt -qy install aria2<br>(self-llm) root@ksp-registry:/opt/code_repos/AI_models/DeepSeek-R1-Q4_K_M# bash download-DeepSeek-R1-Q4_K_M.sh<br></code></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">##下载方法2（会下载DeepSeek-R1-Zero-Q4_K_M-xxx文件，这些文件不需要）</span></span><br>(self-llm) root@ksp-registry:/opt/code_repos/AI_models# pip install modelscope<br>(self-llm) root@ksp-registry:/opt/code_repos/AI_models# vi download.py<br>from modelscope import snapshot_download<br>snapshot_download(<br>  repo_id = &quot;unsloth/DeepSeek-R1-GGUF&quot;,<br>  local_dir = &quot;DeepSeek-R1-GGUF&quot;,<br>  allow_patterns = [&quot;*Q4_K_M*&quot;], # Select quant type Q4_K_M<br>)<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">##此python脚本会将“https://www.modelscope.cn/models/unsloth/DeepSeek-R1-GGUF/files/DeepSeek-R1-Q4_K_M”h目录下所有18个文件都下载下来，即除了“DeepSeek-R1-Q4_K_M-0000X-of-00009.gguf”，还有“DeepSeek-R1-Zero-Q4_K_M-0000X-of-00009.gguf”</span></span><br>(self-llm) root@ksp-registry:/opt/code_repos/AI_models# python download.py<br>Downloading Model to directory: /opt/code_repos/AI_models/DeepSeek-R1-GGUF<br>2025-03-17 10:50:41,631 - modelscope - INFO - Got 18 files, start to download ...<br>Processing 18 items:   0%|                                                                                                     | 0.00/18.0 [00:00&lt;?, ?it/s]<br>Downloading [DeepSeek-R1-Q4_K_M/DeepSeek-R1-Q4_K_M-00001-of-00009.gguf]:   1%|▎                                      | 322M/45.0G [01:05&lt;2:22:31, 5.61MB/s]<br></code></pre></td></tr></table></figure>
<h3 id="下载配置文件">下载配置文件</h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell">root@ksp-registry:/opt/code_repos/AI_models/DeepSeek-R1-GGUF# wget https://modelscope.cn/models/unsloth/DeepSeek-R1-GGUF/resolve/master/config.json<br><br>root@ksp-registry:/opt/code_repos/AI_models/DeepSeek-R1-GGUF# wget https://modelscope.cn/models/unsloth/DeepSeek-R1-GGUF/resolve/master/.gitattributes<br><br>root@ksp-registry:/opt/code_repos/AI_models/DeepSeek-R1-GGUF# wget https://modelscope.cn/models/unsloth/DeepSeek-R1-GGUF/resolve/master/configuration.json<br><br>root@ksp-registry:/opt/code_repos/AI_models/DeepSeek-R1-GGUF# wget https://modelscope.cn/models/unsloth/DeepSeek-R1-GGUF/resolve/master/README.md<br></code></pre></td></tr></table></figure>
<h3 id="llama.cpp运行模型可选">llama.cpp运行模型(可选)</h3>
<p>参考：https://www.modelscope.cn/models/unsloth/DeepSeek-R1-GGUF</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">###以下方法有风险(可能会覆盖原有的cmake-3.16.3)，暂未执行</span></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">root@ksp-registry:/opt/installPkgs# <span class="hljs-built_in">cd</span> cmake-3.23.0/</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">检查系统环境并生成 Makefile（--prefix=/path	指定安装路径（默认为 /usr/local））</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">root@ksp-registry:/opt/installPkgs/cmake-3.23.0# ./configure</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">编译</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">root@ksp-registry:/opt/installPkgs/cmake-3.23.0# make -j8</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">安装</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">root@ksp-registry:/opt/installPkgs/cmake-3.23.0# make install</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">#建立软链接，使用安装的新版本的cmake</span></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">update-alternatives --install /usr/bin/cmake  cmake /usr/local/bin/cmake  1 –force</span><br></code></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs shell">apt-get update<br>apt-get install build-essential curl libcurl4-openssl-dev -y<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">##下面的步骤可选</span></span><br>(self-llm) root@ksp-registry:~# cd /opt/code_repos/<br>(self-llm) root@ksp-registry:~# git clone https://github.com/ggerganov/llama.cpp<br><br>cmake llama.cpp -B llama.cpp/build \<br>	-DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON<br>cmake --build llama.cpp/build --config Release -j --clean-first --target llama-quantize llama-cli llama-gguf-split<br><br>root@ksp-registry:/opt/code_repos# ll llama.cpp/build/bin/llama-*<br>-rwxr-xr-x 1 root root 451135880 Mar 17 11:13 llama.cpp/build/bin/llama-cli*<br>-rwxr-xr-x 1 root root 449090696 Mar 17 11:13 llama.cpp/build/bin/llama-gguf-split*<br>-rwxr-xr-x 1 root root 449629880 Mar 17 11:12 llama.cpp/build/bin/llama-quantize*<br><span class="hljs-meta prompt_">#</span><span class="language-bash">将生成的3个文件复制到目录llama.cpp 下</span><br>cp llama.cpp/build/bin/llama-* llama.cpp<br><br></code></pre></td></tr></table></figure>
<h2 id="安装基础组件或依赖">3.2 安装基础组件或依赖</h2>
<h3 id="nvidia驱动与cuda">3.2.1 NVIDIA驱动与cuda</h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">NVIDIA驱动已经安装好</span><br>(self-llm) root@ksp-registry:/opt/code_repos# nvidia-smi <br>Mon Mar 17 14:53:50 2025       <br>+-----------------------------------------------------------------------------------------+<br>| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |<br>|-----------------------------------------+------------------------+----------------------+<br>| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |<br>| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |<br>|                                         |                        |               MIG M. |<br>|=========================================+========================+======================|<br>|   0  NVIDIA A40                     Off |   00000000:C1:00.0 Off |                    0 |<br>|  0%   33C    P8             21W /  300W |       0MiB /  46068MiB |      0%      Default |<br>|                                         |                        |                  N/A |<br>+-----------------------------------------+------------------------+----------------------+<br>                                                                                         <br>+-----------------------------------------------------------------------------------------+<br>| Processes:                                                                              |<br>|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |<br>|        ID   ID                                                               Usage      |<br>|=========================================================================================|<br>|  No running processes found                                                             |<br>+-----------------------------------------------------------------------------------------+<br></code></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">安装cuda12.4</span><br>(self-llm) root@ksp-registry:/opt/code_repos# ll /opt/nvidia-driver-cuda-for-A40/<br>total 4650004<br>drwxr-xr-x 2 root root       4096 Feb 14 14:31 ./<br>drwxr-xr-x 8 root root       4096 Mar 17 10:25 ../<br>-rw-r--r-- 1 root root 4454730420 Mar 29  2024 cuda_12.4.1_550.54.15_linux.run<br>-rwxrwxrwx 1 root root  306858135 May 17  2024 NVIDIA-Linux-x86_64-550.54.15.run*<br><span class="hljs-meta prompt_">#</span><span class="language-bash">也已经安装好</span><br>(self-llm) root@ksp-registry:/opt/code_repos# nvcc -V<br>nvcc: NVIDIA (R) Cuda compiler driver<br>Copyright (c) 2005-2024 NVIDIA Corporation<br>Built on Thu_Mar_28_02:18:24_PDT_2024<br>Cuda compilation tools, release 12.4, V12.4.131<br>Build cuda_12.4.r12.4/compiler.34097967_0<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">编辑/root/.bashrc，添加如下内容</span><br>(self-llm) root@ksp-registry:/opt/code_repos# vi /root/.bashrc <br>export PATH=/usr/local/cuda-12.4/bin/:$PATH<br>export LD_LIBRARY_PATH=/usr/local/cuda-12.4/lib64:$LD_LIBRARY_PATH<br>export CUDA_PATH=/usr/local/cuda<br>(self-llm) root@ksp-registry:/opt/code_repos# source /root/.bashrc <br></code></pre></td></tr></table></figure>
<h3 id="安装编译组件">3.2.2 安装编译组件</h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs shell">apt-get update<br>apt-get install gcc g++ ninja-build<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">查看gcc版本</span><br>(self-llm) root@ksp-registry:/opt/code_repos# gcc --version<br>gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0<br>Copyright (C) 2019 Free Software Foundation, Inc.<br>This is free software; see the source for copying conditions.  There is NO<br>warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.<br><span class="hljs-meta prompt_">#</span><span class="language-bash">查看g++版本</span><br>(self-llm) root@ksp-registry:/opt/code_repos# g++ --version<br>g++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0<br>Copyright (C) 2019 Free Software Foundation, Inc.<br>This is free software; see the source for copying conditions.  There is NO<br>warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.<br><span class="hljs-meta prompt_">#</span><span class="language-bash">查看cmake版本</span><br>(self-llm) root@ksp-registry:/opt/code_repos# cmake --version<br>cmake version 3.23.0<br><br>CMake suite maintained and supported by Kitware (kitware.com/cmake).<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">查看ninja版本</span><br>(self-llm) root@ksp-registry:/opt/code_repos# ninja --version<br>1.10.0<br><span class="hljs-meta prompt_">#</span><span class="language-bash">安装基础组件</span><br>(self-llm) root@ksp-registry:/opt/code_repos# apt install build-essential curl libcurl4-openssl-dev -y<br></code></pre></td></tr></table></figure>
<h2 id="创建kt专用python虚拟环境">3.3 创建KT专用python虚拟环境</h2>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">conda create --name kt python=3.11<br>conda activate kt<br></code></pre></td></tr></table></figure>
<h2 id="安装ktransformers">3.4 安装KTransformers</h2>
<p>KT的github仓库：https://github.com/kvcache-ai/ktransformers/tree/v0.2.3post2</p>
<p>安装文档：Kt官方安装文档（https://kvcache-ai.github.io/ktransformers/en/install.html）</p>
<h3 id="执行安装前准备">执行安装前准备</h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs shell">(kt) root@ksp-registry:/opt/code_repos# git clone -b v0.2.3post2 https://gitee.com/sy-jiang/ktransformers.git<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">保证此python虚拟环境使用的GNU C++标准库版本包括GLIBCXX-3.4.32</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">conda提供了一个名为libstdcxx-ng的包，它包含了新版本的libstdc++，其可以通过conda-forge进行安装</span><br>(kt) root@ksp-registry:/opt/code_repos# conda install -c conda-forge libstdcxx-ng<br>(kt) root@ksp-registry:/opt/code_repos# strings ~/anaconda3/envs/ktransformers/lib/libstdc++.so.6 | grep GLIBCXX<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">安装PyTorch, packaging, ninja 等</span><br>(kt) root@ksp-registry:/opt/code_repos# pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu124<br>(kt) root@ksp-registry:/opt/code_repos# pip install packaging ninja cpufeature numpy flash-attn <br></code></pre></td></tr></table></figure>
<h3 id="初始化源码">初始化源码</h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">init <span class="hljs-built_in">source</span> code</span><br>cd ktransformers<br>(kt) root@ksp-registry:/opt/code_repos/ktransformers# git submodule init<br>(kt) root@ksp-registry:/opt/code_repos/ktransformers# git submodule update<br><span class="hljs-meta prompt_">#</span><span class="language-bash">如下llama.cpp、pybind11 两个目录是刚刚新生成的</span><br>(kt) root@ksp-registry:/opt/code_repos/ktransformers# ll third_party/<br>total 20<br>drwxr-xr-x  5 root root 4096 Mar 18 10:41 ./<br>drwxr-xr-x  9 root root 4096 Mar 18 10:50 ../<br>drwxr-xr-x 24 root root 4096 Mar 18 10:58 llama.cpp/<br>drwxr-xr-x  2 root root 4096 Mar 18 10:41 llamafile/<br>drwxr-xr-x  8 root root 4096 Mar 18 10:58 pybind11/<br><br></code></pre></td></tr></table></figure>
<h3 id="编译kt-website">编译kt-website</h3>
<p>参考文档：https://kvcache-ai.github.io/ktransformers/en/api/server/website.html</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">要求Node.js&gt;=18.3</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">如果已经通过ubuntu20默认安装源安装了nodejs，其版本太低，需要先卸载掉</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">apt-get remove nodejs npm -y &amp;&amp; <span class="hljs-built_in">sudo</span> apt-get autoremove -y</span><br>apt-get update -y &amp;&amp; apt-get install -y apt-transport-https ca-certificates curl gnupg<br><br>curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | gpg --dearmor -o /usr/share/keyrings/nodesource.gpg<br><br>chmod 644 /usr/share/keyrings/nodesource.gpg<br><br>echo &quot;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/nodesource.gpg] https://deb.nodesource.com/node_23.x nodistro main&quot; | sudo tee /etc/apt/sources.list.d/nodesource.list<br><br>apt-get update -y<br>apt-get install nodejs -y<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">查看nodejs与npm版本</span><br>(kt) root@ksp-registry:/opt/code_repos/ktransformers# node -v<br>v23.10.0<br>(kt) root@ksp-registry:/opt/code_repos/ktransformers# npm -v<br>10.9.2<br><br></code></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">安装Vue CLI</span><br>(kt) root@ksp-registry:/opt/code_repos/ktransformers/ktransformers/website# npm install @vue/cli<br>(kt) root@ksp-registry:/opt/code_repos/ktransformers/ktransformers/website# npm run build<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">暂时也可不执行，后续会有步骤执行此操作</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">使用website编译ktransformers</span> <br>(kt) root@ksp-registry:/opt/code_repos/ktransformers/ktransformers/website# cd ../../<br>(kt) root@ksp-registry:/opt/code_repos/ktransformers# pip install .<br><span class="hljs-meta prompt_">#</span><span class="language-bash">查看安装的kt信息</span><br>(kt) root@ksp-registry:/opt/code_repos/ktransformers# pip show ktransformers<br>Name: ktransformers<br>Version: 0.2.3.post2<br>Summary: KTransformers, pronounced as Quick Transformers, is designed to enhance your Transformers experience with advanced kernel optimizations and placement/parallelism strategies.<br>Home-page: https://kvcache.ai<br>Author: <br>Author-email: &quot;KVCache.AI&quot; &lt;zhang.mingxing@outlook.com&gt;<br>License: Apache License<br>...<br></code></pre></td></tr></table></figure>
<h3 id="安装kt">安装KT</h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">1）对于有双槽CPU和内存是模型文件两倍大小以上的服务器</span><br>(kt) root@ksp-registry:/opt/code_repos/ktransformers# apt install libnuma-dev<br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">##(kt) root@ksp-registry:/opt/code_repos/ktransformers# export USE_NUMA=1  #不要执行，笔者执行了后面启动模型服务时内存爆了</span></span><br>(kt) root@ksp-registry:/opt/code_repos/ktransformers# bash install.sh   # or #make dev_install<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">2）否则，直接执行如下命令</span><br>(kt) root@ksp-registry:/opt/code_repos/ktransformers# bash install.sh<br></code></pre></td></tr></table></figure>
<figure>
<img src="https://s2.loli.net/2025/03/18/sKoiTC7548E3rGN.png" srcset="/img/loading.gif" lazyload alt="image-20250318155035637" /><figcaption aria-hidden="true">image-20250318155035637</figcaption>
</figure>
<p>如下查看安装成功的KTransformers：<code>pip show ktransformers</code></p>
<figure>
<img src="https://s2.loli.net/2025/03/18/w7XSJMEuI9yi8Om.png" srcset="/img/loading.gif" lazyload alt="image-20250318155131355" /><figcaption aria-hidden="true">image-20250318155131355</figcaption>
</figure>
<h2 id="local-chat本地对话">3.5 Local Chat本地对话</h2>
<p>参考：https://github.com/kvcache-ai/ktransformers/blob/v0.2.3post2/doc/zh/DeepseekR1_V3_tutorial_zh.md#v02-%E5%B1%95%E7%A4%BA</p>
<p><img src="C:\Users\jiangsanyin\AppData\Roaming\Typora\typora-user-images\image-20250319160528821.png" srcset="/img/loading.gif" lazyload alt="image-20250319160528821" style="zoom: 33%;" /></p>
<h3 id="启动本地对话">3.5.1 启动本地对话</h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell">(kt) root@ksp-registry:/opt/code_repos/ktransformers# cp ./ktransformers/models/configuration_deepseek.py /opt/code_repos/AI_models/DeepSeek-R1-GGUF<br>(kt) root@ksp-registry:/opt/code_repos/ktransformers# cp ./ktransformers/models/configuration_deepseek_v3.py /opt/code_repos/AI_models/DeepSeek-R1-GGUF<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">#在KT仓库根目录执行如下命令</span></span><br>(kt) root@ksp-registry:/opt/code_repos/ktransformers# python ./ktransformers/local_chat.py --model_path deepseek-ai/DeepSeek-R1 \<br>  --gguf_path /opt/code_repos/AI_models/DeepSeek-R1-GGUF/DeepSeek-R1-Q4_K_M \<br>  --cpu_infer 36 --max_new_tokens 8192 --port 10002 --web True  <br><span class="hljs-meta prompt_">#</span><span class="language-bash">启动过程上，可以看到进程使用了GPU</span><br></code></pre></td></tr></table></figure>
<figure>
<img src="https://s2.loli.net/2025/03/19/K4hIlFSWxVzQd3e.png" srcset="/img/loading.gif" lazyload alt="image-20250319102718590" /><figcaption aria-hidden="true">image-20250319102718590</figcaption>
</figure>
<p>最终看到对话窗口：</p>
<figure>
<img src="https://s2.loli.net/2025/03/19/RV5CGyF3UTxajkH.png" srcset="/img/loading.gif" lazyload alt="image-20250319174522477" /><figcaption aria-hidden="true">image-20250319174522477</figcaption>
</figure>
<p>加载过程中及完成后，占用的内存很少：</p>
<figure>
<img src="https://s2.loli.net/2025/03/19/AUjrnoeQc1gFstK.png" srcset="/img/loading.gif" lazyload alt="image-20250319175945166" /><figcaption aria-hidden="true">image-20250319175945166</figcaption>
</figure>
<p>但GPU 显存占用较多，但远没满</p>
<figure>
<img src="https://s2.loli.net/2025/03/19/cNX6p1FGWhUC2gl.png" srcset="/img/loading.gif" lazyload alt="image-20250319180135883" /><figcaption aria-hidden="true">image-20250319180135883</figcaption>
</figure>
<h3 id="报错与处理">3.5.2 报错与处理</h3>
<h4 id="couldnt-connect-to-httpshuggingface.co">3.5.2.1 couldn't connect to 'https://huggingface.co'</h4>
<p>第一次，笔者执行命令<code>python ./ktransformers/local_chat.py --model_path unsloth/DeepSeek-R1-GGUF --gguf_path /opt/code_repos/AI_models/DeepSeek-R1-GGUF/DeepSeek-R1-Q4_K_M --cpu_infer 36 --max_new_tokens 8192</code>时，提示</p>
<p>“OSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like unsloth/DeepSeek-R1-GGUF is not the path to a directory containing a file named config.json.</p>
<p>Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.”</p>
<p>具体如下：</p>
<figure>
<img src="https://s2.loli.net/2025/03/18/wcEQDdAToZ3MFUL.png" srcset="/img/loading.gif" lazyload alt="image-20250318162546417" /><figcaption aria-hidden="true">image-20250318162546417</figcaption>
</figure>
<p>解决办法：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">参考https://github.com/huggingface/diffusers/issues/6223</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">添加变量</span><br>(kt) root@ksp-registry:/opt/code_repos/ktransformers# export HF_ENDPOINT=https://hf-mirror.com<br><span class="hljs-meta prompt_">#</span><span class="language-bash">再次执行</span><br>(kt) root@ksp-registry:/opt/code_repos/ktransformers# python ./ktransformers/local_chat.py --model_path unsloth/DeepSeek-R1-GGUF --gguf_path /opt/code_repos/AI_models/DeepSeek-R1-GGUF/DeepSeek-R1-Q4_K_M --cpu_infer 36 --max_new_tokens 8192<br><span class="hljs-meta prompt_">#</span><span class="language-bash">此时还是报错，但报错内容不一样。如下“OSError: unsloth/DeepSeek-R1-GGUF does not appear to have a file named configuration_deepseek.py. Checkout <span class="hljs-string">&#x27;https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main&#x27;</span> <span class="hljs-keyword">for</span> available files.”</span><br></code></pre></td></tr></table></figure>
<figure>
<img src="https://s2.loli.net/2025/03/19/R51KPNGjdcb8E6H.png" srcset="/img/loading.gif" lazyload alt="image-20250319101437770" /><figcaption aria-hidden="true">image-20250319101437770</figcaption>
</figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">笔者先前在“https://modelscope.cn/models/unsloth/DeepSeek-R1-GGUF/files”下载的<span class="hljs-string">&quot;DeepSeek-R1-Q4_K_M&quot;</span>相关文件，但此链接下并没有上述报错中提到的（启动对话过程中需要用到的）configuration_deepseek.py 文件，根据KT官网的示例，需要指定“--model_path deepseek-ai/DeepSeek-R1”，此时再执行就会正常下载configuration_deepseek.py 文件并继续往后执行了</span><br>(kt) root@ksp-registry:/opt/code_repos/ktransformers# python ./ktransformers/local_chat.py --model_path deepseek-ai/DeepSeek-R1 \<br>  --gguf_path /opt/code_repos/AI_models/DeepSeek-R1-GGUF/DeepSeek-R1-Q4_K_M \<br>  --cpu_infer 36 --max_new_tokens 8192<br></code></pre></td></tr></table></figure>
<h4 id="内存不够被-killed">3.5.2.2 内存不够，被 killed</h4>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">(kt) root@ksp-registry:/opt/code_repos/ktransformers# export USE_NUMA=1<br>(kt) root@ksp-registry:/opt/code_repos/ktransformers# bash install.sh   # or #make dev_install<br>(kt) root@ksp-registry:/opt/code_repos/ktransformers# python ./ktransformers/local_chat.py --model_path deepseek-ai/DeepSeek-R1 \<br>  --gguf_path /opt/code_repos/AI_models_/DeepSeek-R1-GGUF/DeepSeek-R1-Q4_K_M \<br>  --cpu_infer 32 --max_new_tokens 8192<br></code></pre></td></tr></table></figure>
<p>同时可以看到可用内存急剧下降。</p>
<p>此服务器只有503G总内存，加载块的过程中因为内存不足，进程最终被自动杀死：</p>
<p><img src="https://s2.loli.net/2025/03/19/mr3EcPARvQbqIxk.png" srcset="/img/loading.gif" lazyload alt="image-20250319103756422" style="zoom:67%;" /></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">创建 800GB 的 Swap 文件（因为笔者此服务器安装操作系统的系统盘就是SSD盘，所以直接从系统盘中划分出300G）</span><br>(kt) root@ksp-registry:/# fallocate -l 800G /opt/code_repos/test_swap/swapfile<br>(kt) root@ksp-registry:/# chmod 600 /opt/code_repos/test_swap/swapfile<br><span class="hljs-meta prompt_">#</span><span class="language-bash">将文件或分区初始化为交换空间</span><br>(kt) root@ksp-registry:/# mkswap /opt/code_repos/test_swap/swapfile<br><span class="hljs-meta prompt_">#</span><span class="language-bash">启用交换文件</span><br>(kt) root@ksp-registry:/# swapon /opt/code_repos/test_swap/swapfile<br><span class="hljs-meta prompt_">#</span><span class="language-bash">查看物理内存与交换内存</span><br>root@ksp-registry:~# free -h<br>              total        used        free      shared  buff/cache   available<br>Mem:          503Gi       4.7Gi       226Gi       8.0Mi       272Gi       496Gi<br>Swap:         799Gi          0B       799Gi<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">删除交换空间</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">swapoff /swapfile</span> <br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-built_in">rm</span> /swapfile</span> <br></code></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">#在KT仓库根目录执行如下命令，使用“--web True”website会一起启动，“--port 10002”指定web访问端口</span></span><br>(kt) root@ksp-registry:/opt/code_repos/ktransformers# python ./ktransformers/local_chat.py --model_path deepseek-ai/DeepSeek-R1 \<br>  --gguf_path /opt/code_repos/AI_models_/DeepSeek-R1-GGUF/DeepSeek-R1-Q4_K_M \<br>  --cpu_infer 32 --max_new_tokens 8192 --port 10002 --web True<br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">##我的实验结果还是失败，加载到28层就被killed，缺内存</span></span><br></code></pre></td></tr></table></figure>
<p><img src="https://s2.loli.net/2025/03/19/3PVtvs7umwoqbzI.png" srcset="/img/loading.gif" lazyload alt="image-20250319164809383" style="zoom:50%;" /></p>
<p>解决办法：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">此步骤很关键</span><br>(kt) root@ksp-registry:/opt/code_repos/ktransformers# unset USE_NUMA<br><span class="hljs-meta prompt_">#</span><span class="language-bash">重新安装KTransformers</span><br>(kt) root@ksp-registry:/opt/code_repos/ktransformers# bash install.sh   # or #make dev_install<br></code></pre></td></tr></table></figure>
<figure>
<img src="https://s2.loli.net/2025/03/19/RV5CGyF3UTxajkH.png" srcset="/img/loading.gif" lazyload alt="image-20250319174522477" /><figcaption aria-hidden="true">image-20250319174522477</figcaption>
</figure>
<h4 id="安装flashinfer可选">3.5.2.3 安装flashinfer(可选)</h4>
<p>每次启动模型服务时，都会提示<code>flashinfer not found, use triton for linux using custom modeling_xxx.py.</code></p>
<p>flashinfer 是一个用于加速大型语言模型（LLM）部署的核库。它通过提供高效的内存带宽共享前缀批处理解码技术，显著提升了自注意力机制的性能。FlashInfer 支持多种 GPU 架构，包括 sm80、sm86、sm89 和 sm90，并且正在开发对 sm75 和 sm70 的支持。</p>
<p>其在github上的代码仓库地址：https://github.com/flashinfer-ai/flashinfer.git</p>
<p>官方安装文档：https://docs.flashinfer.ai/installation.html</p>
<h5 id="通过pip安装">通过pip安装</h5>
<figure>
<img src="https://s2.loli.net/2025/03/19/NpZwmkMroBP2WLt.png" srcset="/img/loading.gif" lazyload alt="image-20250319215515077" /><figcaption aria-hidden="true">image-20250319215515077</figcaption>
</figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs shell">pip install flashinfer -i https://flashinfer.ai/whl/cu124/torch2.4/<br><span class="hljs-meta prompt_">#</span><span class="language-bash">或</span><br>wget https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.0.post1/flashinfer-0.2.0.post1+cu124torch2.4-cp311-cp311-linux_x86_64.whl<br>pip install flashinfer-0.2.0.post1+cu124torch2.4-cp311-cp311-linux_x86_64.whl<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">##但此时会提示flashinfer do not have attribute mla，看到如下处理方法（https://github.com/kvcache-ai/ktransformers/issues/792）：</span></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">replace pip install flashinfer -i https://flashinfer.ai/whl/cu121/torch2.3 with:</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">install JIT version:</span><br>pip install flashinfer-python<br>conda install cuda-nvcc -c nvidia<br>export CUDA_HOME=$CONDA_PREFIX<br>export TORCH_CUDA_ARCH_LIST=&quot;8.0+PTX&quot;<br></code></pre></td></tr></table></figure>
<h5 id="通过源码安装">通过源码安装</h5>
<p>参考官方文档：https://docs.flashinfer.ai/installation.html#install-from-source</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">git clone -b v0.2.1.post1 https://github.com/flashinfer-ai/flashinfer.git --recursive<br></code></pre></td></tr></table></figure>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" class="category-chain-item">大模型</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/KTransformers/" class="print-no-link">#KTransformers</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>KTransformers部署DeepSeek-R1-Q4_K_M</div>
      <div>https://jiangsanyin.github.io/2025/03/12/KTransformers部署DeepSeek-R1-Q4-K-M/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>sanyinjiang</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年3月12日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/03/14/4%E2%9C%968%E2%9C%96A800%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%83%A8%E7%BD%B2%E6%BB%A1%E8%A1%80%E7%89%88DeepSeek-R1-671B%E6%AD%A5%E9%AA%A4/" title="4✖8✖A800服务器部署满血版DeepSeek-R1-671B步骤">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">4✖8✖A800服务器部署满血版DeepSeek-R1-671B步骤</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/03/11/Datawhale%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E8%AF%BE%E7%A8%8B%E5%88%86%E7%BB%84%E5%AD%A6%E4%B9%A0/" title="跟随《大语言模型-赵鑫教授团队》入门大语言模型">
                        <span class="hidden-mobile">跟随《大语言模型-赵鑫教授团队》入门大语言模型</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"lhgjNNoNy5Syl0F4Bw8i5P5K-gzGzoHsz","appKey":"0d6M8Wx7ZmYewOQqA20Nbqen","path":"window.location.pathname","placeholder":null,"avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="leancloud-site-pv-container" style="display: none">
        总访问量 
        <span id="leancloud-site-pv"></span>
         次
      </span>
    
    
      <span id="leancloud-site-uv-container" style="display: none">
        总访客数 
        <span id="leancloud-site-uv"></span>
         人
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>





  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script defer src="/js/leancloud.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
