

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="sanyinjiang">
  <meta name="keywords" content="">
  
    <meta name="description" content="一、服务器及配置信息 1.1 服务器列表 此次用来部署满血版DeepSeek-R1-671B，用到4个物理服务器。每个物理服务器上有8个NVIDIA A800，相关具体硬件信息如下。     主机名 IP CPU 内存 硬盘 GPU情况 IB网卡 万兆网卡 其他信息     deepseek1 10.119.165.139(万兆内网)10.119.85.">
<meta property="og:type" content="article">
<meta property="og:title" content="4✖8✖A800服务器部署满血版DeepSeek-R1-671B步骤">
<meta property="og:url" content="https://jiangsanyin.github.io/2025/03/14/4%E2%9C%968%E2%9C%96A800%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%83%A8%E7%BD%B2%E6%BB%A1%E8%A1%80%E7%89%88DeepSeek-R1-671B%E6%AD%A5%E9%AA%A4/index.html">
<meta property="og:site_name" content="sanyinjiang">
<meta property="og:description" content="一、服务器及配置信息 1.1 服务器列表 此次用来部署满血版DeepSeek-R1-671B，用到4个物理服务器。每个物理服务器上有8个NVIDIA A800，相关具体硬件信息如下。     主机名 IP CPU 内存 硬盘 GPU情况 IB网卡 万兆网卡 其他信息     deepseek1 10.119.165.139(万兆内网)10.119.85.">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2025/02/26/Y96Gwv4AnultSVZ.png">
<meta property="og:image" content="https://s2.loli.net/2025/02/21/iv6oBq75Rr8EzDO.png">
<meta property="og:image" content="https://s2.loli.net/2025/02/21/i8vKNVPkExZjD65.png">
<meta property="og:image" content="https://s2.loli.net/2025/02/22/s8wCKUVEL2Sphna.png">
<meta property="og:image" content="https://s2.loli.net/2025/02/24/V8FOf1Q4Z6Ww9sg.png">
<meta property="og:image" content="https://s2.loli.net/2025/02/24/F3td4gkAeSfwijK.png">
<meta property="og:image" content="https://s2.loli.net/2025/02/24/iqRKSIfvJEY9sm4.png">
<meta property="og:image" content="https://s2.loli.net/2025/02/25/u9lErBStzK3sULx.png">
<meta property="article:published_time" content="2025-03-14T14:22:38.000Z">
<meta property="article:modified_time" content="2025-03-16T14:49:46.000Z">
<meta property="article:author" content="sanyinjiang">
<meta property="article:tag" content="部署满血版DeepSeek">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://s2.loli.net/2025/02/26/Y96Gwv4AnultSVZ.png">
  
  
  
  <title>4✖8✖A800服务器部署满血版DeepSeek-R1-671B步骤 - sanyinjiang</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"jiangsanyin.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":["home"]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":"lhgjNNoNy5Syl0F4Bw8i5P5K-gzGzoHsz","app_key":"0d6M8Wx7ZmYewOQqA20Nbqen","server_url":"https://lhgjnnon.lc-cn-n1-shared.com","path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  

  

  

  
    
  



  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>sanyinjiang</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle">4✖8✖A800服务器部署满血版DeepSeek-R1-671B步骤</span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-03-14 22:22" pubdate>
          2025年3月14日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          18k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          154 分钟
        
      </span>
    

    
    
      
        <span id="leancloud-page-views-container" class="post-meta" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="leancloud-page-views"></span> 次
        </span>
        
      
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">4✖8✖A800服务器部署满血版DeepSeek-R1-671B步骤</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="一服务器及配置信息">一、服务器及配置信息</h1>
<h2 id="服务器列表">1.1 服务器列表</h2>
<p>此次用来部署满血版DeepSeek-R1-671B，用到4个物理服务器。每个物理服务器上有8个NVIDIA
A800，相关具体硬件信息如下。</p>
<table>

<thead>
<tr>
<th>主机名</th>
<th>IP</th>
<th>CPU</th>
<th>内存</th>
<th>硬盘</th>
<th>GPU情况</th>
<th>IB网卡</th>
<th>万兆网卡</th>
<th>其他信息</th>
</tr>
</thead>
<tbody>
<tr>
<td>deepseek1</td>
<td>10.119.165.139(万兆内网)<br />10.119.85.141(IB内网)</td>
<td>Intel 6348*2</td>
<td>64G DDR4*16</td>
<td>（1）960G SATA**2；（2）3.84T U.2 NVMe*1</td>
<td>HGX A800系统（8<em>NVIDIA A800 80GB SXM4 with
6</em>NVSwitch）*1</td>
<td>需要</td>
<td>（1）10G 双光口<em>1；（2）25G 双光口 OCP3.0</em>1；（3）200G
单光口HDR*4；</td>
<td>（1）电源：3500W PSU<em>4；（2）硬RAID：RAID LSI
9540-8i；（3）其他：GPU仓背板</em>2；CPU仓EXP背板<em>1；riser</em>2；1.8M
国标电源线<em>4；托轨</em>1</td>
</tr>
<tr>
<td>deepseek2</td>
<td>10.119.165.138(内网)<br />10.119.85.138(IB内网)</td>
<td>~</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>deepseek3</td>
<td>10.119.165.140(内网)<br />10.119.85.140(IB内网)</td>
<td>~</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>deepseek4</td>
<td>10.119.165.141(内网)<br />10.119.85.139(IB内网)</td>
<td>~</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<figure>
<img src="https://s2.loli.net/2025/02/26/Y96Gwv4AnultSVZ.png" srcset="/img/loading.gif" lazyload
alt="image-20250226103235878" />
<figcaption aria-hidden="true">image-20250226103235878</figcaption>
</figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">10000M network</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">10.119.165.139 deepseek1</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">10.119.165.138 deepseek2</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">10.119.165.140 deepseek3</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">10.119.165.141 deepseek4</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">IB network</span><br>10.119.85.141 deepseek1<br>10.119.85.138 deepseek2<br>10.119.85.140 deepseek3<br>10.119.85.139 deepseek4<br></code></pre></td></tr></table></figure>
<p>（1）当前，物理服务器的系统盘由两个SATA盘做 RAID1
构成，按照相关资料说明，建议使用两个1T左右的SSD做RAID1，安装操作系统</p>
<p>（2）现在模型文件是存放在nvme盘的挂载目录上</p>
<p>（3）服务访问、节点间网络通信，能用IB网络尽量用IB网络</p>
<p>（4）节点内8显卡、节点间显卡通信，需要配置好NVLink与NVSwitch</p>
<p>（5）其实上表格中还有一些东西没有用到，比如万兆网卡。</p>
<p>（6）以下deepseek用户具有无密sudo权限，实际操作时也可使用root用户，但还是不建议直接使用root用户。</p>
<h2 id="相关软件信息汇总">1.2 相关软件信息汇总</h2>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs shell">物理服务器操作系统：Ubuntu 22.04.4 LTS-x86_64<br>Nvidia driver version: 550.90.07<br>CUDA runtime version: 12.1.105(node容器内)、V12.4.99(物理服务器上)<br>nvidia-fabricmanager版本：550.90.07<br>nvlink：3.0<br>nvswitch：2.0<br><br>PyTorch version: 2.5.1+cu124<br>CUDA used to build PyTorch: 12.4<br>OS: Ubuntu 22.04.3 LTS (x86_64)<br>GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0<br>CMake version: version 3.31.4<br>Libc version: glibc-2.35<br>Python version: 3.12.9 (main, Feb  5 2025, 08:49:00) [GCC 11.4.0] (64-bit runtime)<br>Python platform: Linux-5.15.0-113-generic-x86_64-with-glibc2.35<br>Is CUDA available: True<br><br>CUDA_MODULE_LOADING set to: LAZY<br>Is XNNPACK available: True<br>CPU: Intel(R) Xeon(R) Gold 6348 CPU @ 2.60GHz, 112核心<br>numpy==1.26.4<br>torch==2.5.1<br>torchaudio==2.5.1<br>torchvision==0.20.1<br>triton==3.1.0<br></code></pre></td></tr></table></figure>
<h1 id="二准备工作">二、准备工作</h1>
<p>以下准备工作在所有服务器上都要执行。</p>
<h2 id="模型文件准备">2.1 模型文件准备</h2>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">添加主机名与ip映射条目</span><br>deepseek@deepseek1:~$ sudo vi /etc/hosts<br><span class="hljs-meta prompt_">#</span><span class="language-bash">添加如下内容</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">10000M network</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">10.119.165.139 deepseek1</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">10.119.165.138 deepseek2</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">10.119.165.140 deepseek3</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">10.119.165.141 deepseek4</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">IB network</span><br>10.119.85.141 deepseek1<br>10.119.85.138 deepseek2<br>10.119.85.140 deepseek3<br>10.119.85.139 deepseek4<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">配置python库安装源</span><br>deepseek@deepseek1:~$ sudo mkdir ~/.pip<br>deepseek@deepseek1:~$ sudo vi ~/.pip/pip.conf <br>deepseek@deepseek1:~$ sudo cat ~/.pip/pip.conf <br>[global]<br>trusted-host = mirrors.aliyun.com<br>index-url = https://mirrors.aliyun.com/pypi/simple<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">下载Anaconda3-2024.06-1-Linux-x86_64.sh，安装anaconda</span><br>deepseek@deepseek1:~/installPkgs$ wget --user-agent=“Mozilla” + https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-2024.06-1-Linux-x86_64.sh<br>deepseek@deepseek1:~/installPkgs$ sudo bash Anaconda3-2024.06-1-Linux-x86_64.sh -p /home/deepseek/anaconda3<br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">##anaconda的安装目录是/home/deepseek/anaconda3</span></span><br>...<br>You can undo this by running `conda init --reverse $SHELL`? [yes|no]<br>[no] &gt;&gt;&gt; yes<br>...<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">加载~/.bashrc</span><br>deepseek@deepseek1:~/installPkgs$ source ~/.bashrc<br><span class="hljs-meta prompt_">#</span><span class="language-bash">查看现有的conda管理的所有虚拟python环境</span><br>(base) deepseek@deepseek1:~/installPkgs$ conda env list<br><span class="hljs-meta prompt_"># </span><span class="language-bash">conda environments:</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"></span><br><span class="language-bash">base                  *  /home/deepseek/anaconda3</span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">准备好python3环境与pip3（使用上述conda创建虚拟python3、pip3环境）</span><br>(base) deepseek@deepseek1:~/installPkgs$ conda create -n self-llm python=3.12 <br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">#查看现有的conda管理的所有虚拟python环境</span></span><br>(base) deepseek@deepseek1:~/installPkgs$ conda env list<br><span class="hljs-meta prompt_"># </span><span class="language-bash">conda environments:</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"></span><br><span class="language-bash">self-llm                 /home/deepseek/.conda/envs/self-llm</span><br>base                  *  /home/deepseek/anaconda3<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">激活self-llm 这个虚拟python环境，并在其中执行相关命令</span><br>(base) deepseek@deepseek1:~/installPkgs$ conda activate self-llm<br>(self-llm) deepseek@deepseek1:~/installPkgs$ <br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">请先通过如下命令安装ModelScope</span><br>(self-llm) deepseek@deepseek1:~/installPkgs$ pip install modelscope<br></code></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">以下4种方法下载不同的模型文件与使用，方法4验证成功</span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">在deepseek1服务器上下载好模型文件，其他3台服务器无需再通过网络下载模型文件，而是将模型文件从此服务器传输上去即可（速度应该快些）</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">##方法1（已经失败）</span></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">下载完整模型repo。加参数“--local_dir ./”就是保存到当前目录，不加就是保存到/root/.cache目录下</span><br>(self-llm) deepseek@deepseek1:~/installPkgs$ modelscope download --model deepseek-ai/DeepSeek-R1<br>Downloading Model to directory: /home/deepseek/.cache/modelscope/hub/models/deepseek-ai/DeepSeek-R1<br>...<br><span class="hljs-meta prompt_">#</span><span class="language-bash">下载完成后，非蒸馏、满血版DeepSeek-R1:671B模型文件总大小共642G</span><br>(self-llm) deepseek@deepseek1:~/.cache/modelscope/hub/models/deepseek-ai/DeepSeek-R1$ du -sh ./      <br>642G    ./<br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">##失败原因：DeepSeek-R1:671B是fp8 模型，需要在支持fp8数据类型的GPU上运行，但NVIDIA A800（A800跟A100设计上基本上是一样的，只是减配了）不支持fp8数据类型。DeepSeek-R1 是 FP8 权重的，FP8 需要cuda架构不低于8.9，A800是8.0，所以需要将FP8转换成BF16</span></span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">##方法2（未验证）</span></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">DeepSeek-R1的BF16版本，但是GGUF文件：https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-BF16</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">总大小为：1341.32G</span><br>(self-llm) deepseek@deepseek1:~/installPkgs$ sudo parted /dev/nvme0n1 -s -- mklabel gpt mkpart DATA01 1 -1 <br>(self-llm) deepseek@deepseek1:~/installPkgs$ sudo mkfs.ext4 /dev/nvme0n1p1 <br>(self-llm) deepseek@deepseek1:~/installPkgs$ sudo mount /dev/nvme0n1p1 /mnt/<br>(self-llm) deepseek@deepseek1:~/installPkgs$ sudo tail -1 /etc/mtab<br>/dev/nvme0n1p1 /mnt ext4 rw,relatime,stripe=32 0 0<br><span class="hljs-meta prompt_">#</span><span class="language-bash">将上述输出内容写入/etc/fstab文件的末尾</span><br>(self-llm) deepseek@deepseek1:~/installPkgs$ sudo vi /etc/fstab<br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">##下载DeepSeek-R1模型的BF16版本</span></span><br>(self-llm) deepseek@deepseek1:~/installPkgs$ sudo /home/deepseek/.conda/envs/self-llm/bin/modelscope download --model unsloth/DeepSeek-R1-GGUF --local_dir /mnt<br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">##目前，vLLM 仅支持加载单文件 GGUF 模型。如果您有多文件的 GGUF 模型，可以使用 gguf-split 工具将其合并为一个单文件模型（参考文档：https://vllm.hyper.ai/docs/quantization/gguf/）</span></span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">##方法3（已经失败）</span></span><br>(self-llm) deepseek@deepseek1:~/installPkgs$ cd ~/code_repos/<br>(self-llm) deepseek@deepseek1:~/code_repos$ git clone https://github.com/deepseek-ai/DeepSeek-V3.git #或git clone https://gitee.com/sy-jiang/DeepSeek-V3.git<br>(self-llm) deepseek@deepseek1:~/code_repos$ cd DeepSeek-V3/<br>(self-llm) deepseek@deepseek1:~/code_repos$ sudo mkdir /mnt/DeepSeek-R1-bf16<br><span class="hljs-meta prompt_">#</span><span class="language-bash">权重转换， DeepSeek-R1 是 FP8 权重的，FP8 需要cuda架构不低于8.9，A800是8.0，所以需要将FP8转换成BF16。转换方法如下：</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">python fp8_cast_bf16.py --input-fp8-hf-path /path/to/DeepSeek-R1 --output-bf16-hf-path /path/to/deepseek-R1-bf16</span><br>(self-llm) deepseek@deepseek1:~/code_repos$ pip install torch==2.5.1<br>(self-llm) deepseek@deepseek1:~/code_repos$ pip install safetensors=0.5.2 numpy==1.26.4 tqdm==4.67.1 <br>(self-llm) deepseek@deepseek1:~/code_repos/DeepSeek-V3$ cd inference/<br>(self-llm) deepseek@deepseek1:~/code_repos/DeepSeek-V3$ sudo /home/deepseek/.conda/envs/self-llm/bin/python fp8_cast_bf16.py --input-fp8-hf-path /home/deepseek/.cache/modelscope/hub/models/deepseek-ai/DeepSeek-R1/ --output-bf16-hf-path /mnt/DeepSeek-R1-fp8_cast_bf16/<br><span class="hljs-meta prompt_">#</span><span class="language-bash">结果表明：运行这个fp8_cast_bf16.py文件，应该也是需要CUDA <span class="hljs-built_in">arch</span> &gt;=8.9的NVIDIA GPU。符合这种条件的GPU有NVIDIA H100（具体看问题2）</span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">##方法4（成功）</span></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">DeepSeek-R1的BF16版本，是.safetensors文件，在vllm中直接可用：https://modelscope.cn/models/unsloth/DeepSeek-R1-BF16/files</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">总大小为：1341.32G左右</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">此时deepseek1正在下载方法2中文件，故此方法的实施先在deepseek2上下载相关文件（下载过程中发现，deepseek1上的下载进程占据大量带宽影响deepseek2下载文件，故将deepseek1上的下载进程杀死）</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">/dev/nvme0n1  nvme硬盘，大小为3.5T</span><br>(self-llm) deepseek@deepseek2:~/installPkgs$ sudo parted /dev/nvme0n1 -s -- mklabel gpt mkpart DATA01 1 -1 <br>(self-llm) deepseek@deepseek2:~/installPkgs$ sudo mkfs.ext4 /dev/nvme0n1p1 <br>(self-llm) deepseek@deepseek2:~/installPkgs$ sudo mount /dev/nvme0n1p1 /mnt/<br>(self-llm) deepseek@deepseek2:~/installPkgs$ sudo tail -1 /etc/mtab<br>/dev/nvme0n1p1 /mnt ext4 rw,relatime,stripe=32 0 0<br><span class="hljs-meta prompt_">#</span><span class="language-bash">将上述输出内容写入/etc/fstab文件的末尾</span><br>(self-llm) deepseek@deepseek2:~/installPkgs$ sudo vi /etc/fstab<br>(self-llm) deepseek@deepseek2:~/installPkgs$ sudo rm -rf /mnt/lost+found<br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">##下载DeepSeek-R1模型的BF16版本</span></span><br>(self-llm) deepseek@deepseek2:~/installPkgs$ sudo mkdir -p hub/models/unsloth/DeepSeek-R1-BF16<br>(self-llm) deepseek@deepseek2:~/installPkgs$ sudo /home/deepseek/anaconda3/envs/self-llm/bin/modelscope download --model unsloth/DeepSeek-R1-BF16 --local_dir /mnt/hub/models/unsloth/DeepSeek-R1-BF16<br>Downloading Model to directory: /mnt/hub/models/unsloth/DeepSeek-R1-BF16<br>Downloading [config.json]: 100%|█████████████████████████████████████| 1.57k/1.57k [00:00&lt;00:00, 6.75kB/s]<br>...<br></code></pre></td></tr></table></figure>
<h3 id="报错与处理">2.1.1 报错与处理</h3>
<h4
id="问题1-方法3-执行python-fp8_cast_bf16.py提示usrbinld-cannot-find--lcuda-no-such-file-or-directory">问题1-方法3-执行"python
fp8_cast_bf16.py"提示“/usr/bin/ld: cannot find -lcuda: No such file or
directory”</h4>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs shell">(self-llm) deepseek@deepseek1:~/code_repos/DeepSeek-V3/inference$ sudo /home/deepseek/.conda/envs/self-llm/bin/python fp8_cast_bf16.py --input-fp8-hf-path /home/deepseek/.cache/modelscope/hub/models/deepseek-ai/DeepSeek-R1/ --output-bf16-hf-path /mnt/DeepSeek-R1-fp8_cast_bf16/<br><span class="hljs-meta prompt_">  0%</span><span class="language-bash">|                                                                                                                                                                                   | 0/163 [00:00&lt;?, ?it/s]/usr/bin/ld: cannot find -lcuda: No such file or directory</span><br>collect2: error: ld returned 1 exit status<br><span class="hljs-meta prompt_">  0%</span><span class="language-bash">|                                                                                                                                                                                   | 0/163 [00:01&lt;?, ?it/s]</span><br>Traceback (most recent call last):<br>  File &quot;/home/deepseek/code_repos/DeepSeek-V3/inference/fp8_cast_bf16.py&quot;, line 111, in &lt;module&gt;<br>    main(args.input_fp8_hf_path, args.output_bf16_hf_path)<br>  File &quot;/home/deepseek/code_repos/DeepSeek-V3/inference/fp8_cast_bf16.py&quot;, line 80, in main<br>    new_state_dict[weight_name] = weight_dequant(weight, scale_inv)<br>                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File &quot;/home/deepseek/code_repos/DeepSeek-V3/inference/kernel.py&quot;, line 104, in weight_dequant<br>    weight_dequant_kernel[grid](x, s, y, M, N, BLOCK_SIZE=block_size)<br>  File &quot;/home/deepseek/.conda/envs/self-llm/lib/python3.12/site-packages/triton/runtime/jit.py&quot;, line 345, in &lt;lambda&gt;<br>    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)<br>                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File &quot;/home/deepseek/.conda/envs/self-llm/lib/python3.12/site-packages/triton/runtime/jit.py&quot;, line 607, in run<br>    device = driver.active.get_current_device()<br>             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File &quot;/home/deepseek/.conda/envs/self-llm/lib/python3.12/site-packages/triton/runtime/driver.py&quot;, line 23, in __getattr__<br>    self._initialize_obj()<br>  File &quot;/home/deepseek/.conda/envs/self-llm/lib/python3.12/site-packages/triton/runtime/driver.py&quot;, line 20, in _initialize_obj<br>    self._obj = self._init_fn()<br>                ^^^^^^^^^^^^^^^<br>  File &quot;/home/deepseek/.conda/envs/self-llm/lib/python3.12/site-packages/triton/runtime/driver.py&quot;, line 9, in _create_driver<br>    return actives[0]()<br>           ^^^^^^^^^^^^<br>  File &quot;/home/deepseek/.conda/envs/self-llm/lib/python3.12/site-packages/triton/backends/nvidia/driver.py&quot;, line 371, in __init__<br>    self.utils = CudaUtils()  # TODO: make static<br>                 ^^^^^^^^^^^<br>  File &quot;/home/deepseek/.conda/envs/self-llm/lib/python3.12/site-packages/triton/backends/nvidia/driver.py&quot;, line 80, in __init__<br>    mod = compile_module_from_src(Path(os.path.join(dirname, &quot;driver.c&quot;)).read_text(), &quot;cuda_utils&quot;)<br>          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File &quot;/home/deepseek/.conda/envs/self-llm/lib/python3.12/site-packages/triton/backends/nvidia/driver.py&quot;, line 57, in compile_module_from_src<br>    so = _build(name, src_path, tmpdir, library_dirs(), include_dir, libraries)<br>         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File &quot;/home/deepseek/.conda/envs/self-llm/lib/python3.12/site-packages/triton/runtime/build.py&quot;, line 48, in _build<br>    ret = subprocess.check_call(cc_cmd)<br>          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File &quot;/home/deepseek/.conda/envs/self-llm/lib/python3.12/subprocess.py&quot;, line 415, in check_call<br>    raise CalledProcessError(retcode, cmd)<br>subprocess.CalledProcessError: Command &#x27;[&#x27;/usr/bin/gcc&#x27;, &#x27;/tmp/tmp2u69cogx/main.c&#x27;, &#x27;-O3&#x27;, &#x27;-shared&#x27;, &#x27;-fPIC&#x27;, &#x27;-o&#x27;, &#x27;/tmp/tmp2u69cogx/cuda_utils.cpython-312-x86_64-linux-gnu.so&#x27;, &#x27;-lcuda&#x27;, &#x27;-L/home/deepseek/.conda/envs/self-llm/lib/python3.12/site-packages/triton/backends/nvidia/lib&#x27;, &#x27;-L/lib/x86_64-linux-gnu&#x27;, &#x27;-I/home/deepseek/.conda/envs/self-llm/lib/python3.12/site-packages/triton/backends/nvidia/include&#x27;, &#x27;-I/tmp/tmp2u69cogx&#x27;, &#x27;-I/home/deepseek/.conda/envs/self-llm/include/python3.12&#x27;]&#x27; returned non-zero exit status 1.<br></code></pre></td></tr></table></figure>
<p>解决办法：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">参考https://developer.aliyun.com/article/377598</span><br>cd /home/deepseek/.conda/envs/self-llm/lib<br>ln -sv /usr/local/cuda-12.4/targets/x86_64-linux/lib/stubs/libcuda.so libcuda.so<br></code></pre></td></tr></table></figure>
<h4
id="问题2-方法3-执行python-fp8_cast_bf16.py提示assertionerror-fp8e4nv-data-type-is-not-supported-on-cuda-arch-89">问题2-方法3-执行"python
fp8_cast_bf16.py"提示“AssertionError: fp8e4nv data type is not supported
on CUDA arch &lt; 89”</h4>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">解决完上述问题1后，又出现如下问题2</span><br>(self-llm) deepseek@deepseek1:~/code_repos/DeepSeek-V3/inference$ sudo /home/deepseek/.conda/envs/self-llm/bin/python fp8_cast_bf16.py --input-fp8-hf-path /home/deepseek/.cache/modelscope/hub/models/deepseek-ai/DeepSeek-R1/ --output-bf16-hf-path /mnt/DeepSeek-R1-fp8_cast_bf16/<br><span class="hljs-meta prompt_">  0%</span><span class="language-bash">|                                                                                                                                                                                   | 0/163 [00:01&lt;?, ?it/s]</span><br>Traceback (most recent call last):<br>  File &quot;/home/deepseek/.conda/envs/self-llm/lib/python3.12/site-packages/triton/language/core.py&quot;, line 35, in wrapper<br>    return fn(*args, **kwargs)<br>           ^^^^^^^^^^^^^^^^^^^<br>  File &quot;/home/deepseek/.conda/envs/self-llm/lib/python3.12/site-packages/triton/language/core.py&quot;, line 993, in to<br>    return semantic.cast(self, dtype, _builder, fp_downcast_rounding)<br>           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File &quot;/home/deepseek/.conda/envs/self-llm/lib/python3.12/site-packages/triton/language/semantic.py&quot;, line 759, in cast<br>    assert builder.options.allow_fp8e4nv, &quot;fp8e4nv data type is not supported on CUDA arch &lt; 89&quot;<br>           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>AssertionError: fp8e4nv data type is not supported on CUDA arch &lt; 89<br><br>The above exception was the direct cause of the following exception:<br><br>Traceback (most recent call last):<br>  File &quot;/home/deepseek/code_repos/DeepSeek-V3/inference/fp8_cast_bf16.py&quot;, line 111, in &lt;module&gt;<br>    main(args.input_fp8_hf_path, args.output_bf16_hf_path)<br>  File &quot;/home/deepseek/code_repos/DeepSeek-V3/inference/fp8_cast_bf16.py&quot;, line 80, in main<br>    new_state_dict[weight_name] = weight_dequant(weight, scale_inv)<br>                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File &quot;/home/deepseek/code_repos/DeepSeek-V3/inference/kernel.py&quot;, line 104, in weight_dequant<br>    weight_dequant_kernel[grid](x, s, y, M, N, BLOCK_SIZE=block_size)<br>  File &quot;/home/deepseek/.conda/envs/self-llm/lib/python3.12/site-packages/triton/runtime/jit.py&quot;, line 345, in &lt;lambda&gt;<br>    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)<br>                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File &quot;/home/deepseek/.conda/envs/self-llm/lib/python3.12/site-packages/triton/runtime/jit.py&quot;, line 662, in run<br>    kernel = self.compile(<br>             ^^^^^^^^^^^^^<br>  File &quot;/home/deepseek/.conda/envs/self-llm/lib/python3.12/site-packages/triton/compiler/compiler.py&quot;, line 276, in compile<br>    module = src.make_ir(options, codegen_fns, context)<br>             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File &quot;/home/deepseek/.conda/envs/self-llm/lib/python3.12/site-packages/triton/compiler/compiler.py&quot;, line 113, in make_ir<br>    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns)<br>           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>triton.compiler.errors.CompilationError: at 23:8:<br><br>    Returns:<br>        None<br>    &quot;&quot;&quot;<br>    pid_m = tl.program_id(axis=0)<br>    pid_n = tl.program_id(axis=1)<br>    n = tl.cdiv(N, BLOCK_SIZE)<br>    offs_m = pid_m * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)<br>    offs_n = pid_n * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)<br>    offs = offs_m[:, None] * N + offs_n[None, :]<br>    mask = (offs_m[:, None] &lt; M) &amp; (offs_n[None, :] &lt; N)<br>    x = tl.load(x_ptr + offs, mask=mask).to(tl.float32)<br>        ^<br></code></pre></td></tr></table></figure>
<p>分析与解决办法：</p>
<p>运行这个fp8_cast_bf16.py文件，应该也是需要CUDA arch &gt;=8.9的NVIDIA
GPU。符合这种条件的GPU有NVIDIA H100。所以如果没有NVIDIA
H100，这种方法不可行</p>
<h2 id="查看服务器显卡">2.2 查看服务器/显卡</h2>
<p>4个物理服务器，每个物理服务器上的磁盘信息如下。系统盘是893G左右，其余还有6块893G
SATA盘、一块3.5T的nvme盘</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs shell">deepseek@deepseek1:~$ lsblk <br>NAME    MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS<br>loop0     7:0    0  63.9M  1 loop /snap/core20/2318<br>loop1     7:1    0    87M  1 loop /snap/lxd/28373<br>loop2     7:2    0  38.8M  1 loop /snap/snapd/21759<br>loop3     7:3    0  44.4M  1 loop /snap/snapd/23545<br>loop4     7:4    0  63.7M  1 loop /snap/core20/2496<br>loop5     7:5    0  89.4M  1 loop /snap/lxd/31333<br>sda       8:0    0 893.8G  0 disk <br>sdb       8:16   0 893.8G  0 disk <br>├─sdb1    8:17   0   550M  0 part /boot/efi<br>├─sdb2    8:18   0     8M  0 part <br>├─sdb3    8:19   0 893.1G  0 part /<br>└─sdb4    8:20   0    65M  0 part <br>sdc       8:32   0 893.8G  0 disk <br>sdd       8:48   0 893.8G  0 disk <br>sde       8:64   0 893.8G  0 disk <br>sdf       8:80   0 893.8G  0 disk <br>sdg       8:96   0 893.8G  0 disk <br>nvme0n1 259:0    0   3.5T  0 disk <br></code></pre></td></tr></table></figure>
<p>每个物理服务器上有8个NVIDIA A100-SXM4-80GB ，GPU型号、驱动版本、
CUDA版本信息如下。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs shell">deepseek@deepseek1:~$ nvidia-smi <br>Fri Feb 21 09:25:35 2025       <br>+-----------------------------------------------------------------------------------------+<br>| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |<br>|-----------------------------------------+------------------------+----------------------+<br>| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |<br>| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |<br>|                                         |                        |               MIG M. |<br>|=========================================+========================+======================|<br>|   0  NVIDIA A800-SXM4-80GB          On  |   00000000:3D:00.0 Off |                    0 |<br>| N/A   33C    P0             61W /  400W |       1MiB /  81920MiB |      0%      Default |<br>|                                         |                        |             Disabled |<br>+-----------------------------------------+------------------------+----------------------+<br>|   1  NVIDIA A800-SXM4-80GB          On  |   00000000:42:00.0 Off |                    0 |<br>| N/A   29C    P0             58W /  400W |       1MiB /  81920MiB |      0%      Default |<br>|                                         |                        |             Disabled |<br>+-----------------------------------------+------------------------+----------------------+<br>|   2  NVIDIA A800-SXM4-80GB          On  |   00000000:61:00.0 Off |                    0 |<br>| N/A   30C    P0             61W /  400W |       1MiB /  81920MiB |      0%      Default |<br>|                                         |                        |             Disabled |<br>+-----------------------------------------+------------------------+----------------------+<br>|   3  NVIDIA A800-SXM4-80GB          On  |   00000000:67:00.0 Off |                    0 |<br>| N/A   33C    P0             64W /  400W |       1MiB /  81920MiB |      0%      Default |<br>|                                         |                        |             Disabled |<br>+-----------------------------------------+------------------------+----------------------+<br>|   4  NVIDIA A800-SXM4-80GB          On  |   00000000:AD:00.0 Off |                    0 |<br>| N/A   32C    P0             57W /  400W |       1MiB /  81920MiB |      0%      Default |<br>|                                         |                        |             Disabled |<br>+-----------------------------------------+------------------------+----------------------+<br>|   5  NVIDIA A800-SXM4-80GB          On  |   00000000:B1:00.0 Off |                    0 |<br>| N/A   29C    P0             61W /  400W |       1MiB /  81920MiB |      0%      Default |<br>|                                         |                        |             Disabled |<br>+-----------------------------------------+------------------------+----------------------+<br>|   6  NVIDIA A800-SXM4-80GB          On  |   00000000:D0:00.0 Off |                    0 |<br>| N/A   30C    P0             62W /  400W |       1MiB /  81920MiB |      0%      Default |<br>|                                         |                        |             Disabled |<br>+-----------------------------------------+------------------------+----------------------+<br>|   7  NVIDIA A800-SXM4-80GB          On  |   00000000:D3:00.0 Off |                    0 |<br>| N/A   32C    P0             60W /  400W |       1MiB /  81920MiB |      0%      Default |<br>|                                         |                        |             Disabled |<br>+-----------------------------------------+------------------------+----------------------+<br>                                                                                         <br>+-----------------------------------------------------------------------------------------+<br>| Processes:                                                                              |<br>|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |<br>|        ID   ID                                                               Usage      |<br>|=========================================================================================|<br>|  No running processes found                                                             |<br>+-----------------------------------------------------------------------------------------+<br></code></pre></td></tr></table></figure>
<p>显卡驱动、cuda、以及nvidia-fabricmanager三者版本一定要相互匹配。</p>
<p>笔者显卡驱动、cuda、nvidia-fabricmanager版本信息分别是：550.90.07、V12.4.99、550.90.07
。</p>
<h2 id="安装cuda">2.3 安装cuda</h2>
<p>Cuda安装可参考NVIDIA官方文档：<a
target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/#ubuntu">cuda-installation-guide-linux</a></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">服务器拿到手时，发现已经安装了Cuda11.5.119</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">##如果要使用apt安装nvcc（此方式在Ubuntu22.04LTS上安装的是Cuda11.5.119）：sudo apt install nvidia-cuda-toolkit</span></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">查看现有nvcc版本</span><br>deepseek@deepseek1:~/installPkgs$ nvcc -V<br>nvcc: NVIDIA (R) Cuda compiler driver<br>Copyright (c) 2005-2021 NVIDIA Corporation<br>Built on Thu_Nov_18_09:45:30_PST_2021<br>Cuda compilation tools, release 11.5, V11.5.119<br>Build cuda_11.5.r11.5/compiler.30672275_0<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">##卸载现有的Cuda11.5.119：</span></span><br>deepseek@deepseek1:~/installPkgs$ sudo apt remove nvidia-cuda-toolkit<br></code></pre></td></tr></table></figure>
<p>我想改成使用Cuda12.4。访问<a
target="_blank" rel="noopener" href="https://developer.nvidia.com/cuda-12-4-0-download-archive">NVIDIA官网cuda安装文件下载网页</a>
，依次选择如下选项找到下载与安装命令：</p>
<figure>
<img src="https://s2.loli.net/2025/02/21/iv6oBq75Rr8EzDO.png" srcset="/img/loading.gif" lazyload
alt="image-20250221172033572" />
<figcaption aria-hidden="true">image-20250221172033572</figcaption>
</figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">下载安装文件</span><br>(self-llm) deepseek@deepseek1:~/installPkgs$ wget https://developer.download.nvidia.com/compute/cuda/12.4.0/local_installers/cuda_12.4.0_550.54.14_linux.run<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">执行安装</span><br>(self-llm) deepseek@deepseek1:~/installPkgs$ sudo sh cuda_12.4.0_550.54.14_linux.run<br><span class="hljs-meta prompt_">#</span><span class="language-bash">在弹出对话中输入“accept”接受条款，然后如下选择</span><br></code></pre></td></tr></table></figure>
<p><img src="https://s2.loli.net/2025/02/21/i8vKNVPkExZjD65.png" srcset="/img/loading.gif" lazyload alt="image-20250221173309528" style="zoom:50%;" /></p>
<figure>
<img src="https://s2.loli.net/2025/02/22/s8wCKUVEL2Sphna.png" srcset="/img/loading.gif" lazyload
alt="image-20250222162736130" />
<figcaption aria-hidden="true">image-20250222162736130</figcaption>
</figure>
<p>安装进程完成后，输出如下内容：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs shell">===========<br>= Summary =<br>===========<br><br>Driver:   Not Selected<br>Toolkit:  Installed in /usr/local/cuda-12.4/<br><br>Please make sure that<br> -   PATH includes /usr/local/cuda-12.4/bin<br> -   LD_LIBRARY_PATH includes /usr/local/cuda-12.4/lib64, or, add /usr/local/cuda-12.4/lib64 to /etc/ld.so.conf and run ldconfig as root<br><br>To uninstall the CUDA Toolkit, run cuda-uninstaller in /usr/local/cuda-12.4/bin<br>***WARNING: Incomplete installation! This installation did not install the CUDA Driver. A driver of version at least 550.00 is required for CUDA 12.4 functionality to work.<br>To install the driver using this installer, run the following command, replacing &lt;CudaInstaller&gt; with the name of this run file:<br>    sudo &lt;CudaInstaller&gt;.run --silent --driver<br><br>Logfile is /var/log/cuda-installer.log<br></code></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">发现系统已经创建软链接/usr/local/cuda，指向/usr/local/cuda-12.4/。后续使用此软链接</span><br>(self-llm) deepseek@deepseek1:~/installPkgs$ ls -al /usr/local/cuda<br>lrwxrwxrwx 1 root root 21 Feb 21 17:55 /usr/local/cuda -&gt; /usr/local/cuda-12.4/<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">按照提示修改系统环境变量PATH、LD_LIBRARY_PATH</span><br>(self-llm) deepseek@deepseek1:~/installPkgs$ sudo vi /etc/profile<br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">##在最后添加如下内容</span></span><br>export PATH=$PATH:/usr/local/cuda/bin/<br>export LD_LIBRARY_PATH=/usr/local/cuda/lib64/<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">重新打一个终端会话窗口或重新加载上述文件以启用上述两个修改或添加的系统环境变量</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">重新加载上述文件的方法：<span class="hljs-built_in">source</span> /etc/profile</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">以下以重新打一个终端会话窗口为例继续后续操作</span><br>(base) deepseek@deepseek1:~$ conda activate self-llm<br>(self-llm) deepseek@deepseek1:~$ echo $PATH<br>/home/deepseek/anaconda3/envs/self-llm/bin:/home/deepseek/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda/bin<br>(self-llm) deepseek@deepseek1:~$ echo $LD_LIBRARY_PATH  <br>/usr/local/cuda/lib64/<br><span class="hljs-meta prompt_">#</span><span class="language-bash">可以看到当前使用的Cuda版本已经是V12.4.99</span><br>(self-llm) deepseek@deepseek1:~$ nvcc -V<br>nvcc: NVIDIA (R) Cuda compiler driver<br>Copyright (c) 2005-2024 NVIDIA Corporation<br>Built on Tue_Feb_27_16:19:38_PST_2024<br>Cuda compilation tools, release 12.4, V12.4.99<br>Build cuda_12.4.r12.4/compiler.33961263_0<br>(self-llm) deepseek@deepseek1:~$ <br></code></pre></td></tr></table></figure>
<h2 id="单机多gpu卡间互联nvlink">2.4 单机多GPU卡间互联NVlink</h2>
<p>单机上的8个NVIDIA A800-SXM4-80GB之间使用NVLink进行了连接。与NVIDIA
A800所使用的Ampere架构相适配的，在NVIDIA
A800使用NVLink版本是NVLink3.0，NvSwitch版本是NvSwitch 3.0。</p>
<p>NVLink3.0有12条Link接口，每个Link接口有8个lane，NVLink3.0的每个Link接口单向通信速度是25GB/s，所以每个NVIDIA
A800使用NVLink3.0时，单向宽带是 25GB/s * 12 =
300GB/s，双向宽带在单向宽带的基础上乘以2即为600GB/s。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><code class="hljs shell">deepseek@deepseek1:~$ nvidia-smi nvlink --status<br>GPU 0: NVIDIA A800-SXM4-80GB (UUID: GPU-f275597c-05e4-f7e8-35bc-a3ab26194262)<br>         Link 0: 25 GB/s<br>         Link 1: 25 GB/s<br>         Link 2: 25 GB/s<br>         Link 3: 25 GB/s<br>         Link 4: 25 GB/s<br>         Link 5: 25 GB/s<br>         Link 6: 25 GB/s<br>         Link 7: 25 GB/s<br>GPU 1: NVIDIA A800-SXM4-80GB (UUID: GPU-56afa4fb-5618-6b47-7861-51c811bb87d8)<br>         Link 0: 25 GB/s<br>         Link 1: 25 GB/s<br>         Link 2: 25 GB/s<br>         Link 3: 25 GB/s<br>         Link 4: 25 GB/s<br>         Link 5: 25 GB/s<br>         Link 6: 25 GB/s<br>         Link 7: 25 GB/s<br>GPU 2: NVIDIA A800-SXM4-80GB (UUID: GPU-9039f623-9c6f-bc1a-f7d4-fd706a7cd7f5)<br>         Link 0: 25 GB/s<br>         Link 1: 25 GB/s<br>         Link 2: 25 GB/s<br>         Link 3: 25 GB/s<br>         Link 4: 25 GB/s<br>         Link 5: 25 GB/s<br>         Link 6: 25 GB/s<br>         Link 7: 25 GB/s<br>GPU 3: NVIDIA A800-SXM4-80GB (UUID: GPU-3029201c-2230-e6d4-290b-267c7c8adb03)<br>         Link 0: 25 GB/s<br>         Link 1: 25 GB/s<br>         Link 2: 25 GB/s<br>         Link 3: 25 GB/s<br>         Link 4: 25 GB/s<br>         Link 5: 25 GB/s<br>         Link 6: 25 GB/s<br>         Link 7: 25 GB/s<br>GPU 4: NVIDIA A800-SXM4-80GB (UUID: GPU-54d02ccd-6b42-8ec6-1b2a-624974742a62)<br>         Link 0: 25 GB/s<br>         Link 1: 25 GB/s<br>         Link 2: 25 GB/s<br>         Link 3: 25 GB/s<br>         Link 4: 25 GB/s<br>         Link 5: 25 GB/s<br>         Link 6: 25 GB/s<br>         Link 7: 25 GB/s<br>GPU 5: NVIDIA A800-SXM4-80GB (UUID: GPU-13254428-498a-5ec1-5dd5-d2c46dd29c36)<br>         Link 0: 25 GB/s<br>         Link 1: 25 GB/s<br>         Link 2: 25 GB/s<br>         Link 3: 25 GB/s<br>         Link 4: 25 GB/s<br>         Link 5: 25 GB/s<br>         Link 6: 25 GB/s<br>         Link 7: 25 GB/s<br>GPU 6: NVIDIA A800-SXM4-80GB (UUID: GPU-5442c93d-d690-3603-f0c3-c0592cf3797c)<br>         Link 0: 25 GB/s<br>         Link 1: 25 GB/s<br>         Link 2: 25 GB/s<br>         Link 3: 25 GB/s<br>         Link 4: 25 GB/s<br>         Link 5: 25 GB/s<br>         Link 6: 25 GB/s<br>         Link 7: 25 GB/s<br>GPU 7: NVIDIA A800-SXM4-80GB (UUID: GPU-bfe7f980-f583-3778-9779-85c7ebbb9432)<br>         Link 0: 25 GB/s<br>         Link 1: 25 GB/s<br>         Link 2: 25 GB/s<br>         Link 3: 25 GB/s<br>         Link 4: 25 GB/s<br>         Link 5: 25 GB/s<br>         Link 6: 25 GB/s<br>         Link 7: 25 GB/s<br></code></pre></td></tr></table></figure>
<h2 id="nvidia-fabricmanager安装与确认">2.5
nvidia-fabricmanager安装与确认</h2>
<h3 id="安装nvidia-fabricmanager">2.5.1 安装nvidia-fabricmanager</h3>
<p>A800 GPU 并支持 <strong>NvLink &amp;
NvSwitch</strong>，需额外安装与驱动版本对应的 nvidia-fabricmanager
服务使 GPU 卡间能够互联。安装 nvidia-fabricmanager 服务后都能正常使用
GPU 实例，安装方法如下。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">Install the new cuda-keyring package（将创建文件/etc/apt/sources.list.d/cuda-ubuntu2204-x86_64.list ）</span><br>wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb<br>sudo dpkg -i cuda-keyring_1.1-1_all.deb<br></code></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">安装nvidia-fabricmanager-550=550.90.07-1</span><br>version=550.90.07<br>main_version=$(echo $version | awk -F &#x27;.&#x27; &#x27;&#123;print $1&#125;&#x27;)<br>sudo apt-get update<br>sudo apt-get -y install nvidia-fabricmanager-$&#123;main_version&#125;=$&#123;version&#125;-*<br><span class="hljs-meta prompt_">#</span><span class="language-bash">卸载不再需要的组件</span><br>sudo apt autoremove<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">查看已经安装的nvidia-fabricmanager版本</span><br>deepseek@deepseek1:~$ dpkg -l | grep nvidia-fabricmanager<br>ii  nvidia-fabricmanager-550               550.90.07-1                                              amd64        Fabric Manager for NVSwitch based systems.<br>deepseek@deepseek1:~$ nv-fabricmanager -v<br>Fabric Manager version is : 550.90.07<br></code></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">nvidia-fabricmanager 服务开机自启动、启动</span><br>sudo systemctl enable nvidia-fabricmanager<br>sudo systemctl start nvidia-fabricmanager.service<br><span class="hljs-meta prompt_">#</span><span class="language-bash">查看 nvidia-fabricmanager 服务</span><br>sudo systemctl status nvidia-fabricmanager<br></code></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">可以看到“已成功配置所有可用的gpu和nvswitch以路由NVLink流量”</span><br>(self-llm) deepseek@deepseek1:~/installPkgs$ sudo systemctl status nvidia-fabricmanager<br>● nvidia-fabricmanager.service - NVIDIA fabric manager service<br>     Loaded: loaded (/lib/systemd/system/nvidia-fabricmanager.service; enabled; vendor preset: enabled)<br>     Active: active (running) since Thu 2025-02-20 18:11:52 CST; 21h ago<br>   Main PID: 3697 (nv-fabricmanage)<br>      Tasks: 19 (limit: 629145)<br>     Memory: 22.4M<br>        CPU: 36.002s<br>     CGroup: /system.slice/nvidia-fabricmanager.service<br>             └─3697 /usr/bin/nv-fabricmanager -c /usr/share/nvidia/nvswitch/fabricmanager.cfg<br><br>Feb 20 18:11:40 deepseek1 systemd[1]: Starting NVIDIA fabric manager service...<br>Feb 20 18:11:41 deepseek1 nv-fabricmanager[3697]: Connected to 1 node.<br>Feb 20 18:11:52 deepseek1 nv-fabricmanager[3697]: Successfully configured all the available GPUs and NVSwitches to route NVLink traffic.<br>Feb 20 18:11:52 deepseek1 systemd[1]: Started NVIDIA fabric manager service.<br></code></pre></td></tr></table></figure>
<h3 id="查看显卡拓扑结构">2.5.2 查看显卡拓扑结构</h3>
<figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs crmsh">deepseek@deepseek1:~/installPkgs$ nvidia-smi topo --matrix<br>        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    CPU Affinity    NUMA Affinity   GPU NUMA ID<br>GPU0     X      NV8     NV8     NV8     NV8     NV8     NV8     NV8     PXB     <span class="hljs-keyword">NODE</span>    <span class="hljs-title">SYS</span>     SYS     <span class="hljs-keyword">NODE</span>    <span class="hljs-title">0-27</span>,<span class="hljs-number">56</span>-<span class="hljs-number">83</span>      <span class="hljs-number">0</span>               N/A<br>GPU1    NV8      X      NV8     NV8     NV8     NV8     NV8     NV8     PXB     <span class="hljs-keyword">NODE</span>    <span class="hljs-title">SYS</span>     SYS     <span class="hljs-keyword">NODE</span>    <span class="hljs-title">0-27</span>,<span class="hljs-number">56</span>-<span class="hljs-number">83</span>      <span class="hljs-number">0</span>               N/A<br>GPU2    NV8     NV8      X      NV8     NV8     NV8     NV8     NV8     <span class="hljs-keyword">NODE</span>    <span class="hljs-title">PXB</span>     SYS     SYS     <span class="hljs-keyword">NODE</span>    <span class="hljs-title">0-27</span>,<span class="hljs-number">56</span>-<span class="hljs-number">83</span>      <span class="hljs-number">0</span>               N/A<br>GPU3    NV8     NV8     NV8      X      NV8     NV8     NV8     NV8     <span class="hljs-keyword">NODE</span>    <span class="hljs-title">PXB</span>     SYS     SYS     <span class="hljs-keyword">NODE</span>    <span class="hljs-title">0-27</span>,<span class="hljs-number">56</span>-<span class="hljs-number">83</span>      <span class="hljs-number">0</span>               N/A<br>GPU4    NV8     NV8     NV8     NV8      X      NV8     NV8     NV8     SYS     SYS     PXB     <span class="hljs-keyword">NODE</span>    <span class="hljs-title">SYS</span>     <span class="hljs-number">28</span>-<span class="hljs-number">55</span>,<span class="hljs-number">84</span>-<span class="hljs-number">111</span>    <span class="hljs-number">1</span>               N/A<br>GPU5    NV8     NV8     NV8     NV8     NV8      X      NV8     NV8     SYS     SYS     PXB     <span class="hljs-keyword">NODE</span>    <span class="hljs-title">SYS</span>     <span class="hljs-number">28</span>-<span class="hljs-number">55</span>,<span class="hljs-number">84</span>-<span class="hljs-number">111</span>    <span class="hljs-number">1</span>               N/A<br>GPU6    NV8     NV8     NV8     NV8     NV8     NV8      X      NV8     SYS     SYS     <span class="hljs-keyword">NODE</span>    <span class="hljs-title">PXB</span>     SYS     <span class="hljs-number">28</span>-<span class="hljs-number">55</span>,<span class="hljs-number">84</span>-<span class="hljs-number">111</span>    <span class="hljs-number">1</span>               N/A<br>GPU7    NV8     NV8     NV8     NV8     NV8     NV8     NV8      X      SYS     SYS     <span class="hljs-keyword">NODE</span>    <span class="hljs-title">PXB</span>     SYS     <span class="hljs-number">28</span>-<span class="hljs-number">55</span>,<span class="hljs-number">84</span>-<span class="hljs-number">111</span>    <span class="hljs-number">1</span>               N/A<br>NIC0    PXB     PXB     <span class="hljs-keyword">NODE</span>    <span class="hljs-title">NODE</span>    SYS     SYS     SYS     SYS      X      <span class="hljs-keyword">NODE</span>    <span class="hljs-title">SYS</span>     SYS     <span class="hljs-keyword">NODE</span><br><span class="hljs-title">NIC1</span>    <span class="hljs-keyword">NODE</span>    <span class="hljs-title">NODE</span>    PXB     PXB     SYS     SYS     SYS     SYS     <span class="hljs-keyword">NODE</span>     <span class="hljs-title">X</span>      SYS     SYS     <span class="hljs-keyword">NODE</span><br><span class="hljs-title">NIC2</span>    SYS     SYS     SYS     SYS     PXB     PXB     <span class="hljs-keyword">NODE</span>    <span class="hljs-title">NODE</span>    SYS     SYS      X      <span class="hljs-keyword">NODE</span>    <span class="hljs-title">SYS</span><br>NIC3    SYS     SYS     SYS     SYS     <span class="hljs-keyword">NODE</span>    <span class="hljs-title">NODE</span>    PXB     PXB     SYS     SYS     <span class="hljs-keyword">NODE</span>     <span class="hljs-title">X</span>      SYS<br>NIC4    <span class="hljs-keyword">NODE</span>    <span class="hljs-title">NODE</span>    <span class="hljs-keyword">NODE</span>    <span class="hljs-title">NODE</span>    SYS     SYS     SYS     SYS     <span class="hljs-keyword">NODE</span>    <span class="hljs-title">NODE</span>    SYS     SYS      X <br><br>Legend:<br><br>  X    = Self<br>  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)<br>  <span class="hljs-keyword">NODE</span> <span class="hljs-title">= Connection</span> traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA <span class="hljs-keyword">node</span><br>  <span class="hljs-title">PHB</span>  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)<br>  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)<br>  PIX  = Connection traversing at most a single PCIe bridge<br>  NV<span class="hljs-comment">#  = Connection traversing a bonded set of # NVLinks</span><br><br>NIC Legend:<br><br>  NIC0: mlx5_2<br>  NIC1: mlx5_3<br>  NIC2: mlx5_4<br>  NIC3: mlx5_5<br>  NIC4: mlx5_bond_0<br></code></pre></td></tr></table></figure>
<p><strong>可以看到GPU0与其他GPU如GPU1之间是通过NV</strong></p>
<h2 id="超高性能网络-infiniband">2.6 超高性能网络-Infiniband</h2>
<p>InfiniBand（以下简称IB网络），是一种能力很强的通信技术协议。它的英文直译过来，就是“无限带宽”。
它是一种专为高性能计算（HPC）和超大规模数据中心设计的网络技术，以亚微秒级超低延迟和超高带宽为核心优势。</p>
<p>IB网络因其低延迟、高带宽的网络特性被用于很多高性能计算（High
Performance Computing，HPC）项目，IB网络采用了100G Mellanox
IB网卡，通过专用IB交换机和控制器软件UFM实现网络通信和管理。IB网络通过Partition
Key实现网络隔离，不同租户的IB网络可通过不同的Partition
Key来隔离，类似于以太网的VLAN。在BMS场景，IB网络支持RDMA和IPoIB通信方式。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">首先查看是否具有 Infiniband controller</span><br>(self-llm) deepseek@deepseek1:~/installPkgs$ lspci | grep -i infiniband<br>44:00.0 Infiniband controller: Mellanox Technologies MT28908 Family [ConnectX-6]<br>68:00.0 Infiniband controller: Mellanox Technologies MT28908 Family [ConnectX-6]<br>c2:00.0 Infiniband controller: Mellanox Technologies MT28908 Family [ConnectX-6]<br>db:00.0 Infiniband controller: Mellanox Technologies MT28908 Family [ConnectX-6]<br></code></pre></td></tr></table></figure>
<p>在保证系统上安装且启用了IB网卡的情况下，使用命令<code>ibdev2netdev -v</code>可以看到IB网口的状态。</p>
<p>可以看到最后有一个bond1，是IB网卡做的bond1，就用它了。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">device和网口，mlx5_2是device名称，ibp68s0是网口名称</span><br>(self-llm) deepseek@deepseek2:~$ sudo ibdev2netdev -v<br>0000:44:00.0 mlx5_2 (MT4123 - MCX653105A-HDAT) ConnectX-6 VPI adapter card, HDR IB (200Gb/s) and 200GbE, single-port QSFP56                                                                                                           fw 20.35.3006 port 1 (ACTIVE) ==&gt; ibp68s0 (Up)<br>0000:68:00.0 mlx5_3 (MT4123 - MCX653105A-HDAT) ConnectX-6 VPI adapter card, HDR IB (200Gb/s) and 200GbE, single-port QSFP56                                                                                                           fw 20.35.3006 port 1 (ACTIVE) ==&gt; ibp104s0 (Up)<br>0000:c2:00.0 mlx5_4 (MT4123 - MCX653105A-HDAT) ConnectX-6 VPI adapter card, HDR IB (200Gb/s) and 200GbE, single-port QSFP56                                                                                                           fw 20.35.3006 port 1 (ACTIVE) ==&gt; ibp194s0 (Up)<br>0000:db:00.0 mlx5_5 (MT4123 - MCX653105A-HDAT) ConnectX-6 VPI adapter card, HDR IB (200Gb/s) and 200GbE, single-port QSFP56                                                                                                           fw 20.35.3006 port 1 (ACTIVE) ==&gt; ibp219s0 (Up)<br>0000:17:00.0 mlx5_bond_0 (MT4119 - MCX562A-ACAB) ConnectX-5 EN network interface card for OCP 3.0, with host management, 25GbE Dual-port SFP28, Pull Tab bracket                                                                        fw 16.35.3006 port 1 (ACTIVE) ==&gt; bond1 (Up)<br></code></pre></td></tr></table></figure>
<h2 id="推理引擎准备">2.7 推理引擎准备</h2>
<p>此处使用的推理引擎是vLLM，vLLM是一个快速易用的LLM推理和服务库，旨在提高模型运行的效率和性能。它通过内存优化和推理加速技术，使得在资源有限的环境下也能高效运行大型语言模型。它跟ollama是同一类开源产品。</p>
<p>从<a
target="_blank" rel="noopener" href="https://docs.vllm.ai/en/latest/getting_started/installation/gpu/index.html">官方部署vLLM文档</a>上看，vLLM支持Python与Docker两种安装方式。此文档在多个服务器上以集群的方式运行vLLM进程，为了保证
所有服务器上执行环境如模型文件所在目录、Python环境等的一致性，此处通过Docker的方式安装vLLM与运行vLLM。</p>
<h3 id="安装docker环境">2.7.1 安装Docker环境</h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo apt-get update<br><span class="hljs-meta prompt_">#</span><span class="language-bash">安装 apt 依赖包，用于通过HTTPS来获取仓库</span><br>sudo apt-get -qy install apt-transport-https ca-certificates curl gnupg-agent software-properties-common<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">添加 Docker 的官方 GPG 密钥</span><br>curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -<br><span class="hljs-meta prompt_">#</span><span class="language-bash">上述命名会提示“gpg: no valid OpenPGP data found.”，其中一种解决办法如下：</span><br>sudo gpg --keyserver keyserver.ubuntu.com --recv 7EA0A9C3F273FCD8<br>sudo gpg --export --armor 7EA0A9C3F273FCD8 |sudo apt-key add -<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">设置稳定版仓库(添加到/etc/apt/sources.list中)</span><br>sudo add-apt-repository &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable&quot;<br><br>sudo apt-get update<br><span class="hljs-meta prompt_">#</span><span class="language-bash">查询可安装docker-ce版本</span><br>sudo apt-cache policy docker-ce<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">安装指定版本</span><br>sudo apt-get -qy install docker-ce=5:28.0.0-1~ubuntu.22.04~jammy<br><span class="hljs-meta prompt_">#</span><span class="language-bash">查看docker版本</span><br>sudo docker --version<br></code></pre></td></tr></table></figure>
<h3 id="配置docker使用nvidia-gpu">2.7.2 配置Docker使用NVIDIA GPU</h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">设置 repository 和 GPG key</span><br>distribution=$(. /etc/os-release;echo $ID$VERSION_ID)<br>curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -<br>curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list<br></code></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">安装 nvidia-docker2</span><br>sudo apt-get update<br>sudo apt-get install -y nvidia-docker2<br></code></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">修改docker守护进程配置文件/etc/docker/daemon.json</span> <br>(self-llm) deepseek@deepseek1:~/installPkgs$ sudo vi /etc/docker/daemon.json <br>&#123;<br>  &quot;default-runtime&quot;: &quot;nvidia&quot;,<br>  &quot;runtimes&quot;: &#123;<br>    &quot;nvidia&quot;: &#123;<br>      &quot;path&quot;: &quot;/usr/bin/nvidia-container-runtime&quot;,<br>      &quot;runtimeArgs&quot;: []<br>    &#125; <br>  &#125;,<br>  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],<br>  &quot;dns&quot;: [<br>    &quot;223.5.5.5&quot;,<br>    &quot;223.6.6.6&quot;,<br>    &quot;8.8.8.8&quot;<br>  ],<br>  &quot;log-opts&quot;: &#123;<br>    &quot;max-file&quot;: &quot;5&quot;,<br>    &quot;max-size&quot;: &quot;50m&quot;<br>  &#125;,<br>  &quot;registry-mirrors&quot;: [<br>    &quot;https://registry.aliyuncs.com&quot;,<br>    &quot;https://registry.docker-cn.com&quot;,<br>    &quot;https://docker.chenby.cn&quot;,<br>    &quot;https://docker.registry.cyou&quot;,<br>    &quot;https://docker-cf.registry.cyou&quot;,<br>    &quot;https://dockercf.jsdelivr.fyi&quot;,<br>    &quot;https://docker.jsdelivr.fyi&quot;,<br>    &quot;https://dockertest.jsdelivr.fyi&quot;,<br>    &quot;https://dockerproxy.com&quot;,<br>    &quot;https://docker.m.daocloud.io&quot;,<br>    &quot;https://docker.nju.edu.cn&quot;,<br>    &quot;https://docker.mirrors.sjtug.sjtu.edu.cn&quot;,<br>    &quot;https://docker.mirrors.ustc.edu.cn&quot;,<br>    &quot;https://mirror.iscas.ac.cn&quot;,<br>    &quot;https://docker.rainbond.cc&quot;<br>  ] <br>&#125; <br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">重新加载此文件，重启docker进程</span><br>(self-llm) deepseek@deepseek1:~/installPkgs$ sudo systemctl daemon-reload &amp;&amp; sudo systemctl restart docker<br></code></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">查看docker详细配置信息</span><br>(self-llm) deepseek@deepseek1:~/installPkgs$ sudo docker info<br>Client: Docker Engine - Community<br> Version:    28.0.0<br> Context:    default<br> Debug Mode: false<br><br>Server:<br> Containers: 0<br>  Running: 0<br>  Paused: 0<br>  Stopped: 0<br> Images: 0<br> Server Version: 26.1.4<br> Storage Driver: overlay2<br>  Backing Filesystem: extfs<br>  Supports d_type: true<br>  Using metacopy: false<br>  Native Overlay Diff: true<br>  userxattr: false<br> Logging Driver: json-file<br> Cgroup Driver: systemd<br> Cgroup Version: 2<br> Plugins:<br>  Volume: local<br>  Network: bridge host ipvlan macvlan null overlay<br>  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog<br> Swarm: inactive<br> Runtimes: io.containerd.runc.v2 nvidia runc<br> Default Runtime: nvidia<br> ...<br></code></pre></td></tr></table></figure>
<h3 id="拉取vllm-openai镜像">2.7.3 拉取vllm-openai镜像</h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">此处不使用dockerhub上的镜像，而是使用阿里云上的镜像</span><br>(self-llm) deepseek@deepseek1:~/installPkgs$ sudo docker pull swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/vllm/vllm-openai:v0.7.2<br>(self-llm) deepseek@deepseek1:~/installPkgs$ sudo docker images<br>REPOSITORY                                                            TAG       IMAGE ID       CREATED       SIZE<br>swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/vllm/vllm-openai   v0.7.2    f78c8f2f8ad5   2 weeks ago   16.5GB<br></code></pre></td></tr></table></figure>
<h3 id="下载vllm代码仓库">2.7.4 下载vllm代码仓库</h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">(self-llm) deepseek@deepseek1:~$ mkdir code_repos<br>(self-llm) deepseek@deepseek1:~$ cd code_repos/<br>(self-llm) deepseek@deepseek1:~/code_repos$ git clone https://github.com/vllm-project/vllm.git<br>(self-llm) deepseek@deepseek1:~/code_repos$ cd vllm/<br></code></pre></td></tr></table></figure>
<h1 id="三正式部署验证与使用">三、正式部署验证与使用</h1>
<h2 id="运行vllm推理引擎与验证">3.1 运行vLLM推理引擎与验证</h2>
<p>参考：<a
target="_blank" rel="noopener" href="https://docs.vllm.ai/en/latest/serving/distributed_serving.html#running-vllm-on-multiple-nodes">Running
vLLM on multiple nodes</a></p>
<h3 id="在各节点启动node容器">3.1.1 在各节点启动node容器</h3>
<p>在各节点启动node容器，以创建ray集群或向其中添加节点。</p>
<h4 id="vllm集群主节点">vllm集群主节点</h4>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">对于vllm集群主节点，操作如下</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">##启动集群主节点上的进程命令如下</span></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">bash run_cluster.sh \</span><br><span class="language-bash"><span class="hljs-comment">#    vllm/vllm-openai \</span></span><br><span class="language-bash"><span class="hljs-comment">#    ip_of_head_node \</span></span><br><span class="language-bash"><span class="hljs-comment">#    --head \</span></span><br><span class="language-bash"><span class="hljs-comment">#    /path/to/the/huggingface/home/in/this/node \</span></span><br><span class="language-bash"><span class="hljs-comment">#    -e VLLM_HOST_IP=ip_of_this_node</span></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">以下命令执行会在当前shell中启动子进程且永远阻塞，需要保证此shell进程不被关闭或杀死，如果某个shell进程被关闭或杀死（针对启动vllm集群的每个节点上的shell进程而言）将导致ray集群异常</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">保证/mnt目录有如下目录：hub/models/deepseek-ai/DeepSeek-R1-BP16/，此目录下放置从modelscope下载下来的DeepSeek-R1-BP16模型仓库中所有文件</span><br>(self-llm) deepseek@deepseek1:~/code_repos$ sudo bash examples/online_serving/run_cluster.sh \<br>    swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/vllm/vllm-openai:v0.7.2 \<br>    10.119.85.141 \<br>    --head \<br>    /mnt \<br>    --cap-add SYS_ADMIN \<br>    -e VLLM_HOST_IP=10.119.85.141 \<br>    --privileged -e NCCL_IB_HCA=mlx5 \<br>    -e NCCL_P2P_LEVEL=NVL \<br>    -e NCCL_IB_GID_INDEX=3 \<br>    -e NCCL_IB_DISABLE=0 \<br>    -e NCCL_DEBUG=INFO \<br>    -e NCCL_SOCKET_IFNAME=bond1<br></code></pre></td></tr></table></figure>
<p>其中相关部分参数的解释如下（可参考官网：https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/index.html）：</p>
<ul>
<li><p>NCCL_IB_HCA</p></li>
<li><p>NCCL_P2P_LEVEL</p></li>
</ul>
<p>​
允许用户精细控制何时在gpu之间使用点对点（P2P）传输。值是一个枚举类型，有LOC
、NVL 等</p>
<ul>
<li>NCCL_IB_GID_INDEX</li>
</ul>
<p>​ 定义了RoCE模式中使用的全局ID索引。可选值如下。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs shell">(self-llm) deepseek@deepseek3:~$ show_gids              <br>DEV     PORT    INDEX   GID                                     IPv4            VER     DEV<br>---     ----    -----   ---                                     ------------    ---     ---<br>mlx5_2  1       0       fe80:0000:0000:0000:a088:c203:0014:e6e2                 v1<br>mlx5_3  1       0       fe80:0000:0000:0000:a088:c203:0014:e7ea                 v1<br>mlx5_4  1       0       fe80:0000:0000:0000:a088:c203:0014:e9f6                 v1<br>mlx5_5  1       0       fe80:0000:0000:0000:946d:ae03:00ed:119e                 v1<br>mlx5_bond_0     1       0       fe80:0000:0000:0000:1270:fdff:fec2:6294                 v1      bond1<br>mlx5_bond_0     1       1       fe80:0000:0000:0000:1270:fdff:fec2:6294                 v2      bond1<br>mlx5_bond_0     1       2       0000:0000:0000:0000:0000:ffff:0a77:558c 10.119.85.140   v1      bond1<br>mlx5_bond_0     1       3       0000:0000:0000:0000:0000:ffff:0a77:558c 10.119.85.140   v2      bond1<br>n_gids_found=8<br></code></pre></td></tr></table></figure>
<ul>
<li>NCCL_IB_DISABLE</li>
<li>NCCL_DEBUG</li>
<li>NCCL_SOCKET_IFNAME</li>
</ul>
<p>​
指定用于通信的IP接口。参考：https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/env.html#nccl-socket-ifname</p>
<ul>
<li>NCCL_NET_GDR_LEVEL</li>
</ul>
<p>​ 允许用户精细控制何时在网卡和GPU之间使用GPU直接RDMA。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">成功运行后，输出大致如下</span><br>2025-02-23 22:04:33,124 INFO usage_lib.py:467 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected to be non-interactive. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.<br>2025-02-23 22:04:33,125 INFO scripts.py:865 -- Local node IP: 10.119.165.139<br>2025-02-23 22:04:34,147 SUCC scripts.py:902 -- --------------------<br>2025-02-23 22:04:34,147 SUCC scripts.py:903 -- Ray runtime started.<br>2025-02-23 22:04:34,147 SUCC scripts.py:904 -- --------------------<br>2025-02-23 22:04:34,147 INFO scripts.py:906 -- Next steps<br>2025-02-23 22:04:34,148 INFO scripts.py:909 -- To add another node to this Ray cluster, run<br>2025-02-23 22:04:34,148 INFO scripts.py:912 --   ray start --address=&#x27;10.119.165.139:6379&#x27;<br>2025-02-23 22:04:34,148 INFO scripts.py:921 -- To connect to this Ray cluster:<br>2025-02-23 22:04:34,148 INFO scripts.py:923 -- import ray<br>2025-02-23 22:04:34,148 INFO scripts.py:924 -- ray.init()<br>2025-02-23 22:04:34,148 INFO scripts.py:936 -- To submit a Ray job using the Ray Jobs CLI:<br>2025-02-23 22:04:34,148 INFO scripts.py:937 --   RAY_ADDRESS=&#x27;http://127.0.0.1:8265&#x27; ray job submit --working-dir . -- python my_script.py<br>2025-02-23 22:04:34,148 INFO scripts.py:946 -- See https://docs.ray.io/en/latest/cluster/running-applications/job-submission/index.html <br>2025-02-23 22:04:34,148 INFO scripts.py:950 -- for more information on submitting Ray jobs to the Ray cluster.<br>2025-02-23 22:04:34,148 INFO scripts.py:955 -- To terminate the Ray runtime, run<br>2025-02-23 22:04:34,148 INFO scripts.py:956 --   ray stop<br>2025-02-23 22:04:34,148 INFO scripts.py:959 -- To view the status of the cluster, use<br>2025-02-23 22:04:34,148 INFO scripts.py:960 --   ray status<br>2025-02-23 22:04:34,148 INFO scripts.py:964 -- To monitor and debug Ray, view the dashboard at <br>2025-02-23 22:04:34,148 INFO scripts.py:965 --   127.0.0.1:8265<br>2025-02-23 22:04:34,148 INFO scripts.py:972 -- If connection to the dashboard fails, check your firewall settings and network configuration.<br>2025-02-23 22:04:34,148 INFO scripts.py:1076 -- --block<br>2025-02-23 22:04:34,148 INFO scripts.py:1077 -- This command will now block forever until terminated by a signal.<br>2025-02-23 22:04:34,148 INFO scripts.py:1080 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.<br></code></pre></td></tr></table></figure>
<h4 id="vllm进程集群副节点">vllm进程集群副节点</h4>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">对于vllm进程集群副节点，操作如下</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">此次部署时，一共有4个服务器。deepseek1是主节点，deepseek2、deepseek3、deepseek4都是副节点</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">所以deepseek3、deepseek4上也需要执行如下操作</span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">##启动集群副节点上的进程命令如下</span></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">bash run_cluster.sh \</span><br><span class="language-bash"><span class="hljs-comment">#    vllm/vllm-openai \</span></span><br><span class="language-bash"><span class="hljs-comment">#    ip_of_head_node \</span></span><br><span class="language-bash"><span class="hljs-comment">#    --worker \</span></span><br><span class="language-bash"><span class="hljs-comment">#    /path/to/the/huggingface/home/in/this/node \</span></span><br><span class="language-bash"><span class="hljs-comment">#    -e VLLM_HOST_IP=ip_of_this_node</span></span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">以下命令执行会在当前shell中启动子进程且永远阻塞，需要保证此shell进程不被关闭或杀死，如果某个shell进程被关闭或杀死（针对启动vllm集群的每个节点上的shell进程而言）将导致ray集群不符合预期</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">以下命令中注意修改VLLM_HOST_IP的值为具体节点的IP</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">#10000M network</span></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">10.119.165.139 deepseek1</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">10.119.165.138 deepseek2</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">10.119.165.140 deepseek3</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">10.119.165.141 deepseek4</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">#IB network</span></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">10.119.85.141 deepseek1</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">10.119.85.138 deepseek2</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">10.119.85.140 deepseek3</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">10.119.85.139 deepseek4</span><br>(self-llm) deepseek@deepseek2:~/code_repos/vllm$ sudo bash examples/online_serving/run_cluster.sh \<br>    swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/vllm/vllm-openai:v0.7.2 \<br>    10.119.85.141 \<br>    --worker \<br>    /home/deepseek/.cache/modelscope \<br>    --cap-add SYS_ADMIN \<br>    -e VLLM_HOST_IP=10.119.85.138 \<br>    --privileged -e NCCL_IB_HCA=mlx5 \<br>    -e NCCL_P2P_LEVEL=NVL \<br>    -e NCCL_IB_GID_INDEX=3 \<br>    -e NCCL_IB_DISABLE=0 \<br>    -e NCCL_DEBUG=INFO \<br>    -e NCCL_SOCKET_IFNAME=bond1<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">运行成功后，输出如下</span><br>[2025-02-23 22:06:55,915 W 1 1] global_state_accessor.cc:429: Retrying to get node with node ID de9745473ab8d1adb62391a32ea4254142ca109b2520c761285bc1a0<br>2025-02-23 22:06:55,851 INFO scripts.py:1047 -- Local node IP: 10.119.85.138<br>2025-02-23 22:06:56,937 SUCC scripts.py:1063 -- --------------------<br>2025-02-23 22:06:56,937 SUCC scripts.py:1064 -- Ray runtime started.<br>2025-02-23 22:06:56,937 SUCC scripts.py:1065 -- --------------------<br>2025-02-23 22:06:56,937 INFO scripts.py:1067 -- To terminate the Ray runtime, run<br>2025-02-23 22:06:56,938 INFO scripts.py:1068 --   ray stop<br>2025-02-23 22:06:56,938 INFO scripts.py:1076 -- --block<br>2025-02-23 22:06:56,938 INFO scripts.py:1077 -- This command will now block forever until terminated by a signal.<br>2025-02-23 22:06:56,938 INFO scripts.py:1080 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.<br><br></code></pre></td></tr></table></figure>
<p>上述操作当在每个服务器启动一个名为node的容器。按照vllm官方文档的说法，上述操作针启动一个<code>以容器形式运行的ray集群</code>（英文是“a
ray cluster of
<strong>containers</strong>”，不知如何翻译更准确，暂时就叫这个名字吧），4个服务器上的每个node容器都是这个容器集群的一员。</p>
<h3 id="查看ray集群">3.1.2 查看ray集群</h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">在集群中任何一个节点进入node容器（每个节点上都有一个node容器）</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">比如在deepseek1服务器上，再打开一个终端窗口，执行如下命令</span><br>(base) deepseek@deepseek1:~$ sudo docker exec -it node /bin/bash<br><span class="hljs-meta prompt_">#</span><span class="language-bash">以下在node容器中执行</span><br>root@deepseek1:/vllm-workspace# ray status<br>======== Autoscaler status: 2025-02-23 22:08:18.818774 ========<br>Node status<br>---------------------------------------------------------------<br>Active:<br> 1 node_5a3c4cb16e576f2afb9e2c612f5052ab28c46014fcfa7d7b501eb35c<br> 1 node_de9745473ab8d1adb62391a32ea4254142ca109b2520c761285bc1a0<br> 1 node_f2f75b7120e56b71d416d9af7301c3ad78ead0a19a6bf810a35e016f<br> 1 node_d0f9e04c9e3425460679f6fc59d274be1547d81f6a5f1d3dbd96c0fb<br>Pending:<br> (no pending nodes)<br>Recent failures:<br> (no failures)<br><br>Resources<br>---------------------------------------------------------------<br>Usage:<br> 0.0/448.0 CPU<br> 0.0/32.0 GPU<br> 0B/3.89TiB memory<br> 0B/38.91GiB object_store_memory<br><br>Demands:<br> (no resource demands)<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">##可以看到当前ray集群中一共有4个节点，且都是处于Active状态；ray集群总CPU数是448、总GPU个数是32、总内存是3.89T</span></span><br>root@deepseek1:/vllm-workspace# <br></code></pre></td></tr></table></figure>
<h3 id="收集环境信息">3.1.3 收集环境信息</h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">root@deepseek1:/vllm-workspace# python3 /usr/local/lib/python3.12/dist-packages/torch/utils/collect_env.py<br></code></pre></td></tr></table></figure>
<h3 id="检查跨节点gpu通信">3.1.4 检查跨节点GPU通信</h3>
<p>参考：<a
target="_blank" rel="noopener" href="https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#incorrect-hardware-driver">sanity
check script</a></p>
<h4 id="在node容器内创建test.py">在node容器内创建test.py</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><code class="hljs python">root@deepseek1:/vllm-workspace<span class="hljs-comment"># cat test.py</span><br><span class="hljs-comment"># Test PyTorch NCCL</span><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.distributed <span class="hljs-keyword">as</span> dist<br>dist.init_process_group(backend=<span class="hljs-string">&quot;nccl&quot;</span>)<br>local_rank = dist.get_rank() % torch.cuda.device_count()<br>torch.cuda.set_device(local_rank)<br>data = torch.FloatTensor([<span class="hljs-number">1</span>,] * <span class="hljs-number">128</span>).to(<span class="hljs-string">&quot;cuda&quot;</span>)<br>dist.all_reduce(data, op=dist.ReduceOp.SUM)<br>torch.cuda.synchronize()<br>value = data.mean().item()<br>world_size = dist.get_world_size()<br><span class="hljs-keyword">assert</span> value == world_size, <span class="hljs-string">f&quot;Expected <span class="hljs-subst">&#123;world_size&#125;</span>, got <span class="hljs-subst">&#123;value&#125;</span>&quot;</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;PyTorch NCCL is successful!&quot;</span>)<br><br><span class="hljs-comment"># Test PyTorch GLOO</span><br>gloo_group = dist.new_group(ranks=<span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(world_size)), backend=<span class="hljs-string">&quot;gloo&quot;</span>)<br>cpu_data = torch.FloatTensor([<span class="hljs-number">1</span>,] * <span class="hljs-number">128</span>)<br>dist.all_reduce(cpu_data, op=dist.ReduceOp.SUM, group=gloo_group)<br>value = cpu_data.mean().item()<br><span class="hljs-keyword">assert</span> value == world_size, <span class="hljs-string">f&quot;Expected <span class="hljs-subst">&#123;world_size&#125;</span>, got <span class="hljs-subst">&#123;value&#125;</span>&quot;</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;PyTorch GLOO is successful!&quot;</span>)<br><br><span class="hljs-keyword">if</span> world_size &lt;= <span class="hljs-number">1</span>:<br>    exit()<br><br><span class="hljs-comment"># Test vLLM NCCL, with cuda graph</span><br><span class="hljs-keyword">from</span> vllm.distributed.device_communicators.pynccl <span class="hljs-keyword">import</span> PyNcclCommunicator<br><br>pynccl = PyNcclCommunicator(group=gloo_group, device=local_rank)<br><span class="hljs-comment"># pynccl is enabled by default for 0.6.5+,</span><br><span class="hljs-comment"># but for 0.6.4 and below, we need to enable it manually.</span><br><span class="hljs-comment"># keep the code for backward compatibility when because people</span><br><span class="hljs-comment"># prefer to read the latest documentation.</span><br>pynccl.disabled = <span class="hljs-literal">False</span><br><br>s = torch.cuda.Stream()<br><span class="hljs-keyword">with</span> torch.cuda.stream(s):<br>    data.fill_(<span class="hljs-number">1</span>)<br>    out = pynccl.all_reduce(data, stream=s)<br>    value = out.mean().item()<br>    <span class="hljs-keyword">assert</span> value == world_size, <span class="hljs-string">f&quot;Expected <span class="hljs-subst">&#123;world_size&#125;</span>, got <span class="hljs-subst">&#123;value&#125;</span>&quot;</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;vLLM NCCL is successful!&quot;</span>)<br><br>g = torch.cuda.CUDAGraph()<br><span class="hljs-keyword">with</span> torch.cuda.graph(cuda_graph=g, stream=s):<br>    out = pynccl.all_reduce(data, stream=torch.cuda.current_stream())<br><br>data.fill_(<span class="hljs-number">1</span>)<br>g.replay()<br>torch.cuda.current_stream().synchronize()<br>value = out.mean().item()<br><span class="hljs-keyword">assert</span> value == world_size, <span class="hljs-string">f&quot;Expected <span class="hljs-subst">&#123;world_size&#125;</span>, got <span class="hljs-subst">&#123;value&#125;</span>&quot;</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;vLLM NCCL with cuda graph is successful!&quot;</span>)<br><br>dist.destroy_process_group(gloo_group)<br>dist.destroy_process_group()<br></code></pre></td></tr></table></figure>
<h4 id="在单个节点上测试">在单个节点上测试</h4>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">--nproc-per-node 是此节点上，待要使用的GPU个数</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">NCCL_DEBUG=TRACE torchrun --nproc-per-node=&lt;number-of-GPUs&gt; test.py</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">NCCL_P2P_DISABLE=0</span> <br>NCCL_DEBUG=TRACE torchrun --nproc-per-node=8 test.py<br></code></pre></td></tr></table></figure>
<p>如果上述py文件运行成功且通信是正常的，则输出类似于如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><code class="hljs shell">...<br>deepseek1:1239:1239 [3] NCCL INFO Connected all trees<br>deepseek1:1239:1239 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512<br>deepseek1:1239:1239 [3] NCCL INFO 16 coll channels, 16 collnet channels, 0 nvls channels, 16 p2p channels, 16 p2p channels per peer<br>deepseek1:1240:1240 [4] NCCL INFO Connected all trees<br>deepseek1:1240:1240 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512<br>deepseek1:1240:1240 [4] NCCL INFO 16 coll channels, 16 collnet channels, 0 nvls channels, 16 p2p channels, 16 p2p channels per peer<br>deepseek1:1241:1241 [5] NCCL INFO Connected all trees<br>deepseek1:1241:1241 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512<br>deepseek1:1241:1241 [5] NCCL INFO 16 coll channels, 16 collnet channels, 0 nvls channels, 16 p2p channels, 16 p2p channels per peer<br>deepseek1:1243:1243 [7] NCCL INFO Connected all trees<br>deepseek1:1243:1243 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512<br>deepseek1:1243:1243 [7] NCCL INFO 16 coll channels, 16 collnet channels, 0 nvls channels, 16 p2p channels, 16 p2p channels per peer<br>deepseek1:1242:1242 [6] NCCL INFO Connected all trees<br>deepseek1:1242:1242 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512<br>deepseek1:1242:1242 [6] NCCL INFO 16 coll channels, 16 collnet channels, 0 nvls channels, 16 p2p channels, 16 p2p channels per peer<br>deepseek1:1236:1236 [0] NCCL INFO ncclCommInitRank comm 0xb606410 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 3d000 commId 0x3c2a650fc7de9a50 - Init COMPLETE<br>deepseek1:1238:1238 [2] NCCL INFO ncclCommInitRank comm 0xb88e340 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 61000 commId 0x3c2a650fc7de9a50 - Init COMPLETE<br>deepseek1:1240:1240 [4] NCCL INFO ncclCommInitRank comm 0xad28600 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId ad000 commId 0x3c2a650fc7de9a50 - Init COMPLETE<br>deepseek1:1242:1242 [6] NCCL INFO ncclCommInitRank comm 0xb29e250 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId d0000 commId 0x3c2a650fc7de9a50 - Init COMPLETE<br>deepseek1:1237:1237 [1] NCCL INFO ncclCommInitRank comm 0xb60f020 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 42000 commId 0x3c2a650fc7de9a50 - Init COMPLETE<br>deepseek1:1241:1241 [5] NCCL INFO ncclCommInitRank comm 0xc2f2570 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId b1000 commId 0x3c2a650fc7de9a50 - Init COMPLETE<br>deepseek1:1239:1239 [3] NCCL INFO ncclCommInitRank comm 0xac2c590 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 67000 commId 0x3c2a650fc7de9a50 - Init COMPLETE<br>deepseek1:1243:1243 [7] NCCL INFO ncclCommInitRank comm 0xbd1b050 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId d3000 commId 0x3c2a650fc7de9a50 - Init COMPLETE<br>vLLM NCCL is successful!vLLM NCCL is successful!vLLM NCCL is successful!vLLM NCCL is successful!vLLM NCCL is successful!vLLM NCCL is successful!<br><br><br><br><br><br>vLLM NCCL is successful!<br>vLLM NCCL is successful!<br>vLLM NCCL with cuda graph is successful!vLLM NCCL with cuda graph is successful!vLLM NCCL with cuda graph is successful!<br><br>vLLM NCCL with cuda graph is successful!<br><br>vLLM NCCL with cuda graph is successful!<br>vLLM NCCL with cuda graph is successful!<br>vLLM NCCL with cuda graph is successful!<br>vLLM NCCL with cuda graph is successful!<br>deepseek1:1236:1299 [0] NCCL INFO [Service thread] Connection closed by localRank 0<br>deepseek1:1237:1286 [1] NCCL INFO [Service thread] Connection closed by localRank 1<br>deepseek1:1241:1291 [5] NCCL INFO [Service thread] Connection closed by localRank 5<br>deepseek1:1240:1293 [4] NCCL INFO [Service thread] Connection closed by localRank 4<br>deepseek1:1239:1295 [3] NCCL INFO [Service thread] Connection closed by localRank 3<br>deepseek1:1242:1288 [6] NCCL INFO [Service thread] Connection closed by localRank 6<br>deepseek1:1243:1287 [7] NCCL INFO [Service thread] Connection closed by localRank 7<br>deepseek1:1238:1290 [2] NCCL INFO [Service thread] Connection closed by localRank 2<br>deepseek1:1236:1400 [0] NCCL INFO comm 0x8265490 rank 0 nranks 8 cudaDev 0 busId 3d000 - Abort COMPLETE<br>deepseek1:1243:1405 [7] NCCL INFO comm 0x897f4f0 rank 7 nranks 8 cudaDev 7 busId d3000 - Abort COMPLETE<br>deepseek1:1237:1403 [1] NCCL INFO comm 0x828e430 rank 1 nranks 8 cudaDev 1 busId 42000 - Abort COMPLETE<br>deepseek1:1240:1402 [4] NCCL INFO comm 0x79a3f10 rank 4 nranks 8 cudaDev 4 busId ad000 - Abort COMPLETE<br>deepseek1:1241:1401 [5] NCCL INFO comm 0x8f75840 rank 5 nranks 8 cudaDev 5 busId b1000 - Abort COMPLETE<br>deepseek1:1242:1404 [6] NCCL INFO comm 0x7f1d4b0 rank 6 nranks 8 cudaDev 6 busId d0000 - Abort COMPLETE<br>deepseek1:1238:1407 [2] NCCL INFO comm 0x850c0d0 rank 2 nranks 8 cudaDev 2 busId 61000 - Abort COMPLETE<br>deepseek1:1239:1406 [3] NCCL INFO comm 0x78b0310 rank 3 nranks 8 cudaDev 3 busId 67000 - Abort COMPLETE<br></code></pre></td></tr></table></figure>
<h4 id="在多个节点间测试">在多个节点间测试</h4>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">##方法1）</span></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">--nnodes 是节点个数</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">--nproc-per-node 是各节点上，待要使用的GPU个数</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">--rdzv_endpoint 是ray集群中主节点的IP（保证这个IP在所有节点上都是可访问的。既然都已经搭建出ray集群，当然是可访问的）</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">在4个服务器上的node容器中执行如下相同的命令</span><br>NCCL_DEBUG=TRACE torchrun --nnodes 4 --nproc-per-node=8 --rdzv_backend=c10d --rdzv_endpoint=10.119.85.141 test.py<br><span class="hljs-meta prompt_">#</span><span class="language-bash">必须等4个容器中的命令都执行起来后，测试程序才会往下执行与输出相关内容</span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">##方法2）</span></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">在4个服务器上的node容器中分别执行如下命令：</span><br>NCCL_DEBUG=TRACE torchrun --nnodes 4 --nproc-per-node=8 --node-rank 0 --master_addr 10.119.85.141 test.py<br>NCCL_DEBUG=TRACE torchrun --nnodes 4 --nproc-per-node=8 --node-rank 1 --master_addr 10.119.85.141 test.py<br>NCCL_DEBUG=TRACE torchrun --nnodes 4 --nproc-per-node=8 --node-rank 2 --master_addr 10.119.85.141 test.py<br>NCCL_DEBUG=TRACE torchrun --nnodes 4 --nproc-per-node=8 --node-rank 3 --master_addr 10.119.85.141 test.py<br><span class="hljs-meta prompt_">#</span><span class="language-bash">必须等4个容器中的命令都执行起来后，测试程序才会往下执行与输出相关内容</span><br></code></pre></td></tr></table></figure>
<p>如果上述py文件运行成功且通信是正常的，则某个node容器test.py将输出类似于如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br></pre></td><td class="code"><pre><code class="hljs shell">...<br>INFO 02-24 00:01:07 pynccl.py:69] vLLM is using nccl==2.21.5<br>deepseek2:460:460 [4] NCCL INFO Using non-device net plugin version 0<br>deepseek2:460:460 [4] NCCL INFO Using network IB<br>deepseek2:459:459 [3] NCCL INFO ncclCommInitRank comm 0xb0c19c0 rank 11 nranks 32 cudaDev 3 nvmlDev 3 busId 67000 commId 0x8da567a0342af828 - Init START<br>deepseek2:458:458 [2] NCCL INFO ncclCommInitRank comm 0xb7b98b0 rank 10 nranks 32 cudaDev 2 nvmlDev 2 busId 61000 commId 0x8da567a0342af828 - Init START<br>deepseek2:457:457 [1] NCCL INFO ncclCommInitRank comm 0xc8103e0 rank 9 nranks 32 cudaDev 1 nvmlDev 1 busId 42000 commId 0x8da567a0342af828 - Init START<br>deepseek2:460:460 [4] NCCL INFO ncclCommInitRank comm 0xc1ff420 rank 12 nranks 32 cudaDev 4 nvmlDev 4 busId ad000 commId 0x8da567a0342af828 - Init START<br>deepseek2:456:456 [0] NCCL INFO ncclCommInitRank comm 0xbd9b580 rank 8 nranks 32 cudaDev 0 nvmlDev 0 busId 3d000 commId 0x8da567a0342af828 - Init START<br>deepseek2:461:461 [5] NCCL INFO ncclCommInitRank comm 0xc041a00 rank 13 nranks 32 cudaDev 5 nvmlDev 5 busId b1000 commId 0x8da567a0342af828 - Init START<br>deepseek2:463:463 [7] NCCL INFO ncclCommInitRank comm 0xc41ace0 rank 15 nranks 32 cudaDev 7 nvmlDev 7 busId d3000 commId 0x8da567a0342af828 - Init START<br>deepseek2:462:462 [6] NCCL INFO ncclCommInitRank comm 0xb794080 rank 14 nranks 32 cudaDev 6 nvmlDev 6 busId d0000 commId 0x8da567a0342af828 - Init START<br>deepseek2:463:463 [7] NCCL INFO Setting affinity for GPU 7 to ffff,fff00000,00ffffff,f0000000<br>deepseek2:463:463 [7] NCCL INFO NVLS multicast support is not available on dev 7<br>deepseek2:459:459 [3] NCCL INFO Setting affinity for GPU 3 to 0fffff,ff000000,0fffffff<br>deepseek2:459:459 [3] NCCL INFO NVLS multicast support is not available on dev 3<br>deepseek2:460:460 [4] NCCL INFO Setting affinity for GPU 4 to ffff,fff00000,00ffffff,f0000000<br>deepseek2:460:460 [4] NCCL INFO NVLS multicast support is not available on dev 4<br>deepseek2:461:461 [5] NCCL INFO Setting affinity for GPU 5 to ffff,fff00000,00ffffff,f0000000<br>deepseek2:461:461 [5] NCCL INFO NVLS multicast support is not available on dev 5<br>deepseek2:456:456 [0] NCCL INFO Setting affinity for GPU 0 to 0fffff,ff000000,0fffffff<br>deepseek2:456:456 [0] NCCL INFO NVLS multicast support is not available on dev 0<br>deepseek2:457:457 [1] NCCL INFO Setting affinity for GPU 1 to 0fffff,ff000000,0fffffff<br>deepseek2:457:457 [1] NCCL INFO NVLS multicast support is not available on dev 1<br>deepseek2:458:458 [2] NCCL INFO Setting affinity for GPU 2 to 0fffff,ff000000,0fffffff<br>deepseek2:458:458 [2] NCCL INFO NVLS multicast support is not available on dev 2<br>deepseek2:462:462 [6] NCCL INFO Setting affinity for GPU 6 to ffff,fff00000,00ffffff,f0000000<br>deepseek2:462:462 [6] NCCL INFO NVLS multicast support is not available on dev 6<br>deepseek2:463:463 [7] NCCL INFO comm 0xc41ace0 rank 15 nRanks 32 nNodes 4 localRanks 8 localRank 7 MNNVL 0<br>deepseek2:462:462 [6] NCCL INFO comm 0xb794080 rank 14 nRanks 32 nNodes 4 localRanks 8 localRank 6 MNNVL 0<br>deepseek2:457:457 [1] NCCL INFO comm 0xc8103e0 rank 9 nRanks 32 nNodes 4 localRanks 8 localRank 1 MNNVL 0<br>deepseek2:461:461 [5] NCCL INFO comm 0xc041a00 rank 13 nRanks 32 nNodes 4 localRanks 8 localRank 5 MNNVL 0<br>deepseek2:460:460 [4] NCCL INFO comm 0xc1ff420 rank 12 nRanks 32 nNodes 4 localRanks 8 localRank 4 MNNVL 0<br>deepseek2:458:458 [2] NCCL INFO comm 0xb7b98b0 rank 10 nRanks 32 nNodes 4 localRanks 8 localRank 2 MNNVL 0<br>deepseek2:462:462 [6] NCCL INFO Trees [0] 15/-1/-1-&gt;14-&gt;13 [1] 15/-1/-1-&gt;14-&gt;13 [2] 15/-1/-1-&gt;14-&gt;13 [3] 15/-1/-1-&gt;14-&gt;13<br>deepseek2:463:463 [7] NCCL INFO Trees [0] -1/-1/-1-&gt;15-&gt;14 [1] 8/-1/-1-&gt;15-&gt;14 [2] -1/-1/-1-&gt;15-&gt;14 [3] 8/-1/-1-&gt;15-&gt;14<br>deepseek2:459:459 [3] NCCL INFO comm 0xb0c19c0 rank 11 nRanks 32 nNodes 4 localRanks 8 localRank 3 MNNVL 0<br>deepseek2:456:456 [0] NCCL INFO comm 0xbd9b580 rank 8 nRanks 32 nNodes 4 localRanks 8 localRank 0 MNNVL 0<br>deepseek2:462:462 [6] NCCL INFO P2P Chunksize set to 131072<br>deepseek2:461:461 [5] NCCL INFO Trees [0] 14/-1/-1-&gt;13-&gt;12 [1] 14/-1/-1-&gt;13-&gt;12 [2] 14/-1/-1-&gt;13-&gt;12 [3] 14/-1/-1-&gt;13-&gt;12<br>deepseek2:463:463 [7] NCCL INFO P2P Chunksize set to 131072<br>deepseek2:457:457 [1] NCCL INFO Trees [0] 10/-1/-1-&gt;9-&gt;8 [1] -1/-1/-1-&gt;9-&gt;8 [2] 10/16/-1-&gt;9-&gt;8 [3] -1/-1/-1-&gt;9-&gt;8<br>deepseek2:460:460 [4] NCCL INFO Trees [0] 13/-1/-1-&gt;12-&gt;11 [1] 13/-1/-1-&gt;12-&gt;11 [2] 13/-1/-1-&gt;12-&gt;11 [3] 13/-1/-1-&gt;12-&gt;11<br>deepseek2:461:461 [5] NCCL INFO P2P Chunksize set to 131072<br>deepseek2:458:458 [2] NCCL INFO Trees [0] 11/-1/-1-&gt;10-&gt;9 [1] 11/-1/-1-&gt;10-&gt;19 [2] 11/-1/-1-&gt;10-&gt;9 [3] 11/6/-1-&gt;10-&gt;26<br>deepseek2:457:457 [1] NCCL INFO P2P Chunksize set to 131072<br>deepseek2:459:459 [3] NCCL INFO Trees [0] 12/-1/-1-&gt;11-&gt;10 [1] 12/-1/-1-&gt;11-&gt;10 [2] 12/-1/-1-&gt;11-&gt;10 [3] 12/18/-1-&gt;11-&gt;10<br>deepseek2:458:458 [2] NCCL INFO P2P Chunksize set to 131072<br>deepseek2:460:460 [4] NCCL INFO P2P Chunksize set to 131072<br>deepseek2:459:459 [3] NCCL INFO P2P Chunksize set to 131072<br>deepseek2:456:456 [0] NCCL INFO Trees [0] 9/-1/-1-&gt;8-&gt;17 [1] 9/-1/-1-&gt;8-&gt;15 [2] 9/2/-1-&gt;8-&gt;24 [3] 9/-1/-1-&gt;8-&gt;15<br>deepseek2:456:456 [0] NCCL INFO P2P Chunksize set to 131072<br>deepseek2:460:460 [4] NCCL INFO Channel 00/0 : 12[4] -&gt; 11[3] via P2P/CUMEM/read<br>deepseek2:462:462 [6] NCCL INFO Channel 00/0 : 14[6] -&gt; 13[5] via P2P/CUMEM/read<br>deepseek2:461:461 [5] NCCL INFO Channel 00/0 : 13[5] -&gt; 12[4] via P2P/CUMEM/read<br>deepseek2:460:460 [4] NCCL INFO Channel 01/0 : 12[4] -&gt; 11[3] via P2P/CUMEM/read<br>deepseek2:462:462 [6] NCCL INFO Channel 01/0 : 14[6] -&gt; 13[5] via P2P/CUMEM/read<br>deepseek2:461:461 [5] NCCL INFO Channel 01/0 : 13[5] -&gt; 12[4] via P2P/CUMEM/read<br>deepseek2:460:460 [4] NCCL INFO Channel 02/0 : 12[4] -&gt; 11[3] via P2P/CUMEM/read<br>deepseek2:462:462 [6] NCCL INFO Channel 02/0 : 14[6] -&gt; 13[5] via P2P/CUMEM/read<br>deepseek2:461:461 [5] NCCL INFO Channel 02/0 : 13[5] -&gt; 12[4] via P2P/CUMEM/read<br>deepseek2:460:460 [4] NCCL INFO Channel 03/0 : 12[4] -&gt; 11[3] via P2P/CUMEM/read<br>deepseek2:462:462 [6] NCCL INFO Channel 03/0 : 14[6] -&gt; 13[5] via P2P/CUMEM/read<br>deepseek2:461:461 [5] NCCL INFO Channel 03/0 : 13[5] -&gt; 12[4] via P2P/CUMEM/read<br>deepseek2:457:457 [1] NCCL INFO Channel 00/0 : 9[1] -&gt; 16[0] [send] via NET/IB/0/GDRDMA<br>deepseek2:459:459 [3] NCCL INFO Channel 01/0 : 11[3] -&gt; 18[2] [send] via NET/IB/1/GDRDMA<br>deepseek2:457:457 [1] NCCL INFO Channel 02/0 : 9[1] -&gt; 16[0] [send] via NET/IB/0/GDRDMA<br>deepseek2:459:459 [3] NCCL INFO Channel 03/0 : 11[3] -&gt; 18[2] [send] via NET/IB/1/GDRDMA<br>deepseek2:456:456 [0] NCCL INFO Channel 00/0 : 3[3] -&gt; 8[0] [receive] via NET/IB/0/GDRDMA<br>deepseek2:458:458 [2] NCCL INFO Channel 01/0 : 7[7] -&gt; 10[2] [receive] via NET/IB/1/GDRDMA<br>deepseek2:456:456 [0] NCCL INFO Channel 02/0 : 3[3] -&gt; 8[0] [receive] via NET/IB/0/GDRDMA<br>deepseek2:458:458 [2] NCCL INFO Channel 03/0 : 7[7] -&gt; 10[2] [receive] via NET/IB/1/GDRDMA<br>deepseek2:456:456 [0] NCCL INFO Channel 00/0 : 8[0] -&gt; 15[7] via P2P/CUMEM/read<br>deepseek2:456:456 [0] NCCL INFO Channel 01/0 : 8[0] -&gt; 15[7] via P2P/CUMEM/read<br>deepseek2:456:456 [0] NCCL INFO Channel 02/0 : 8[0] -&gt; 15[7] via P2P/CUMEM/read<br>deepseek2:458:458 [2] NCCL INFO Channel 00/0 : 10[2] -&gt; 9[1] via P2P/CUMEM/read<br>deepseek2:456:456 [0] NCCL INFO Channel 03/0 : 8[0] -&gt; 15[7] via P2P/CUMEM/read<br>deepseek2:459:459 [3] NCCL INFO Channel 00/0 : 11[3] -&gt; 10[2] via P2P/CUMEM/read<br>deepseek2:458:458 [2] NCCL INFO Channel 01/0 : 10[2] -&gt; 9[1] via P2P/CUMEM/read<br>deepseek2:457:457 [1] NCCL INFO Channel 01/0 : 9[1] -&gt; 8[0] via P2P/CUMEM/read<br>deepseek2:459:459 [3] NCCL INFO Channel 02/0 : 11[3] -&gt; 10[2] via P2P/CUMEM/read<br>deepseek2:463:463 [7] NCCL INFO Channel 00/0 : 15[7] -&gt; 14[6] via P2P/CUMEM/read<br>deepseek2:457:457 [1] NCCL INFO Channel 03/0 : 9[1] -&gt; 8[0] via P2P/CUMEM/read<br>deepseek2:458:458 [2] NCCL INFO Channel 02/0 : 10[2] -&gt; 9[1] via P2P/CUMEM/read<br>deepseek2:463:463 [7] NCCL INFO Channel 01/0 : 15[7] -&gt; 14[6] via P2P/CUMEM/read<br>deepseek2:458:458 [2] NCCL INFO Channel 03/0 : 10[2] -&gt; 9[1] via P2P/CUMEM/read<br>deepseek2:463:463 [7] NCCL INFO Channel 02/0 : 15[7] -&gt; 14[6] via P2P/CUMEM/read<br>deepseek2:463:463 [7] NCCL INFO Channel 03/0 : 15[7] -&gt; 14[6] via P2P/CUMEM/read<br>deepseek2:458:458 [2] NCCL INFO Connected all rings<br>deepseek2:459:459 [3] NCCL INFO Connected all rings<br>deepseek2:460:460 [4] NCCL INFO Connected all rings<br>deepseek2:458:458 [2] NCCL INFO Channel 00/0 : 10[2] -&gt; 11[3] via P2P/CUMEM/read<br>deepseek2:458:458 [2] NCCL INFO Channel 01/0 : 10[2] -&gt; 11[3] via P2P/CUMEM/read<br>deepseek2:458:458 [2] NCCL INFO Channel 02/0 : 10[2] -&gt; 11[3] via P2P/CUMEM/read<br>deepseek2:459:459 [3] NCCL INFO Channel 00/0 : 11[3] -&gt; 12[4] via P2P/CUMEM/read<br>deepseek2:458:458 [2] NCCL INFO Channel 03/0 : 10[2] -&gt; 11[3] via P2P/CUMEM/read<br>deepseek2:460:460 [4] NCCL INFO Channel 00/0 : 12[4] -&gt; 13[5] via P2P/CUMEM/read<br>deepseek2:459:459 [3] NCCL INFO Channel 01/0 : 11[3] -&gt; 12[4] via P2P/CUMEM/read<br>deepseek2:456:456 [0] NCCL INFO Connected all rings<br>deepseek2:457:457 [1] NCCL INFO Connected all rings<br>deepseek2:456:456 [0] NCCL INFO Channel 00/0 : 8[0] -&gt; 9[1] via P2P/CUMEM/read<br>deepseek2:460:460 [4] NCCL INFO Channel 01/0 : 12[4] -&gt; 13[5] via P2P/CUMEM/read<br>deepseek2:459:459 [3] NCCL INFO Channel 02/0 : 11[3] -&gt; 12[4] via P2P/CUMEM/read<br>deepseek2:456:456 [0] NCCL INFO Channel 01/0 : 8[0] -&gt; 9[1] via P2P/CUMEM/read<br>deepseek2:460:460 [4] NCCL INFO Channel 02/0 : 12[4] -&gt; 13[5] via P2P/CUMEM/read<br>deepseek2:459:459 [3] NCCL INFO Channel 03/0 : 11[3] -&gt; 12[4] via P2P/CUMEM/read<br>deepseek2:456:456 [0] NCCL INFO Channel 02/0 : 8[0] -&gt; 9[1] via P2P/CUMEM/read<br>deepseek2:460:460 [4] NCCL INFO Channel 03/0 : 12[4] -&gt; 13[5] via P2P/CUMEM/read<br>deepseek2:456:456 [0] NCCL INFO Channel 03/0 : 8[0] -&gt; 9[1] via P2P/CUMEM/read<br>deepseek2:457:457 [1] NCCL INFO Channel 00/0 : 9[1] -&gt; 10[2] via P2P/CUMEM/read<br>deepseek2:459:459 [3] NCCL INFO Channel 03/0 : 18[2] -&gt; 11[3] [receive] via NET/IB/1/GDRDMA<br>deepseek2:457:457 [1] NCCL INFO Channel 02/0 : 9[1] -&gt; 10[2] via P2P/CUMEM/read<br>deepseek2:456:456 [0] NCCL INFO Channel 02/0 : 2[2] -&gt; 8[0] [receive] via NET/IB/0/GDRDMA<br>deepseek2:458:458 [2] NCCL INFO Channel 03/0 : 6[6] -&gt; 10[2] [receive] via NET/IB/1/GDRDMA<br>deepseek2:457:457 [1] NCCL INFO Channel 02/0 : 16[0] -&gt; 9[1] [receive] via NET/IB/0/GDRDMA<br>deepseek2:456:456 [0] NCCL INFO Channel 00/0 : 8[0] -&gt; 17[1] [send] via NET/IB/0/GDRDMA<br>deepseek2:458:458 [2] NCCL INFO Channel 01/0 : 10[2] -&gt; 19[3] [send] via NET/IB/1/GDRDMA<br>deepseek2:456:456 [0] NCCL INFO Channel 02/0 : 24[0] -&gt; 8[0] [receive] via NET/IB/0/GDRDMA<br>deepseek2:456:456 [0] NCCL INFO Channel 02/0 : 8[0] -&gt; 24[0] [send] via NET/IB/0/GDRDMA<br>deepseek2:458:458 [2] NCCL INFO Channel 03/0 : 26[2] -&gt; 10[2] [receive] via NET/IB/1/GDRDMA<br>deepseek2:461:461 [5] NCCL INFO Connected all rings<br>deepseek2:463:463 [7] NCCL INFO Connected all rings<br>deepseek2:458:458 [2] NCCL INFO Channel 03/0 : 10[2] -&gt; 26[2] [send] via NET/IB/1/GDRDMA<br>deepseek2:462:462 [6] NCCL INFO Connected all rings<br>deepseek2:456:456 [0] NCCL INFO Channel 00/0 : 17[1] -&gt; 8[0] [receive] via NET/IB/0/GDRDMA<br>deepseek2:458:458 [2] NCCL INFO Channel 01/0 : 19[3] -&gt; 10[2] [receive] via NET/IB/1/GDRDMA<br>deepseek2:458:458 [2] NCCL INFO Channel 03/0 : 10[2] -&gt; 6[6] [send] via NET/IB/1/GDRDMA<br>deepseek2:459:459 [3] NCCL INFO Channel 01/0 : 11[3] -&gt; 10[2] via P2P/CUMEM/read<br>deepseek2:459:459 [3] NCCL INFO Channel 03/0 : 11[3] -&gt; 10[2] via P2P/CUMEM/read<br>deepseek2:457:457 [1] NCCL INFO Channel 00/0 : 9[1] -&gt; 8[0] via P2P/CUMEM/read<br>deepseek2:461:461 [5] NCCL INFO Channel 00/0 : 13[5] -&gt; 14[6] via P2P/CUMEM/read<br>deepseek2:457:457 [1] NCCL INFO Channel 02/0 : 9[1] -&gt; 8[0] via P2P/CUMEM/read<br>deepseek2:461:461 [5] NCCL INFO Channel 01/0 : 13[5] -&gt; 14[6] via P2P/CUMEM/read<br>deepseek2:462:462 [6] NCCL INFO Channel 00/0 : 14[6] -&gt; 15[7] via P2P/CUMEM/read<br>deepseek2:461:461 [5] NCCL INFO Channel 02/0 : 13[5] -&gt; 14[6] via P2P/CUMEM/read<br>deepseek2:462:462 [6] NCCL INFO Channel 01/0 : 14[6] -&gt; 15[7] via P2P/CUMEM/read<br>deepseek2:461:461 [5] NCCL INFO Channel 03/0 : 13[5] -&gt; 14[6] via P2P/CUMEM/read<br>deepseek2:462:462 [6] NCCL INFO Channel 02/0 : 14[6] -&gt; 15[7] via P2P/CUMEM/read<br>deepseek2:462:462 [6] NCCL INFO Channel 03/0 : 14[6] -&gt; 15[7] via P2P/CUMEM/read<br>deepseek2:463:463 [7] NCCL INFO Channel 01/0 : 15[7] -&gt; 8[0] via P2P/CUMEM/read<br>deepseek2:463:463 [7] NCCL INFO Channel 03/0 : 15[7] -&gt; 8[0] via P2P/CUMEM/read<br>deepseek2:456:456 [0] NCCL INFO Channel 02/0 : 8[0] -&gt; 2[2] [send] via NET/IB/0/GDRDMA<br>deepseek2:462:462 [6] NCCL INFO Connected all trees<br>deepseek2:462:462 [6] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512<br>deepseek2:462:462 [6] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer<br>deepseek2:461:461 [5] NCCL INFO Connected all trees<br>deepseek2:461:461 [5] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512<br>deepseek2:461:461 [5] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer<br>deepseek2:460:460 [4] NCCL INFO Connected all trees<br>deepseek2:460:460 [4] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512<br>deepseek2:460:460 [4] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer<br>deepseek2:458:458 [2] NCCL INFO Connected all trees<br>deepseek2:459:459 [3] NCCL INFO Connected all trees<br>deepseek2:458:458 [2] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512<br>deepseek2:458:458 [2] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer<br>deepseek2:459:459 [3] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512<br>deepseek2:459:459 [3] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer<br>deepseek2:463:463 [7] NCCL INFO Connected all trees<br>deepseek2:463:463 [7] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512<br>deepseek2:463:463 [7] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer<br>deepseek2:457:457 [1] NCCL INFO Connected all trees<br>deepseek2:457:457 [1] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512<br>deepseek2:457:457 [1] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer<br>deepseek2:456:456 [0] NCCL INFO Connected all trees<br>deepseek2:456:456 [0] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512<br>deepseek2:456:456 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer<br>deepseek2:457:457 [1] NCCL INFO ncclCommInitRank comm 0xc8103e0 rank 9 nranks 32 cudaDev 1 nvmlDev 1 busId 42000 commId 0x8da567a0342af828 - Init COMPLETE<br>deepseek2:459:459 [3] NCCL INFO ncclCommInitRank comm 0xb0c19c0 rank 11 nranks 32 cudaDev 3 nvmlDev 3 busId 67000 commId 0x8da567a0342af828 - Init COMPLETE<br>deepseek2:462:462 [6] NCCL INFO ncclCommInitRank comm 0xb794080 rank 14 nranks 32 cudaDev 6 nvmlDev 6 busId d0000 commId 0x8da567a0342af828 - Init COMPLETE<br>deepseek2:461:461 [5] NCCL INFO ncclCommInitRank comm 0xc041a00 rank 13 nranks 32 cudaDev 5 nvmlDev 5 busId b1000 commId 0x8da567a0342af828 - Init COMPLETE<br>deepseek2:458:458 [2] NCCL INFO ncclCommInitRank comm 0xb7b98b0 rank 10 nranks 32 cudaDev 2 nvmlDev 2 busId 61000 commId 0x8da567a0342af828 - Init COMPLETE<br>deepseek2:463:463 [7] NCCL INFO ncclCommInitRank comm 0xc41ace0 rank 15 nranks 32 cudaDev 7 nvmlDev 7 busId d3000 commId 0x8da567a0342af828 - Init COMPLETE<br>deepseek2:460:460 [4] NCCL INFO ncclCommInitRank comm 0xc1ff420 rank 12 nranks 32 cudaDev 4 nvmlDev 4 busId ad000 commId 0x8da567a0342af828 - Init COMPLETE<br>deepseek2:456:456 [0] NCCL INFO ncclCommInitRank comm 0xbd9b580 rank 8 nranks 32 cudaDev 0 nvmlDev 0 busId 3d000 commId 0x8da567a0342af828 - Init COMPLETE<br>vLLM NCCL is successful!vLLM NCCL is successful!vLLM NCCL is successful!vLLM NCCL is successful!<br>vLLM NCCL is successful!<br>vLLM NCCL is successful!vLLM NCCL is successful!<br><br><br><br><br>vLLM NCCL is successful!<br>vLLM NCCL with cuda graph is successful!vLLM NCCL with cuda graph is successful!<br>vLLM NCCL with cuda graph is successful!<br>vLLM NCCL with cuda graph is successful!<br>vLLM NCCL with cuda graph is successful!<br>vLLM NCCL with cuda graph is successful!vLLM NCCL with cuda graph is successful!<br>vLLM NCCL with cuda graph is successful!<br><br><br>deepseek2:456:544 [0] NCCL INFO [Service thread] Connection closed by localRank 0<br>deepseek2:458:548 [2] NCCL INFO [Service thread] Connection closed by localRank 2<br>deepseek2:456:544 [0] NCCL INFO [Service thread] Connection closed by localRank 5<br>deepseek2:460:550 [4] NCCL INFO [Service thread] Connection closed by localRank 5<br>deepseek2:457:552 [1] NCCL INFO [Service thread] Connection closed by localRank 1<br>deepseek2:461:549 [5] NCCL INFO [Service thread] Connection closed by localRank 5<br>deepseek2:462:546 [6] NCCL INFO [Service thread] Connection closed by localRank 6<br>deepseek2:459:551 [3] NCCL INFO [Service thread] Connection closed by localRank 3<br>deepseek2:456:544 [0] NCCL INFO [Service thread] Connection closed by localRank 6<br>deepseek2:458:548 [2] NCCL INFO [Service thread] Connection closed by localRank 5<br>deepseek2:463:547 [7] NCCL INFO [Service thread] Connection closed by localRank 7<br>deepseek2:456:544 [0] NCCL INFO [Service thread] Connection closed by localRank 1<br>deepseek2:458:548 [2] NCCL INFO [Service thread] Connection closed by localRank 6<br>deepseek2:460:550 [4] NCCL INFO [Service thread] Connection closed by localRank 6<br>deepseek2:462:546 [6] NCCL INFO [Service thread] Connection closed by localRank 4<br>deepseek2:456:544 [0] NCCL INFO [Service thread] Connection closed by localRank 2<br>deepseek2:458:548 [2] NCCL INFO [Service thread] Connection closed by localRank 0<br>deepseek2:460:550 [4] NCCL INFO [Service thread] Connection closed by localRank 0<br>deepseek2:462:546 [6] NCCL INFO [Service thread] Connection closed by localRank 5<br>deepseek2:456:544 [0] NCCL INFO [Service thread] Connection closed by localRank 4<br>deepseek2:458:548 [2] NCCL INFO [Service thread] Connection closed by localRank 7<br>deepseek2:462:546 [6] NCCL INFO [Service thread] Connection closed by localRank 1<br>deepseek2:460:550 [4] NCCL INFO [Service thread] Connection closed by localRank 4<br>deepseek2:456:544 [0] NCCL INFO [Service thread] Connection closed by localRank 3<br>deepseek2:458:548 [2] NCCL INFO [Service thread] Connection closed by localRank 4<br>deepseek2:462:546 [6] NCCL INFO [Service thread] Connection closed by localRank 3<br>deepseek2:460:550 [4] NCCL INFO [Service thread] Connection closed by localRank 1<br>deepseek2:456:544 [0] NCCL INFO [Service thread] Connection closed by localRank 7<br>deepseek2:458:548 [2] NCCL INFO [Service thread] Connection closed by localRank 1<br>deepseek2:462:546 [6] NCCL INFO [Service thread] Connection closed by localRank 7<br>deepseek2:460:550 [4] NCCL INFO [Service thread] Connection closed by localRank 2<br>deepseek2:458:548 [2] NCCL INFO [Service thread] Connection closed by localRank 3<br>deepseek2:462:546 [6] NCCL INFO [Service thread] Connection closed by localRank 0<br>deepseek2:460:550 [4] NCCL INFO [Service thread] Connection closed by localRank 3<br>deepseek2:462:546 [6] NCCL INFO [Service thread] Connection closed by localRank 2<br>deepseek2:460:550 [4] NCCL INFO [Service thread] Connection closed by localRank 7<br>deepseek2:456:660 [0] NCCL INFO comm 0x8a11620 rank 8 nranks 32 cudaDev 0 busId 3d000 - Abort COMPLETE<br>deepseek2:458:664 [2] NCCL INFO comm 0x8430430 rank 10 nranks 32 cudaDev 2 busId 61000 - Abort COMPLETE<br>deepseek2:462:658 [6] NCCL INFO comm 0x84085f0 rank 14 nranks 32 cudaDev 6 busId d0000 - Abort COMPLETE<br>deepseek2:460:661 [4] NCCL INFO comm 0x8e5a270 rank 12 nranks 32 cudaDev 4 busId ad000 - Abort COMPLETE<br>deepseek2:463:662 [7] NCCL INFO comm 0x9090570 rank 15 nranks 32 cudaDev 7 busId d3000 - Abort COMPLETE<br>deepseek2:461:657 [5] NCCL INFO comm 0x8cb26b0 rank 13 nranks 32 cudaDev 5 busId b1000 - Abort COMPLETE<br>deepseek2:457:659 [1] NCCL INFO comm 0x948a0f0 rank 9 nranks 32 cudaDev 1 busId 42000 - Abort COMPLETE<br>deepseek2:459:663 [3] NCCL INFO comm 0x7d3d650 rank 11 nranks 32 cudaDev 3 busId 67000 - Abort COMPLETE<br>deepseek2:460:636 [32679] NCCL INFO [Service thread] Connection closed by localRank 0<br>deepseek2:462:632 [32661] NCCL INFO [Service thread] Connection closed by localRank 0<br>deepseek2:460:636 [32679] NCCL INFO [Service thread] Connection closed by localRank 2<br>deepseek2:460:636 [32679] NCCL INFO [Service thread] Connection closed by localRank 6<br></code></pre></td></tr></table></figure>
<h5 id="报错与处理-1">报错与处理</h5>
<h6
id="问题1-提示超时torch.distributed.elastic.rendezvous.api.rendezvoustimeouterror">问题1-提示超时”torch.distributed.elastic.rendezvous.api.RendezvousTimeoutError“</h6>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">如果只是在ray集群的主节点node容器中执行如下命令，则会报错，如下。</span><br>root@deepseek1:/vllm-workspace# NCCL_DEBUG=TRACE torchrun --nnodes 4 --nproc-per-node=8 --rdzv_backend=c10d --rdzv_endpoint=10.119.165.139 test.py<br>W0223 01:40:19.140000 1778 torch/distributed/run.py:793] <br>W0223 01:40:19.140000 1778 torch/distributed/run.py:793] *****************************************<br>W0223 01:40:19.140000 1778 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. <br>W0223 01:40:19.140000 1778 torch/distributed/run.py:793] *****************************************<br>Traceback (most recent call last):<br>  File &quot;/usr/local/bin/torchrun&quot;, line 8, in &lt;module&gt;<br>    sys.exit(main())<br>             ^^^^^^<br>  File &quot;/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py&quot;, line 355, in wrapper<br>    return f(*args, **kwargs)<br>           ^^^^^^^^^^^^^^^^^^<br>  File &quot;/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py&quot;, line 919, in main<br>    run(args)<br>  File &quot;/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py&quot;, line 910, in run<br>    elastic_launch(<br>  File &quot;/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py&quot;, line 138, in __call__<br>    return launch_agent(self._config, self._entrypoint, list(args))<br>           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File &quot;/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py&quot;, line 260, in launch_agent<br>    result = agent.run()<br>             ^^^^^^^^^^^<br>  File &quot;/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/metrics/api.py&quot;, line 137, in wrapper<br>    result = f(*args, **kwargs)<br>             ^^^^^^^^^^^^^^^^^^<br>  File &quot;/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/api.py&quot;, line 696, in run<br>    result = self._invoke_run(role)<br>             ^^^^^^^^^^^^^^^^^^^^^^<br>  File &quot;/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/api.py&quot;, line 849, in _invoke_run<br>    self._initialize_workers(self._worker_group)<br>  File &quot;/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/metrics/api.py&quot;, line 137, in wrapper<br>    result = f(*args, **kwargs)<br>             ^^^^^^^^^^^^^^^^^^<br>  File &quot;/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/api.py&quot;, line 668, in _initialize_workers<br>    self._rendezvous(worker_group)<br>  File &quot;/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/metrics/api.py&quot;, line 137, in wrapper<br>    result = f(*args, **kwargs)<br>             ^^^^^^^^^^^^^^^^^^<br>  File &quot;/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/api.py&quot;, line 500, in _rendezvous<br>    rdzv_info = spec.rdzv_handler.next_rendezvous()<br>                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File &quot;/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/rendezvous/dynamic_rendezvous.py&quot;, line 1157, in next_rendezvous<br>    self._op_executor.run(join_op, deadline, self._get_deadline)<br>  File &quot;/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/rendezvous/dynamic_rendezvous.py&quot;, line 679, in run<br>    raise RendezvousTimeoutError<br>torch.distributed.elastic.rendezvous.api.RendezvousTimeoutError<br></code></pre></td></tr></table></figure>
<p>解决办法：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">在4个服务器上的node容器中分别执行如下命令：</span><br>NCCL_DEBUG=TRACE torchrun --nnodes 4 --nproc-per-node=8 --node-rank 0 --master_addr 10.119.85.141 test.py<br>NCCL_DEBUG=TRACE torchrun --nnodes 4 --nproc-per-node=8 --node-rank 1 --master_addr 10.119.85.141 test.py<br>NCCL_DEBUG=TRACE torchrun --nnodes 4 --nproc-per-node=8 --node-rank 2 --master_addr 10.119.85.141 test.py<br>NCCL_DEBUG=TRACE torchrun --nnodes 4 --nproc-per-node=8 --node-rank 3 --master_addr 10.119.85.141 test.py<br><span class="hljs-meta prompt_">#</span><span class="language-bash">必须等4个容器中的命令都执行起来后，测试程序才会往下执行与输出相关内容</span><br></code></pre></td></tr></table></figure>
<h3 id="启动推理引擎vllm进程">3.1.5 启动推理引擎vllm进程</h3>
<h4 id="执行启动">执行启动</h4>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment">##启动vllm进程的命令如下（当然还可以添加其他参数。可使用“vllm serve --help”查看）</span></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">vllm serve /path/to/the/model/in/the/container \</span><br><span class="language-bash"><span class="hljs-comment">#     --tensor-parallel-size 8 \</span></span><br><span class="language-bash"><span class="hljs-comment">#     --pipeline-parallel-size 2</span></span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">这vllm官网说可以在集群中任何一个节点进入node容器并执行“vllm serve ...”，但在ray集群主节点即<span class="hljs-built_in">head</span>节点执行命令，会提示“No available node types can fulfill resource request”。所以笔者选择在ray集群副节点即worker节点执行</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">比如在deepseek2服务器上，再打开一个终端窗口并进入node容器（也可以继续使用“查看ray集群”步骤中打开的容器会话）</span><br>(base) deepseek@deepseek2:~$ sudo docker exec -it node /bin/bash<br><span class="hljs-meta prompt_">#</span><span class="language-bash">在node容器中执行以下命令，启动vllm进程。将有很多输出内容，以下截取部分</span><br>root@deepseek2:/vllm-workspace# vllm serve /root/.cache/huggingface/hub/models/unsloth/DeepSeek-R1-BF16/ \<br>    --served-model-name DeepSeek-R1-671B \<br>    --enable-prefix-caching \<br>    --max-model-len 32768 \<br>    --gpu-memory-utilization 0.95 \<br>    --tensor-parallel-size 8 \<br>    --pipeline-parallel-size 4 \<br>    --enable-chunked-prefill \<br>    --max-num-batched-tokens 32768 \<br>    --trust-remote-code \<br>    --port 8000 \<br>    --dtype auto \<br>    --api-key zY0MrQwXV9Oo3g==<br><span class="hljs-meta prompt_">#</span><span class="language-bash">相关参数的意义，参考“vllm serve --<span class="hljs-built_in">help</span>”输出或“https://docs.vllm.ai/en/latest/serving/engine_args.html”。</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">其中--tensor-parallel-size 是指每个节点上GPU个数，--pipeline-parallel-size 指节点个数</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">输出内容很多，以下截取一部分（包括最后的输出内容）</span><br>...<br>Loading safetensors checkpoint shards:  96% Completed | 157/163 [00:33&lt;00:01,  3.41it/s]<br>Loading safetensors checkpoint shards:  97% Completed | 158/163 [00:33&lt;00:01,  3.86it/s]<br>Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:33&lt;00:00,  4.85it/s]<br><br>INFO 02-24 04:45:42 model_runner.py:1115] Loading model weights took 35.4806 GB<br>(RayWorkerWrapper pid=1124) INFO 02-24 04:45:44 model_runner.py:1115] Loading model weights took 35.4806 GB<br>(RayWorkerWrapper pid=7564, ip=10.119.165.139) INFO 02-24 04:45:08 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json [repeated 30x across cluster]<br>(RayWorkerWrapper pid=1138) INFO 02-24 04:45:08 cuda.py:161] Using Triton MLA backend. [repeated 31x across cluster]<br>(RayWorkerWrapper pid=976, ip=10.119.85.140) INFO 02-24 04:45:08 utils.py:950] Found nccl from library libnccl.so.2 [repeated 31x across cluster]<br>(RayWorkerWrapper pid=976, ip=10.119.85.140) INFO 02-24 04:45:08 pynccl.py:69] vLLM is using nccl==2.21.5 [repeated 31x across cluster]<br>(RayWorkerWrapper pid=1149) NCCL version 2.21.5+cuda12.4 [repeated 7x across cluster]<br>(RayWorkerWrapper pid=1149) deepseek2:1149:1149 [7] NCCL INFO Channel 15/0 : 7[7] -&gt; 6[6] via P2P/IPC/read [repeated 43x across cluster]<br>(RayWorkerWrapper pid=1116)  09/0 : 3[3] -&gt; 2[2] via P2P/IPC/read [repeated 2x across cluster]<br>(RayWorkerWrapper pid=1149) deepseek2:1149:1149 [7] NCCL INFO Connected all trees [repeated 6x across cluster]<br>(RayWorkerWrapper pid=1149) deepseek2:1149:1149 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512 [repeated 6x across cluster]<br>(RayWorkerWrapper pid=1149) deepseek2:1149:1149 [7] NCCL INFO 16 coll channels, 16 collnet channels, 0 nvls channels, 16 p2p channels, 16 p2p channels per peer [repeated 6x across cluster]<br>(RayWorkerWrapper pid=1149) deepseek2:1149:1149 [7] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so [repeated 6x across cluster]<br>(RayWorkerWrapper pid=1149) deepseek2:1149:1149 [7] NCCL INFO TUNER/Plugin: Using internal tuner plugin. [repeated 6x across cluster]<br>(RayWorkerWrapper pid=1149) deepseek2:1149:1149 [7] NCCL INFO ncclCommInitRank comm 0xd15faf0 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId d3000 commId 0x7f39c29d5bac68b4 - Init COMPLETE [repeated 6x across cluster]<br>(RayWorkerWrapper pid=1115) Channel 09/0 : 6[6] -&gt; 5[5] via P2P/IPC/read [repeated 2x across cluster]<br>(RayWorkerWrapper pid=973, ip=10.119.85.140) INFO 02-24 04:45:08 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip=&#x27;127.0.0.1&#x27;, local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, &#x27;psm_3a784ba1&#x27;), local_subscribe_port=56925, remote_subscribe_port=None) [repeated 2x across cluster]<br>(RayWorkerWrapper pid=976, ip=10.119.85.140) INFO 02-24 04:45:08 model_runner.py:1110] Starting to load model /root/.cache/huggingface/hub/models/unsloth/DeepSeek-R1-BF16/... [repeated 30x across cluster]<br>(RayWorkerWrapper pid=978, ip=10.119.85.140) INFO 02-24 04:46:17 model_runner.py:1115] Loading model weights took 42.8992 GB [repeated 7x across cluster]<br>...<br>...<br>...<br>(RayWorkerWrapper pid=7565, ip=10.119.165.139) deepseek1:7565:7565 [1] NCCL INFO P2P Chunksize set to 131072<br>(RayWorkerWrapper pid=7565, ip=10.119.165.139) deepseek1:7565:7565 [1] NCCL INFO Channel 00/0 : 2[1] -&gt; 3[1] [receive] via NET/IB/1<br>(RayWorkerWrapper pid=7565, ip=10.119.165.139) deepseek1:7565:7565 [1] NCCL INFO Channel 01/0 : 2[1] -&gt; 3[1] [receive] via NET/IB/1<br>(RayWorkerWrapper pid=7565, ip=10.119.165.139) deepseek1:7565:7565 [1] NCCL INFO Channel 00/0 : 3[1] -&gt; 0[1] [send] via NET/IB/1<br>(RayWorkerWrapper pid=7565, ip=10.119.165.139) deepseek1:7565:7565 [1] NCCL INFO Channel 01/0 : 3[1] -&gt; 0[1] [send] via NET/IB/1<br>(RayWorkerWrapper pid=7565, ip=10.119.165.139) deepseek1:7565:31505 [1] NCCL INFO NCCL_IB_GID_INDEX set by environment to 3.<br>(RayWorkerWrapper pid=7565, ip=10.119.165.139) deepseek1:7565:7565 [1] NCCL INFO Connected all rings<br>(RayWorkerWrapper pid=7565, ip=10.119.165.139) deepseek1:7565:7565 [1] NCCL INFO Channel 01/0 : 1[1] -&gt; 3[1] [receive] via NET/IB/1<br>(RayWorkerWrapper pid=7565, ip=10.119.165.139) deepseek1:7565:7565 [1] NCCL INFO Channel 01/0 : 3[1] -&gt; 1[1] [send] via NET/IB/1<br>(RayWorkerWrapper pid=7565, ip=10.119.165.139) deepseek1:7565:7565 [1] NCCL INFO Channel 00/0 : 3[1] -&gt; 2[1] [send] via NET/IB/1<br>(RayWorkerWrapper pid=7565, ip=10.119.165.139) deepseek1:7565:7565 [1] NCCL INFO Connected all trees<br>(RayWorkerWrapper pid=7565, ip=10.119.165.139) deepseek1:7565:7565 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512<br>(RayWorkerWrapper pid=7565, ip=10.119.165.139) deepseek1:7565:7565 [1] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer<br>(RayWorkerWrapper pid=7565, ip=10.119.165.139) deepseek1:7565:7565 [1] NCCL INFO ncclCommInitRank comm 0x1179c2d0 rank 3 nranks 4 cudaDev 1 nvmlDev 1 busId 42000 commId 0x8b115fa040bc44fc - Init COMPLETE<br>(RayWorkerWrapper pid=7565, ip=10.119.165.139) deepseek1:7565:31563 [1] NCCL INFO Using non-device net plugin version 0<br>(RayWorkerWrapper pid=7565, ip=10.119.165.139) deepseek1:7565:31563 [1] NCCL INFO Using network IB<br>(RayWorkerWrapper pid=7565, ip=10.119.165.139) deepseek1:7565:31563 [1] NCCL INFO ncclCommInitRank comm 0x30ca9860 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 42000 commId 0xd37e8e722df3123f - Init START<br>(RayWorkerWrapper pid=7565, ip=10.119.165.139) deepseek1:7565:31563 [1] NCCL INFO Setting affinity for GPU 1 to 0fffff,ff000000,0fffffff<br>(RayWorkerWrapper pid=7565, ip=10.119.165.139) deepseek1:7565:31563 [1] NCCL INFO NVLS multicast support is not available on dev 1<br>(RayWorkerWrapper pid=7565, ip=10.119.165.139) deepseek1:7565:31563 [1] NCCL INFO comm 0x30ca9860 rank 1 nRanks 8 nNodes 1 localRanks 8 localRank 1 MNNVL 0<br>(RayWorkerWrapper pid=7565, ip=10.119.165.139) deepseek1:7565:31563 [1] NCCL INFO Trees [0] 2/-1/-1-&gt;1-&gt;0 [1] 2/-1/-1-&gt;1-&gt;0 [2] 2/-1/-1-&gt;1-&gt;0 [3] 2/-1/-1-&gt;1-&gt;0 [4] 2/-1/-1-&gt;1-&gt;0 [5] 2/-1/-1-&gt;1-&gt;0 [6] 2/-1/-1-&gt;1-&gt;0 [7] 2/-1/-1-&gt;1-&gt;0 [8] 2/-1/-1-&gt;1-&gt;0 [9] 2/-1/-1-&gt;1-&gt;0 [10] 2/-1/-1-&gt;1-&gt;0 [11] 2/-1/-1-&gt;1-&gt;0 [12] 2/-1/-1-&gt;1-&gt;0 [13] 2/-1/-1-&gt;1-&gt;0 [14] 2/-1/-1-&gt;1-&gt;0 [15] 2/-1/-1-&gt;1-&gt;0<br>(RayWorkerWrapper pid=7565, ip=10.119.165.139) deepseek1:7565:31563 [1] NCCL INFO P2P Chunksize set to 524288<br>(RayWorkerWrapper pid=7565, ip=10.119.165.139) deepseek1:7565:31563 [1] NCCL INFO Channel 00/0 : 1[1] -&gt; 2[2] via P2P/IPC/read<br>(RayWorkerWrapper pid=7565, ip=10.119.165.139) deepseek1:7565:31563 [1] NCCL INFO Channel 01/0 : 1[1] -&gt; 2[2] via P2P/IPC/read<br>(RayWorkerWrapper pid=7565, ip=10.119.165.139) deepseek1:7565:31563 [1] NCCL INFO Channel 02/0 : 1[<br>(RayWorkerWrapper pid=7587, ip=10.119.165.139) ] NCCL INFO Connected all rings<br>(RayWorkerWrapper pid=7591, ip=10.119.165.139)  Channel 09/0 : 7[7] -&gt; 6[6] via P2P/IPC/read<br>(RayWorkerWrapper pid=7580, ip=10.119.165.139) 4[4] -&gt; 3[3] via P2P/IPC/read<br>(RayWorkerWrapper pid=7580, ip=10.119.165.139) deepseek1:7580:31567 [4] NCCL I<br>(RayWorkerWrapper pid=7591, ip=10.119.165.139) <br>(RayWorkerWrapper pid=7591, ip=10.119.165.139) deepseek1:759<br>(RayWorkerWrapper pid=7580, ip=10.119.165.139) NFO Channel 02/0 : 4[4] -&gt; 5[5] via P2P/IPC/read<br>(RayWorkerWrapper pid=7580, ip=10.119.165.139) deepseek1:7580:31594 [4] NCCL INFO Channel 15<br>INFO 02-24 17:15:16 executor_base.py:110] # CUDA blocks: 68440, # CPU blocks: 13107<br>...<br>...<br>...<br>Capturing CUDA graph shapes:  43%|████████████████████████████████████████████████████████████▍                                                                                | 15/35 [00:06&lt;00:09,  2.18it/s]<br>Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:13&lt;00:01,  2.46it/s]<br>Capturing CUDA graph shapes:  46%|████████████████████████████████████████████████████████████████▍                                                                            | 16/35 [00:07&lt;00:08,  2.18it/s]<br>Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:13&lt;00:00,  2.46it/s]<br>Capturing CUDA graph shapes:  49%|████████████████████████████████████████████████████████████████████▍                                                                        | 17/35 [00:07&lt;00:08,  2.19it/s]<br>Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:13&lt;00:00,  2.47it/s]<br>Capturing CUDA graph shapes:  51%|████████████████████████████████████████████████████████████████████████▌                                                                    | 18/35 [00:08&lt;00:07,  2.13it/s]<br>Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:14&lt;00:00,  2.47it/s]<br>Capturing CUDA graph shapes:  63%|████████████████████████████████████████████████████████████████████████████████████████▋                                                    | 22/35 [00:10&lt;00:06,  2.07it/s]<br>Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:13&lt;00:01,  2.36it/s] [repeated 24x across cluster]<br>Capturing CUDA graph shapes:  66%|████████████████████████████████████████████████████████████████████████████████████████████▋                                                | 23/35 [00:10&lt;00:05,  2.08it/s]<br>(RayWorkerWrapper pid=974, ip=10.119.85.140) INFO 02-24 04:48:08 model_runner.py:1562] Graph capturing finished in 64 secs, took 1.19 GiB<br>Capturing CUDA graph shapes:  71%|████████████████████████████████████████████████████████████████████████████████████████████████████▋                                        | 25/35 [00:11&lt;00:04,  2.04it/s]<br>(RayWorkerWrapper pid=7561, ip=10.119.165.139) INFO 02-24 04:48:09 custom_all_reduce.py:226] Registering 4480 cuda graph addresses [repeated 16x across cluster]<br>Capturing CUDA graph shapes: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:15&lt;00:00,  2.19it/s]<br>INFO 02-24 04:48:13 custom_all_reduce.py:226] Registering 4340 cuda graph addresses<br>(RayWorkerWrapper pid=7564, ip=10.119.165.139) INFO 02-24 04:48:13 model_runner.py:1562] Graph capturing finished in 68 secs, took 1.19 GiB [repeated 23x across cluster]<br>(RayWorkerWrapper pid=1138) INFO 02-24 04:48:18 custom_all_reduce.py:226] Registering 4340 cuda graph addresses [repeated 14x across cluster]<br>INFO 02-24 04:48:18 model_runner.py:1562] Graph capturing finished in 73 secs, took 1.16 GiB<br>INFO 02-24 04:48:18 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 84.43 seconds<br>INFO 02-24 04:48:18 api_server.py:756] Using supplied chat template:<br>INFO 02-24 04:48:18 api_server.py:756] None<br>INFO 02-24 04:48:18 launcher.py:21] Available routes are:<br>INFO 02-24 04:48:18 launcher.py:29] Route: /openapi.json, Methods: HEAD, GET<br>INFO 02-24 04:48:18 launcher.py:29] Route: /docs, Methods: HEAD, GET<br>INFO 02-24 04:48:18 launcher.py:29] Route: /docs/oauth2-redirect, Methods: HEAD, GET<br>INFO 02-24 04:48:18 launcher.py:29] Route: /redoc, Methods: HEAD, GET<br>INFO 02-24 04:48:18 launcher.py:29] Route: /health, Methods: GET<br>INFO 02-24 04:48:18 launcher.py:29] Route: /ping, Methods: GET, POST<br>INFO 02-24 04:48:18 launcher.py:29] Route: /tokenize, Methods: POST<br>INFO 02-24 04:48:18 launcher.py:29] Route: /detokenize, Methods: POST<br>INFO 02-24 04:48:18 launcher.py:29] Route: /v1/models, Methods: GET<br>INFO 02-24 04:48:18 launcher.py:29] Route: /version, Methods: GET<br>INFO 02-24 04:48:18 launcher.py:29] Route: /v1/chat/completions, Methods: POST<br>INFO 02-24 04:48:18 launcher.py:29] Route: /v1/completions, Methods: POST<br>INFO 02-24 04:48:18 launcher.py:29] Route: /v1/embeddings, Methods: POST<br>INFO 02-24 04:48:18 launcher.py:29] Route: /pooling, Methods: POST<br>INFO 02-24 04:48:18 launcher.py:29] Route: /score, Methods: POST<br>INFO 02-24 04:48:18 launcher.py:29] Route: /v1/score, Methods: POST<br>INFO 02-24 04:48:18 launcher.py:29] Route: /rerank, Methods: POST<br>INFO 02-24 04:48:18 launcher.py:29] Route: /v1/rerank, Methods: POST<br>INFO 02-24 04:48:18 launcher.py:29] Route: /v2/rerank, Methods: POST<br>INFO 02-24 04:48:18 launcher.py:29] Route: /invocations, Methods: POST<br>INFO:     Started server process [12020]<br>INFO:     Waiting for application startup.<br>INFO:     Application startup complete.<br>INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)<br></code></pre></td></tr></table></figure>
<figure>
<img src="https://s2.loli.net/2025/02/24/V8FOf1Q4Z6Ww9sg.png" srcset="/img/loading.gif" lazyload
alt="image-20250224205008780" />
<figcaption aria-hidden="true">image-20250224205008780</figcaption>
</figure>
<h4 id="报错与处理-2">报错与处理</h4>
<h5
id="问题1-执行vllm-serve提示error-executing-method-determine_num_available_blocks.-this-might-cause-deadlock-in-distributed-execution">问题1-执行"vllm
serve"提示“Error executing method 'determine_num_available_blocks'. This
might cause deadlock in distributed execution”</h5>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">执行如下命令时报错，报错信息的部分如下：</span><br>root@deepseek1:/vllm-workspace# vllm serve /root/.cache/huggingface/hub/models/deepseek-ai/DeepSeek-R1 \<br>    --tensor-parallel-size 8 \<br>    --pipeline-parallel-size 4 \<br>    --trust-remote-code \<br>    --host 0.0.0.0 \<br>    --port 8000 \<br>    --enforce-eager<br>...<br>Loading safetensors checkpoint shards:  95% Completed | 155/163 [00:10&lt;00:00, 22.69it/s]<br>Loading safetensors checkpoint shards:  97% Completed | 158/163 [00:11&lt;00:00, 18.17it/s]<br>Loading safetensors checkpoint shards:  99% Completed | 161/163 [00:11&lt;00:00, 18.87it/s]<br>Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:11&lt;00:00, 14.15it/s]<br><br>INFO 02-22 06:50:13 model_runner.py:1115] Loading model weights took 18.1361 GB<br>(RayWorkerWrapper pid=5507) INFO 02-22 06:50:18 model_runner.py:1115] Loading model weights took 18.1361 GB<br>(pid=1451, ip=10.119.165.141) INFO 02-22 06:49:57 __init__.py:190] Automatically detected platform cuda. [repeated 31x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)<br>(RayWorkerWrapper pid=1447, ip=10.119.165.138) INFO 02-22 06:50:01 cuda.py:161] Using Triton MLA backend. [repeated 61x across cluster]<br>(RayWorkerWrapper pid=1451, ip=10.119.165.141) WARNING 02-22 06:49:59 triton_decode_attention.py:44] The following error message &#x27;operation scheduled before its operands&#x27; can be ignored. [repeated 30x across cluster]<br>(RayWorkerWrapper pid=5486) INFO 02-22 06:50:01 utils.py:950] Found nccl from library libnccl.so.2 [repeated 61x across cluster]<br>(RayWorkerWrapper pid=5486) INFO 02-22 06:50:01 pynccl.py:69] vLLM is using nccl==2.21.5 [repeated 61x across cluster]<br>(RayWorkerWrapper pid=1447, ip=10.119.165.138) INFO 02-22 06:50:01 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json [repeated 30x across cluster]<br>(RayWorkerWrapper pid=1444, ip=10.119.165.141) INFO 02-22 06:50:01 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip=&#x27;127.0.0.1&#x27;, local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, &#x27;psm_9dec0a20&#x27;), local_subscribe_port=44443, remote_subscribe_port=None) [repeated 2x across cluster]<br>(RayWorkerWrapper pid=1451, ip=10.119.165.141) INFO 02-22 06:50:01 model_runner.py:1110] Starting to load model /root/.cache/huggingface/hub/models/deepseek-ai/DeepSeek-R1... [repeated 30x across cluster]<br>(RayWorkerWrapper pid=1451, ip=10.119.165.141) WARNING 02-22 06:50:01 utils.py:159] The model class DeepseekV3ForCausalLM has not defined `packed_modules_mapping`, this may lead to incorrect mapping of quantized or ignored modules [repeated 30x across cluster]<br>(RayWorkerWrapper pid=1450, ip=10.119.165.141) INFO 02-22 06:50:23 model_runner.py:1115] Loading model weights took 23.1804 GB [repeated 23x across cluster]<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574] Error executing method &#x27;determine_num_available_blocks&#x27;. This might cause deadlock in distributed execution.<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574] Traceback (most recent call last):<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]   File &quot;/usr/local/lib/python3.12/dist-packages/triton/language/core.py&quot;, line 35, in wrapper<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     return fn(*args, **kwargs)<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]            ^^^^^^^^^^^^^^^^^^^<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]   File &quot;/usr/local/lib/python3.12/dist-packages/triton/language/core.py&quot;, line 993, in to<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     return semantic.cast(self, dtype, _builder, fp_downcast_rounding)<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]   File &quot;/usr/local/lib/python3.12/dist-packages/triton/language/semantic.py&quot;, line 759, in cast<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     assert builder.options.allow_fp8e4nv, &quot;fp8e4nv data type is not supported on CUDA arch &lt; 89&quot;<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574] AssertionError: fp8e4nv data type is not supported on CUDA arch &lt; 89<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574] <br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574] The above exception was the direct cause of the following exception:<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574] <br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574] Traceback (most recent call last):<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]   File &quot;/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py&quot;, line 566, in execute_method<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     return run_method(target, method, args, kwargs)<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]   File &quot;/usr/local/lib/python3.12/dist-packages/vllm/utils.py&quot;, line 2220, in run_method<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     return func(*args, **kwargs)<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]            ^^^^^^^^^^^^^^^^^^^^^<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]   File &quot;/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py&quot;, line 116, in decorate_context<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     return func(*args, **kwargs)<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]            ^^^^^^^^^^^^^^^^^^^^^<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]   File &quot;/usr/local/lib/python3.12/dist-packages/vllm/worker/worker.py&quot;, line 229, in determine_num_available_blocks<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     self.model_runner.profile_run()<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]   File &quot;/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py&quot;, line 116, in decorate_context<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     return func(*args, **kwargs)<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]            ^^^^^^^^^^^^^^^^^^^^^<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]   File &quot;/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py&quot;, line 1235, in profile_run<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     self._dummy_run(max_num_batched_tokens, max_num_seqs)<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]   File &quot;/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py&quot;, line 1346, in _dummy_run<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     self.execute_model(model_input, kv_caches, intermediate_tensors)<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]   File &quot;/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py&quot;, line 116, in decorate_context<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     return func(*args, **kwargs)<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]            ^^^^^^^^^^^^^^^^^^^^^<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]   File &quot;/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py&quot;, line 1719, in execute_model<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     hidden_or_intermediate_states = model_executable(<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]                                     ^^^^^^^^^^^^^^^^^<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]   File &quot;/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py&quot;, line 1736, in _wrapped_call_impl<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     return self._call_impl(*args, **kwargs)<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]   File &quot;/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py&quot;, line 1747, in _call_impl<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     return forward_call(*args, **kwargs)<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]   File &quot;/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py&quot;, line 687, in forward<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     hidden_states = self.model(input_ids, positions, kv_caches,<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]   File &quot;/usr/local/lib/python3.12/dist-packages/vllm/compilation/decorators.py&quot;, line 172, in __call__<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     return self.forward(*args, **kwargs)<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]   File &quot;/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py&quot;, line 643, in forward<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     hidden_states, residual = layer(positions, hidden_states,<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]   File &quot;/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py&quot;, line 1736, in _wrapped_call_impl<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     return self._call_impl(*args, **kwargs)<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]   File &quot;/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py&quot;, line 1747, in _call_impl<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     return forward_call(*args, **kwargs)<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]   File &quot;/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py&quot;, line 561, in forward<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     hidden_states = self.self_attn(<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]                     ^^^^^^^^^^^^^^^<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]   File &quot;/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py&quot;, line 1736, in _wrapped_call_impl<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     return self._call_impl(*args, **kwargs)<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]   File &quot;/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py&quot;, line 1747, in _call_impl<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     return forward_call(*args, **kwargs)<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]   File &quot;/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/deepseek_v2.py&quot;, line 473, in forward<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     ckq = self.q_a_proj(hidden_states)[0]<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]   File &quot;/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py&quot;, line 1736, in _wrapped_call_impl<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     return self._call_impl(*args, **kwargs)<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]   File &quot;/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py&quot;, line 1747, in _call_impl<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     return forward_call(*args, **kwargs)<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]   File &quot;/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/linear.py&quot;, line 248, in forward<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     output = self.quant_method.apply(self, x, bias)<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]   File &quot;/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/fp8.py&quot;, line 359, in apply<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     return apply_w8a8_block_fp8_linear(<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]   File &quot;/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/utils/fp8_utils.py&quot;, line 70, in apply_w8a8_block_fp8_linear<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     q_input, x_scale = per_token_group_quant_fp8(input_2d,<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]   File &quot;/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/utils/fp8_utils.py&quot;, line 307, in per_token_group_quant_fp8<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     _per_token_group_quant_fp8[(M, )](<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]   File &quot;/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py&quot;, line 345, in &lt;lambda&gt;<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]   File &quot;/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py&quot;, line 662, in run<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     kernel = self.compile(<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]              ^^^^^^^^^^^^^<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]   File &quot;/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py&quot;, line 276, in compile<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     module = src.make_ir(options, codegen_fns, context)<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]   File &quot;/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py&quot;, line 113, in make_ir<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns)<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574] triton.compiler.errors.CompilationError: at 32:10:<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     y_ptr += g_id * group_size<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     y_q_ptr += g_id * group_size<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     y_s_ptr += g_id<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574] <br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     cols = tl.arange(0, BLOCK)  # N &lt;= BLOCK<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     mask = cols &lt; group_size<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574] <br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     y = tl.load(y_ptr + cols, mask=mask, other=0.0).to(tl.float32)<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     # Quant<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     _absmax = tl.maximum(tl.max(tl.abs(y)), eps)<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     y_s = _absmax / fp8_max<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]     y_q = tl.clamp(y / y_s, fp8_min, fp8_max).to(y_q_ptr.dtype.element_ty)<br>(RayWorkerWrapper pid=1444, ip=10.119.165.140) ERROR 02-22 06:50:24 worker_base.py:574]           ^<br>ERROR 02-22 06:50:25 worker_base.py:574] Error executing method &#x27;determine_num_available_blocks&#x27;. This might cause deadlock in distributed execution.<br>...<br></code></pre></td></tr></table></figure>
<p>问题分析：</p>
<p>DeepSeek-R1:671B是fp8
模型，需要在支持fp8数据类型的GPU上运行，但NVIDIA
A800（A800跟A100设计上基本上是一样的，只是减配了）不支持fp8数据类型。https://github.com/vllm-project/vllm/issues/12581#issuecomment-2625310475
、
https://github.com/vllm-project/vllm/issues?q=fp8e4nv%20data%20type%20is%20not%20supported%20on%20CUDA%20arch%20%3C%2089、https://github.com/vllm-project/vllm/issues/12581</p>
<h3 id="再次查看ray集群状态">3.1.6 再次查看ray集群状态</h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs shell">root@deepseek1:/vllm-workspace# ray status<br>======== Autoscaler status: 2025-02-24 06:09:38.499412 ========<br>Node status<br>---------------------------------------------------------------<br>Active:<br> 1 node_78c610008905942ec65274e7c7ce990d1a554e9627512bf633c15c28<br> 1 node_0aee3e0efd8a7f95dfa4205cd692f7f08e7d665b779a0facf0d3201d<br> 1 node_8407508c22842dea6182c7accc2565b42daf4c7b051f1d37a4629258<br> 1 node_3ec95e5ee056a4484f0f81cc518716edd4d2bfd98ffa771b024edc27<br>Pending:<br> (no pending nodes)<br>Recent failures:<br> (no failures)<br><br>Resources<br>---------------------------------------------------------------<br>Usage:<br> 0.0/448.0 CPU<br> 32.0/32.0 GPU (32.0 used of 32.0 reserved in placement groups)<br> 0B/3.89TiB memory<br> 0B/38.91GiB object_store_memory<br><br>Demands:<br> (no resource demands)<br><span class="hljs-meta prompt_"> </span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">32块GPU都处于使用状态了</span><br></code></pre></td></tr></table></figure>
<h3 id="production-metrics">3.1.7 Production Metrics</h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs shell">(self-llm) deepseek@deepseek2:~$ curl http://10.119.85.138:8000/metrics<br>540    0     # TYPE python_gc_objects_collected_total counter<br>0  7756k   python_gc_objects_collected_total&#123;generation=&quot;0&quot;&#125; 37427.0<br>   0 --:python_gc_objects_collected_total&#123;generation=&quot;1&quot;&#125; 14232.0<br>--:-- --:--:-- python_gc_objects_collected_total&#123;generation=&quot;2&quot;&#125; 16818.0<br>--:--:-- 9615k<br><span class="hljs-meta prompt_"># </span><span class="language-bash">HELP python_gc_objects_uncollectable_total Uncollectable objects found during GC</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">TYPE python_gc_objects_uncollectable_total counter</span><br>python_gc_objects_uncollectable_total&#123;generation=&quot;0&quot;&#125; 0.0<br>python_gc_objects_uncollectable_total&#123;generation=&quot;1&quot;&#125; 0.0<br>python_gc_objects_uncollectable_total&#123;generation=&quot;2&quot;&#125; 0.0<br><span class="hljs-meta prompt_"># </span><span class="language-bash">HELP python_gc_collections_total Number of <span class="hljs-built_in">times</span> this generation was collected</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">TYPE python_gc_collections_total counter</span><br>python_gc_collections_total&#123;generation=&quot;0&quot;&#125; 3033.0<br>python_gc_collections_total&#123;generation=&quot;1&quot;&#125; 267.0<br>python_gc_collections_total&#123;generation=&quot;2&quot;&#125; 315.0<br>...<br></code></pre></td></tr></table></figure>
<h3 id="openai-api接口测试">3.1.8 openai API接口测试</h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">其中 10.119.85.138 是deepseek2节点的IB网卡IP</span><br>(self-llm) deepseek@deepseek2:~$ curl 10.119.85.138:8000/v1/models -H &quot;Authorization: Bearer zY0MrQwXV9Oo3g==&quot; | jq  <br><span class="hljs-meta prompt_">#</span><span class="language-bash">输出内容如下</span><br><span class="hljs-meta prompt_">  % </span><span class="language-bash">Total    % Received % Xferd  Average Speed   Time    Time     Time  Current</span><br>                                 Dload  Upload   Total   Spent    Left  Speed<br>100   523  100   523    0     0   105k      0 --:--:-- --:--:-- --:--:--  127k<br>&#123;<br>  &quot;object&quot;: &quot;list&quot;,<br>  &quot;data&quot;: [<br>    &#123;<br>      &quot;id&quot;: &quot;DeepSeek-R1-671B&quot;,<br>      &quot;object&quot;: &quot;model&quot;,<br>      &quot;created&quot;: 1740405511,<br>      &quot;owned_by&quot;: &quot;vllm&quot;,<br>      &quot;root&quot;: &quot;/root/.cache/huggingface/hub/models/unsloth/DeepSeek-R1-BF16/&quot;,<br>      &quot;parent&quot;: null,<br>      &quot;max_model_len&quot;: 32768,<br>      &quot;permission&quot;: [<br>        &#123;<br>          &quot;id&quot;: &quot;modelperm-ced685e8156b4618b593580109205165&quot;,<br>          &quot;object&quot;: &quot;model_permission&quot;,<br>          &quot;created&quot;: 1740405511,<br>          &quot;allow_create_engine&quot;: false,<br>          &quot;allow_sampling&quot;: true,<br>          &quot;allow_logprobs&quot;: true,<br>          &quot;allow_search_indices&quot;: false,<br>          &quot;allow_view&quot;: true,<br>          &quot;allow_fine_tuning&quot;: false,<br>          &quot;organization&quot;: &quot;*&quot;,<br>          &quot;group&quot;: null,<br>          &quot;is_blocking&quot;: false<br>        &#125;<br>      ]<br>    &#125;<br>  ]<br>&#125;<br></code></pre></td></tr></table></figure>
<p>同时在vllm serve命令执行的窗口会看到如下输出内容</p>
<figure>
<img src="https://s2.loli.net/2025/02/24/F3td4gkAeSfwijK.png" srcset="/img/loading.gif" lazyload
alt="image-20250224215955409" />
<figcaption aria-hidden="true">image-20250224215955409</figcaption>
</figure>
<h3 id="服务功能验证">3.1.9 服务功能验证</h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs shell">(self-llm) deepseek@deepseek2:~$ curl -X POST &quot;http://10.119.85.138:8000/v1/chat/completions&quot; -H &quot;Content-Type: application/json&quot; -H &quot;Authorization: Bearer zY0MrQwXV9Oo3g==&quot;  -d &#x27;&#123; &quot;model&quot;: &quot;DeepSeek-R1-671B&quot;, &quot;messages&quot;: [&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;你好&quot;&#125;]&#125;&#x27;<br>    <br>(self-llm) deepseek@deepseek2:~$ curl -X POST &quot;http://10.119.85.138:8000/v1/chat/completions&quot; -H &quot;Content-Type: application/json&quot; -H &quot;Authorization: Bearer zY0MrQwXV9Oo3g==&quot;  -d &#x27;&#123; &quot;model&quot;: &quot;DeepSeek-R1-671B&quot;, &quot;messages&quot;: [&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;请证明勾股定理&quot;&#125;]&#125;&#x27;<br><span class="hljs-meta prompt_">#</span><span class="language-bash">回答</span><br>&#123;&quot;id&quot;:&quot;chatcmpl-11ae1ddf321343af848b5c683e67b72d&quot;,&quot;object&quot;:&quot;chat.completion&quot;,&quot;created&quot;:1740411348,&quot;model&quot;:&quot;deepseek-r1&quot;,&quot;choices&quot;:[&#123;&quot;index&quot;:0,&quot;message&quot;:&#123;&quot;role&quot;:&quot;assistant&quot;,&quot;reasoning_content&quot;:null,&quot;content&quot;:&quot;&lt;think&gt;\n嗯，用户让我证明勾股定理。勾股定理是数学里非常基础但又重要的定理，肯定有很多不同的证明方法。先回忆一下，勾股定理是说在直角三角形中，斜边的平方等于两条直角边的平方和，也就是a² + b² = c²。现在得选一<br>种合适的证明方式，可能是几何的或者代数的。\n\n首先想到的是几何证明中的拼接法，通过将四个直角三角形拼成一个大的正方形，然后比较面积。我要不要试试这个方法？比如说，四个全等的直角三角形，设它们的直角边为a和b，<br>斜边为c，拼起来的话中间应该会形成一个边长为（a+b）的正方形，中间的空隙可能是小正方形，边长是c或者别的？\n\n不对，应该会形成一个边长为c的正方形，或者这个？等一下，可能得仔细画个图想象一下。假设我们把四个三角<br>形每个的直角边朝外，那斜边就会组成里面的一个正方形，这时候那个正方形的边长应该是a - b吗？或者可能是其他情况？\n\n另一个方法是代数证明，利用相似三角形或者欧几里得的证明方式。我记得欧几里得在《几何原本》里的证明比较复杂，可能不太直观，适合进阶的学生。但普通学生应该更容易接受面积拼接法，或者是利用代数展开。还有一种方法是总统证明法，据说是美国第20任总统加菲尔德想到的一种梯形面积法，对吗？\n\n比如说，画一个梯形，由<br>两个直角三角形和一个等腰直角三角形组成，计算面积两种方式。可能这个更简单。先复习一下这种方法的具体步骤。总面积的公式应该等于梯形的高乘以上底加下底的一半，对吗？\n\n或者回到基本的拼图方法。四个直角三角形，边<br>长为a和b，斜边c，将它们放在一个大正方形里面，这样有两种不同的拼法，分别形成边长为a + b的大正方形，一种是四个三角形围成一个边长为c的正方形，另一种则是中间留有边长为a和b的小正方形。\n\n这时候整个大正方形的面积有两种表达式，一种等于(a + b)²，另一种等于4个三角形的面积加上中间的正方形面积，也就是4*(1/2 ab) + c²。另一边，同一个大正方形的面积也可以通过另一种拼法得到，这时候中间可能是两个小正方形，边长分别是a和b，所以<br>总面积是4*(1/2 ab) + a² + b²。这样等式两边相等的话就能得到a² + b² = c²。这可能就是常见的拼图法证明。\n\n或者考虑用相似三角形的比例来推导。在直角三角形中，画一条高，由此产生两个小三角形，跟原来的大三角形相似<br>。根据相似三角形的边长比例，各个边的关系可以得出勾股定理。\n\n比如，设原三角形ABC，C为直角，从C向AB边作高，交于D点。这样，AD和DB分别是两个小三角形的边，之后利用比例关系得出AC² = AD * AB，BC² = DB * AB，相加<br>后得出AC² + BC² = AB²（AD + DB）= AB²，这样就得到结论。这可能是欧几里得的方法。\n\n不过，可能用户需要的是更直观的证明方法，比如面积法，可能适合更多人理解。像用四个直角三角形围成一个大的正方形，中间形成的部分是c²的面积，然后另外方式拼接得到a² + b²的总面积，两者相等。\n\n这个时候应该详细画出这个图形。但因为是文字描述，我需要解释清楚：将四个直角三角形以边长为a和b的直角边向外，组成一个大正方形，边长为a + b，中间的<br>区域则是一个边长为c的正方形。大正方形的面积等于四个三角形的面积再加上中间正方形的面积。也就是(a + b)² = 4*(1/2 ab) + c²。展开左边得到a² + 2ab + b² = 2ab + c²，两边减去2ab即得a² + b² = c²，得证。\n\n或者，换<br>一种方式拼凑，以直角边为排列方向，或许中间形成的区域会是两个小正方形，边长a和b，这可能导致同样的结论。但上述的证明应该更直接，步骤更简单。\n\n另一个可能是代数计算法，比如构造一个边长为c的正方形，分割成四个三角形和一个中间的小正方形，不过这可能需要不同的分割方式。这时同样是利用面积平衡去推导。\n\n确定之后，我觉得用四个三角形的方法，也就是所谓的“面积拼图法”应该是最直接的方式。这个方法的步骤比较清晰，而且代数运算<br>简单，只有几步展开，容易理解。\n\n另外，要注意用户的疑问可能是什么。这些证明方法是不是正确的？有没有逻辑漏洞？比如在拼接过程中，是否真能组成一个正方形？是否存在对应的几何条件是否符合，例如直角是否拼接正确，<br>边长是否确实a + b等等。\n\n所以需要在证明过程中详细说明图形是如何构造的，确保各个边对的长度相等，角度正确，这样才能保证中间区域是正方形。例如，四个直角三角形的直角边a和b交替排列，使得各外角都是直角，从而外框是一个边长为a + b的正方形。而中间的部分因为每个边都是斜边c，每个角都是原来的直角拼接后的角度，所以中间肯定是边长为c的正方形。\n\n这样的话，整个过程在几何上是严密的，可以放心使用。\n\n总结一下，我打算使用面积拼接法来证明勾股定理，详细描述如何将四个直角三角形拼成一个大正方形，然后通过两种不同的面积计算方式得到方程，从而解出a² + b² = c²。这种方法直观且步骤清晰，适合各种数学基础的用户理解。同时，提醒自己需要检查各<br>个步骤是否正确，例如正方形的构造是否正确，面积计算是否有误，代数推导是否严谨。\n&lt;/think&gt;\n\n勾股定理表明，在直角三角形中，斜边长（\\( c \\)）的平方等于两直角边（\\( a \\) 和 \\( b \\)）的平方和，即  \n\\[\na^2 + b^2 = c^2.\n\\]  \n**证明**（面积拼接法）：  \n\n1. **构造图形**：  \n   取四个全等的直角三角形，其直角边分别为 \\( a \\) 和 \\( b \\)，斜边为 \\( c \\)。将它们以直角相接的方式拼接成一个大正方形，如图1<br>所示。  \n   - 外框大正方形的边长为 \\( a + b \\)。  \n   - 中间空出的区域是一个边长为 \\( c \\) 的小正方形。  \n\n   ![图1：四个直角三角形构成边长为 \\( a + b \\) 的大正方形，中间空出边长为 \\( c \\) 的小正方形](https://i.imgur.com/yyNUyzB.png)  \n\n2. **计算总面积**：  \n   外框大正方形的面积可表示为：  \n   \\[\n   (a + b)^2 = a^2 + 2ab + b^2.\n   \\]  \n\n3. **另一种方式的总面积**：  \n   总面积也可以看作四个直角三角形的面积加上中间小正方形的面积：  \n   \\[\n   \\text&#123;总面积&#125; = 4 \\times \\left( \\frac&#123;1&#125;&#123;2&#125;ab \\right) + c^2 = 2ab + c^2.\n   \\]  \n\n4. **联立方程**：  \n   将两种表达式联立：  \n   \\[\n   a^2 + 2ab + b^2 = 2ab + c^2.\n   \\]  \n   两边减去 \\( 2ab \\)，得到：  \n   \\[\n   a^2 + b^2 = c^2.\n   \\]  \n\n**结论**：在直角三角形中，斜边的平方等于两直角边的平方和。此证法通过几何构造与代数运算的结合<br>，直观展示了勾股定理的必然性。&quot;,&quot;tool_calls&quot;:[]&#125;,&quot;logprobs&quot;:null,&quot;finish_reason&quot;:&quot;stop&quot;,&quot;stop_reason&quot;:null&#125;],&quot;usage&quot;:&#123;&quot;prompt_tokens&quot;:8,&quot;total_tokens&quot;:1606,&quot;completion_tokens&quot;:1598,&quot;prompt_tokens_details&quot;:null&#125;,&quot;prompt_logprobs&quot;:null&#125;<br></code></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">回答问题的同时在vllm serve命令执行的窗口会看到如下，显示token平均生成吞吐率</span><br>INFO 02-24 17:21:12 metrics.py:455] Avg prompt throughput: 1.6 tokens/s, Avg generation throughput: 36.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.<br>INFO 02-24 17:21:17 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 37.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.<br>INFO 02-24 17:21:22 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 36.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.<br>...<br><span class="hljs-meta prompt_">#</span><span class="language-bash">甚至更高速度</span><br>INFO 02-24 23:32:00 metrics.py:455] Avg prompt throughput: 442.9 tokens/s, Avg generation throughput: 38.8 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.<br>INFO 02-24 23:32:05 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.4 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.<br>INFO 02-24 23:32:07 async_llm_engine.py:179] Finished request chatcmpl-03add50cba264c84afe98fd6cce9907f.<br>INFO 02-24 23:32:10 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 79.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.<br></code></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">apt install nvtop</span><br>(self-llm) deepseek@deepseek1:~/installPkgs$ nvtop<br><span class="hljs-meta prompt_">#</span><span class="language-bash">如下是`nvtop`命令输出</span><br></code></pre></td></tr></table></figure>
<figure>
<img src="https://s2.loli.net/2025/02/24/iqRKSIfvJEY9sm4.png" srcset="/img/loading.gif" lazyload
alt="image-20250224233912301" />
<figcaption aria-hidden="true">image-20250224233912301</figcaption>
</figure>
<h2 id="运行openwebui">3.2 运行OpenWebUI</h2>
<h3 id="下载镜像">3.2.1 下载镜像</h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">下载容器镜像</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-built_in">sudo</span> docker pull ghcr.io/open-webui/open-webui:v0.5.10</span><br>sudo docker pull ghcr.io/open-webui/open-webui:v0.5.16<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">如果无法下载或下载速度太慢，也可以使用如下镜像，它们是同步的（建议使用此镜像）</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-built_in">sudo</span> docker pull swr.cn-north-4.myhuaweicloud.com/ddn-k8s/ghcr.io/open-webui/open-webui:v0.5.10</span><br>sudo docker pull swr.cn-north-4.myhuaweicloud.com/ddn-k8s/ghcr.io/open-webui/open-webui:v0.5.16<br></code></pre></td></tr></table></figure>
<h3 id="启动open-webui容器">3.2.2 启动open-webui容器</h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs shell">(self-llm) deepseek@deepseek2:~$ sudo docker run -d --name open-webui \<br>	--hostname=open-webui \<br>    --volume=open-webui:/app/backend/data \<br>    --workdir=/app/backend \<br>    -p 18080:8080 \<br>    --restart always \<br>	--runtime=nvidia \<br>    --env=OPENAI_API_BASE_URL=http://10.119.85.138:8000/v1 \<br>    --env=OPENAI_API_KEY=zY0MrQwXV9Oo3g== \<br>    --env=ENABLE_OLLAMA_API=false \<br>    --env=ENABLE_SIGNUP=true \<br>    swr.cn-north-4.myhuaweicloud.com/ddn-k8s/ghcr.io/open-webui/open-webui:v0.5.16 \<br>    bash start.sh<br><span class="hljs-meta prompt_">#</span><span class="language-bash">其中OPENAI_API_BASE_URL 指定了vllm暴露的openai API 服务地址</span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">等待，直到open-webui容器状态更新为healthy</span><br>(self-llm) deepseek@deepseek2:~$ sudo watch docker ps -a<br><span class="hljs-meta prompt_">#</span><span class="language-bash">实时更新查看open-webui容器日志</span><br>(self-llm) deepseek@deepseek2:~$ sudo docker logs open-webui -f<br>Loading WEBUI_SECRET_KEY from file, not provided as an environment variable.<br>Generating WEBUI_SECRET_KEY<br>Loading WEBUI_SECRET_KEY from .webui_secret_key<br>/app/backend/open_webui<br>/app/backend<br>/app<br>Running migrations<br>INFO  [alembic.runtime.migration] Context impl SQLiteImpl.<br>INFO  [alembic.runtime.migration] Will assume non-transactional DDL.<br>INFO  [open_webui.env] &#x27;ENABLE_SIGNUP&#x27; loaded from the latest database entry<br>INFO  [open_webui.env] &#x27;DEFAULT_LOCALE&#x27; loaded from the latest database entry<br>INFO  [open_webui.env] &#x27;DEFAULT_PROMPT_SUGGESTIONS&#x27; loaded from the latest database entry<br>WARNI [open_webui.env] <br><br>WARNING: CORS_ALLOW_ORIGIN IS SET TO &#x27;*&#x27; - NOT RECOMMENDED FOR PRODUCTION DEPLOYMENTS.<br><br>INFO  [open_webui.env] Embedding model set: sentence-transformers/all-MiniLM-L6-v2<br>WARNI [langchain_community.utils.user_agent] USER_AGENT environment variable not set, consider setting it to identify your requests.<br>INFO:     Started server process [1]<br>INFO:     Waiting for application startup.<br>INFO:     Application startup complete.<br>INFO:     Uvicorn running on http://0.0.0.0:8080 (Press CTRL+C to quit)  #出现了日志内容就是可用了<br></code></pre></td></tr></table></figure>
<h3 id="访问open-webui界面">3.2.3 访问open-webui界面</h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">其中 10.119.85.138 是deepseek2节点的IB网卡IP</span><br>(self-llm) deepseek@deepseek2:~$ curl http://10.119.85.138:18080<br><span class="hljs-meta prompt_">#</span><span class="language-bash">或在浏览器中直接访问上述地址。第一个注册的用户，默认就是管理员。注册后登录、提问</span><br></code></pre></td></tr></table></figure>
<figure>
<img src="https://s2.loli.net/2025/02/25/u9lErBStzK3sULx.png" srcset="/img/loading.gif" lazyload
alt="image-20250225154520858" />
<figcaption aria-hidden="true">image-20250225154520858</figcaption>
</figure>
<h2 id="备注说明">3.3 备注说明</h2>
<p>要学与做的东西很多，此笔记只是一个记录分享。</p>
<p>当然后续希望：</p>
<p>（1）以开源解决文案为基础，实现对模型服务的高可用与负载均衡配置，尽量实现高并发、多用户使用。</p>
<p>（2）以此模型服务为基础，开发RAG应用或agent，丰富此大模型的功能应用。</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" class="category-chain-item">大模型</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E9%83%A8%E7%BD%B2%E6%BB%A1%E8%A1%80%E7%89%88DeepSeek/" class="print-no-link">#部署满血版DeepSeek</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>4✖8✖A800服务器部署满血版DeepSeek-R1-671B步骤</div>
      <div>https://jiangsanyin.github.io/2025/03/14/4✖8✖A800服务器部署满血版DeepSeek-R1-671B步骤/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>sanyinjiang</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年3月14日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - sanyinjiang">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/03/16/Centos8-x86-64%E4%B8%8A%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8nvidia-docker/" title="Centos8-x86_64上安装使用nvidia-docker">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Centos8-x86_64上安装使用nvidia-docker</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/03/12/KTransformers%E9%83%A8%E7%BD%B2DeepSeek-R1-Q4-K-M/" title="KTransformers部署DeepSeek-R1-Q4_K_M">
                        <span class="hidden-mobile">KTransformers部署DeepSeek-R1-Q4_K_M</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"lhgjNNoNy5Syl0F4Bw8i5P5K-gzGzoHsz","appKey":"0d6M8Wx7ZmYewOQqA20Nbqen","path":"window.location.pathname","placeholder":null,"avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="leancloud-site-pv-container" style="display: none">
        总访问量 
        <span id="leancloud-site-pv"></span>
         次
      </span>
    
    
      <span id="leancloud-site-uv-container" style="display: none">
        总访客数 
        <span id="leancloud-site-uv"></span>
         人
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>





  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script defer src="/js/leancloud.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
